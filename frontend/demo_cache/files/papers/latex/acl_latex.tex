\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Adaptive Verified Self-Distillation with Tiered Critic Ensembles and Shadow-Update Robustification}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}


% Essential math packages (auto-injected by tiny_scientist)
\usepackage{amsmath}   % For advanced math environments and \text{}
\usepackage{amssymb}   % For additional math symbols
\usepackage{amsthm}    % For theorem environments
\usepackage{mathtools} % Enhanced math support
\usepackage{bm}        % For bold math symbols

% Algorithm packages (supporting both old and new syntax)
\usepackage{algorithm}      % For algorithm floating environment
\usepackage{algorithmic}    % Old-style commands (\STATE, \FOR, etc.)
\usepackage{algpseudocode}  % New-style commands (\State, \For, etc.)
\usepackage{algorithmicx}   % Enhanced algorithm support

\begin{document}
\maketitle
% File: abstract.tex
% \textit{SEARCH/REPLACE} BEGIN

\begin{abstract}
We study whether an autonomous agent can reliably improve its expected task performance by retraining on data it generates itself, while avoiding confirmation bias, verifier gullibility, catastrophic forgetting, and performance collapse. This capability is important for scalable lifelong learning and maintenance but is challenging because self-generated candidates can be adversarial or out-of-distribution, verifiers can drift or be gamed, statistical acceptance is underpowered with modest validation budgets, and repeated updates risk forgetting. We propose AVSD-TCE-SR: an adaptive, auditable pipeline that combines (i) an adversary-strength scheduler that produces informative near-boundary yet plausibly in-distribution candidates, (ii) a tiered verification stack that uses cheap ensemble screening, a learned meta-verifier to route candidates, and budgeted expensive checks, and (iii) shadow-update robustification plus statistically principled acceptance (bootstrap one-sided lower bounds, shadow-consistency, and Benjamini–Hochberg correction) before committing updates. Commits are conservative: accepted data are mixed with replay, training uses importance-weighted sampling and optional EWC-style penalties, per-commit trust-region constraints are enforced, and full provenance/audit logs are retained. In controlled experiments (three independent AG\_NEWS runs) the pipeline produced reproducible final agents with mean validation accuracy 82.65\% and mean test accuracy 81.72\%, stable verifier signals, and no catastrophic performance collapse under the configured conservative acceptance rules. Results show the method operates as designed—reducing obvious false accepts and providing auditable evidence for each commit—while absolute gains remained modest under the chosen compact models and limited validation budgets, highlighting trade-offs between conservatism and detection power. Our main contributions are (1) an adaptive adversary scheduler for informative candidate generation, (2) a learnable tiered critic ensemble that routes candidates to budgeted verifications, (3) shadow-update ensemble robustification paired with principled statistical acceptance to control false-accepts, and (4) conservative commit procedures with replay/EWC and comprehensive auditing to mitigate forgetting and enable interpretability.
\end{abstract}

% \textit{SEARCH/REPLACE} END

\section{Introduction}

Autonomous self-improvement—where an agent retrains on data it generates to improve expected task performance—promises to reduce dependence on external labeling and enable continual adaptation, and related techniques such as pseudo-labeling and data-free distillation have demonstrated that models can expand or preserve knowledge without fresh human labels \cite{reminding2021,reducing2024}. Concrete instances include medical and scientific applications where pseudo-labeling or self-distillation reduce annotation needs (e.g., brain tumor segmentation, skin lesion diagnosis, rare-disease imaging) \cite{a2024,a2022,unsupervised2021}; for example, pseudo-labeling has been applied to diabetic retinopathy and small-lesion detection \cite{pseudolabeling2020}, self-supervised and multi-modality approaches have been explored for skin-lesion and multi-label classification \cite{selfsupervised2023,an2021}, and anti-curriculum and similarity-based pseudo-labeling schemes target robustness under label scarcity \cite{acpl2021,splal2023}. Cross-modal or missing-modality self-distillation methods improve segmentation and diagnostic robustness when modalities are absent or corrupted \cite{ccsd2025,m3ae2023,msdkmamba2025,modalityagnostic2023}. In related work, semi-supervised and distillation-focused algorithms tailored to medical segmentation have recently been proposed to better leverage limited labeled data \cite{distillmatch2024,symmatch2024,desd2022,volumetric2023}.

This question is both timely and important because the research community increasingly demands scalable, low-label-cost lifelong learners that operate in resource-constrained or privacy-sensitive settings where external re-labeling is impractical \cite{reminding2021,reducing2024}. Practical trends—larger pretrained models, edge and on-device deployment, and federated or source-free adaptation—create strong incentives for self-contained update mechanisms that preserve previously learned capabilities while incorporating new information \cite{federated2024,reminding2021,communicationefficient2024}. Scientific domains in particular (e.g., foundation-model efforts for brain dynamics and fMRI time-series) expose both the potential and unique challenges of large self-distilled representations in safety- and accuracy-critical contexts \cite{brainsemantoks2025,fdas2025}. In applied pipelines for medicine and industrial monitoring, self-generated supervision could materially reduce annotation costs and prolong deployment utility, but these benefits are contingent on robust safeguards against silent degradation and unverifiable commits \cite{a2024,embedding2022,domain2022}.

Achieving reliable autonomous self-improvement is technically hard because several failure modes interact and amplify one another. First, candidate-generation mechanisms that intentionally seek informative, near-boundary examples risk producing unrealistic out-of-distribution (OOD) or adversarial inputs that induce misleading gradients and brittle updates \cite{semisupervised2022,a2021,anyattack2024,adversarial2019}; adaptive adversary schedules can increase informativeness but also raise the probability of harmful, distribution-shifting candidates, and recent class-wise adversarial self-distillation work for medical segmentation highlights how adversarially-generated supervision can both help and harm training when not carefully constrained \cite{adversarial2025}. Second, internal verifiers (ensembles, calibrators, or learned critics) can drift, become overconfident, or be systematically gamed unless their uncertainties and routing decisions are rigorously controlled \cite{gets2024,calibration2017,improving2024,adaptive2022,proximityinformed2023,dualbranch2023,calibrating2024}; uncertainty-driven adaptive self-knowledge distillation methods show one path to incorporate model uncertainty into distillation objectives and verifier design \cite{uncertainty2025}, and in medical imaging image artifacts and missing modalities further complicate pseudo-label reliability and verifier calibration \cite{uslnet2023,missu2022,an2022}. Third, the statistical power of any acceptance test is limited by the validation budget $n_V$ and the empirical metric $\mathcal{L}_V$, so small genuine gains may not be detectable while noisy optimizers and stochastic shadow updates can produce spurious apparent improvements \cite{a2003,comparison2013,investigating2023,quaternion2021}; pseudo-label instability, boundary-label diffusion, and class-imbalance issues further exacerbate detectability problems in semi-supervised and transductive settings \cite{semisupervised2026,upl2025,splal2023}. Finally, repeated commits without explicit anti-forgetting measures invite catastrophic forgetting and coverage loss familiar from the continual-learning literature, so replay, regularization, or other preservation mechanisms are necessary but can conflict with adaptation objectives \cite{elastic2020,catastrophic2023,a2019,overcoming2018,replayenhanced2023,selective2021,curious2023,continuous2025}.

Why has no prior system fully solved this end-to-end problem? Existing lines of work address important subproblems but leave crucial gaps: many self-distillation and iterative synthesis approaches implicitly assume benign pseudo-labels or rely on heuristic filtering rather than auditable, statistically principled acceptance \cite{selfdistillation2026,a2024,a2022,unsupervised2021,selflabeling2025,you2025,diod2024,improved2025,distillmatch2024,symmatch2024}. In medical segmentation and few-shot regimes, supervised affinity attention, hierarchical dense-correlation, and masked cross-modality distillation have improved granularity but generally do not couple these advances to auditable commit tests \cite{a2021,hierarchical2023,cmmasksd2023}. Volume- and transformer-based self-distillation works demonstrate decoder-side and long-range consistency gains but typically assume trusted pseudo-labels or curated modality inputs \cite{volumetric2023,missu2022,m3ae2023,msdkmamba2025,ccsd2025,desd2022,dsvmunet2026}. Put succinctly, prior proposals typically (i) lack adversary-aware candidate generation and severity control \cite{semisupervised2022,a2021,adversarial2025}, (ii) lack multi-tier verification and meta-routing \cite{gets2024,adaptive2022,calibrating2024}, or (iii) do not enforce conservative, statistically justified commit rules and anti-forgetting safeguards \cite{a2003,comparison2013,elastic2020,overcoming2018,reducing2019,unified2019,selective2021,scrollnet2023}.

To address these gaps we introduce Adaptive Verified Self-Distillation with Tiered Critic Ensemble and Shadow-Update Robustification (AVSD-TCE-SR). In one sentence: AVSD-TCE-SR is an auditable pipeline that adaptively proposes near-boundary candidates under an adversary-strength scheduler, routes candidates through a tiered verifier stack (cheap ensemble screening, a learned meta-verifier, and budgeted expensive calibrators), executes shadow updates to estimate commit robustness, and enforces conservative commits using statistically principled acceptance tests together with replay and EWC-style regularization \cite{semisupervised2022,a2021,gets2024,adaptive2022,calibrating2024,a2003,comparison2013,replayenhanced2023,elastic2020,overcoming2018,reducing2019,unified2019,selective2021,scrollnet2023,confidenceweighted2025}. Concretely, the pipeline combines three key components: (i) an adaptive adversary scheduler that modulates candidate severity to balance informativeness and in-distribution plausibility, informed by adversarial-severity and semi-supervised adversarial design principles \cite{semisupervised2022,a2021,adversarial2025}; (ii) a tiered verification stack comprising fast ensemble-based filters, a learned meta-verifier for routing and triage, and budgeted expensive calibrators for high-stakes checks \cite{gets2024,adaptive2022,calibrating2024,confidenceweighted2025,communicationefficient2024}; and (iii) shadow-update ensembles plus hypothesis-testing and ensemble-assessment acceptance rules (bootstrap one-sided lower bounds, shadow-consistency constraints, and Benjamini--Hochberg correction) combined with conservative replay and EWC-style penalties to limit forgetting \cite{a2003,comparison2013,replayenhanced2023,elastic2020,overcoming2018}. Our design explicitly draws on recent self-distillation and foundation-model distillation work in medical imaging to inform architecture- and task-specific choices \cite{fdas2025,distillmatch2024,desd2022,dsvmunet2026,volumetric2023}.

Empirically, on controlled benchmarks our mechanistic analyses and statistical tests show that the pipeline (a) reduces false accepts and verifier gullibility, (b) produces auditable provenance for each commit, and (c) avoids catastrophic performance collapse under conservative acceptance regimes, while revealing the expected trade-off between conservatism and sensitivity identified in prior streaming continual-learning and semi-supervised medical evaluations \cite{a2019,a2023,curious2023,bridging2025,semisupervised2021,an2022,distillmatch2024,symmatch2024}.

\section{Related Work}

\paragraph{Self-distillation and continual learning} A body of recent work studies self-distillation and related knowledge-preservation techniques to mitigate catastrophic forgetting in online or incremental regimes: Nagata et al. show that self-distillation can reduce forgetting in class-incremental streams by replay-aware distillation strategies \cite{reducing2024}, Kurmi et al. emphasize incorporating uncertainty estimates from older models into distillation to better preserve past knowledge \cite{do2021}, and Wang et al. propose data-free pseudo-data distillation for incremental language models when previous data are unavailable \cite{reminding2021}. Benchmarking and domain-specific continual-learning studies further expose limitations of naive distillation and replay: Gao et al. construct a crowd-counting lifelong benchmark highlighting replay generalization issues \cite{forget2022}, Shenfeld et al. argue that on-policy and distilled signals can enable more robust continual updates \cite{selfdistillation2026}, and federated setups have adapted progressive self-distillation to personalize without forgetting \cite{federated2024}. Complementary directions address targeted preservation and controlled forgetting—UNDIAL adjusts logits for robust unlearning \cite{undial2024}, FADE combines sparse LoRA with self-distillation for selective forgetting \cite{fade2026}, and approaches that reset or diversify representations aim to retain useful features while learning new ones \cite{diverse2024,teach2025}. Together these works motivate our conservative update, replay and EWC-style regularization choices and highlight gaps: many self-distillation methods assume benign pseudo-labels or stationary data and do not provide tiered verification against adversarially-generated candidates, which motivates our meta-verifier and shadow-update robustness checks.

\paragraph{Adversarial self-generation and robust training} Several lines of research study automated generation of adversarial or self-supervised candidates and their use (or dangers) for model adaptation. Large-scale automated adversarial generation and targeted transfer attacks demonstrate how self-generated examples can be both powerful and brittle \cite{anyattack2024,saattack2023,enhancing2024}, and diffusion-based reversible adversarial example generators illustrate techniques for creating synthetic perturbations with controlled recoverability \cite{raediff2023}. On the defense side, self-supervised adversarial training and fast adversarial training methods have been proposed to improve robustness with limited compute, including ASTrA for adaptive attacks in self-supervised settings \cite{astra2025}, hierarchical/self-supervised adversarial pipelines for medical imagery \cite{hierarchical2025}, and fast-adversarial-training improvements that guide perturbation generation with self-knowledge \cite{improving2024}. Works that combine ensembling, defensive distillation, or sustainable iterative adversarial curricula show promise but also reveal trade-offs in compute and distributional drift \cite{seda2023,sustainable2024,damstf2023,anyattack2024}. Domain-adaptive self-training methods (e.g., DaMSTF) further highlight label-noise risks when bootstrapping from model predictions \cite{damstf2023}. These studies justify our adaptive adversary scheduler, multi-stage filtering (ensemble → meta-verifier → shadow-updates), and the inclusion of adversarial negatives for verifier retraining: prior adversarial generation methods can produce highly informative but potentially out-of-distribution candidates, so layered, calibrated acceptance tests are needed.

\paragraph{Ensembles, calibration, and verification for safe updates} Ensemble-based screening, calibration, and principled verification have been widely used across tasks and domains to improve reliability and to audit model updates. Classic and recent calibration methods (temperature scaling, parameterized or adaptive extensions, bin-wise variants, and proximity-aware adjustments) establish how post-hoc and constrained calibration can reduce overconfidence and enable better decision thresholds \cite{calibration2017,parameterized2021,binwise2019,adaptive2022,calibrating2024,improving2024,proximityinformed2023,dualbranch2023}. Graph- and domain-specific ensemble calibration work (e.g., GETS) further illustrates tailoring calibration to model structure \cite{gets2024}. Ensemble systems have been successfully applied in medical screening and other high-stakes domains—ensemble screening for diabetic retinopathy and glaucoma \cite{an2014,discaware2018}, self-ensembling ViTs for chest X-ray robustness \cite{seda2023}, and broader ensemble/meta-analysis approaches in bio/clinical studies underscore the role of aggregation for stable decisions \cite{an2023,exhaustive2024,association2020,association2023,individual2022,steroid2013,genetic2024,the2024,shortterm2023,infant2026}. Methodologically, variable-subspace ensembles and random-subspace frameworks motivate ensemble diversity strategies \cite{rase2021,tracking2020}, while optimization and verification literatures (trust-region and derivative-free schemes, cross-entropy and centroid-guided samplers, distributed/quasi-Newton DFO, and iterative ensemble smoothers) inform robust update and candidate-search procedures and failure-mode analyses \cite{a2009,a2019,a2023,a2025,bregman2025,distributed2021,benchmarking2021,variational2014,closedloop2010,research2024,dams2020}. Finally, applications that combine ensembling with auditing and provenance logging point to practical reproducibility and interpretability practices we adopt \cite{towards2022,physical2018,raediff2023,exhaustive2024}. Collectively, these works motivate our ensemble temperature scaling, uncertainty CIs, meta-verifier design, and the conservative statistical acceptance tests (bootstrap/sequential) used to guard self-updates.

\section{Method}

We formalize autonomous verified self-distillation as an iterative, auditable update procedure in which an agent autonomously proposes candidate training examples, verifies a controlled subset by internal checks, and commits conservative parameter updates only when statistically robust improvement is observed \cite{iterative2020,improving2023,selfprogressing2020,sprout2019}. Below we (i) state the problem and notation precisely, (ii) present the pipeline as a sequence of mathematical modules, (iii) give design rationales for each module, and (iv) state the statistical acceptance and commit rules that enforce low false-accept risk.

\paragraph{Problem definition and notation.}
Let $\mathcal{X}$ denote the input space and $\mathcal{Y}$ the label space. The agent is a parametric predictor

\begin{align}
f_{\theta}:\mathcal{X}\to\Delta(\mathcal{Y}),\qquad \theta\in\Theta,
\end{align}

where $\Delta(\mathcal{Y})$ denotes probability distributions over $\mathcal{Y}$. The learning target is the (unknown) data distribution $\mathcal{D}$ over $\mathcal{X}\times\mathcal{Y}$. We measure performance by an expected risk

\begin{align}
\mathcal{R}(\theta) &= \mathbb{E}_{(x,y)\sim\mathcal{D}}[\,\ell(f_{\theta}(x),y)\,],
\end{align}

for a supervised loss $\ell$ (e.g., cross-entropy). In practice we approximate $\mathcal{R}$ via a modest held-out validation set

\begin{align}
V=\{(x_i,y_i)\}_{i=1}^{n_V}
\end{align}

and define the empirical validation metric

\begin{align}
\mathcal{L}_V(\theta) &= \frac{1}{n_V}\sum_{i=1}^{n_V} \ell(f_{\theta}(x_i),y_i).
\end{align}


Assumptions used throughout: (A1) the unlabeled pool $\mathcal{U}\subseteq\mathcal{X}$ contains samples that are, with nonzero probability, near the support of $\mathcal{D}$; (A2) validation set $V$ is iid from $\mathcal{D}$ and sufficiently representative to detect meaningful degradations under our chosen risk level; (A3) optimizer and training stochasticity are exchangeable across repeated shadow simulations. We explicitly expose dependence on scheduler and verifier state to emphasize nonstationarity of the pipeline.

At round $t$ the system has access to:
- an unlabeled pool $\mathcal{U}$ from which seeds are drawn,
- a replay buffer $R$ of vetted examples,
- a verifier ensemble $\mathcal{C}=\{c_j\}_{j=1}^{k}$ (each $c_j:\mathcal{X}\to\mathbb{R}^{|\mathcal{Y}|}$ returning logits),
- a learned meta-verifier $M$ mapping feature vectors to a risk estimate,
- and a scheduler state $\alpha_t$ that controls candidate generation.

The pipeline proposes a candidate batch

\begin{align}
B_t &= \{(x_i,\hat y_i,\pi_i)\}_{i=1}^m,\qquad \hat y_i=\arg\max_y f_{\theta_t}(x_i),
\end{align}

where $\pi_i$ denotes provenance metadata (seed id, adversary parameters, etc.). The objective is to produce a sequence $\{\theta_t\}$ such that $\mathcal{R}(\theta_t)$ decreases monotonically in expectation while controlling Type-I errors (false accepts), verifier gullibility, and catastrophic forgetting.

\paragraph{Overall pipeline mapping and desiderata.}
Conceptually one round of the pipeline implements a mapping

\begin{align}
\mathcal{P}:\ (\theta_t,\mathcal{U},R,\mathcal{C},M,\alpha_t)\ \mapsto\ (\theta_{t+1},R',\alpha_{t+1},\mathcal{A}_t),
\end{align}

where $\mathcal{A}_t$ denotes recorded audit artifacts for the round. Desiderata that drive the mapping design are:
(1) conservatism: avoid committing unless evidence of net improvement is statistically strong;
(2) efficiency: allocate expensive verification selectively; and (3) audibility: produce provenance and repeatable artifacts enabling external review.

We next decompose $\mathcal{P}$ into modular components and formalize each.

\paragraph{Adaptive adversarial candidate generation.}
Motivation: generate candidates that are informative (probe near decision boundaries) yet plausibly in-distribution to avoid training on OOD artifacts; static adversaries produce either trivial or highly unrealistic samples so we schedule adversary strength dynamically \cite{anyattack2024,astra2025}.

Define the generator operator

\begin{align}
\mathcal{G}_{\alpha_t}:\ \mathcal{U}\times\Theta &\to \mathcal{X},\\
x' &= \mathcal{G}_{\alpha_t}(x;\theta_t),
\end{align}

where $\alpha_t$ parameterizes allowed perturbation magnitude, iteration budget, and augmentation diversity. Each generated point is required to satisfy a plausibility constraint expressed via an in-distributionness score

\begin{align}
d_{\text{in}}:\mathcal{X}\to\mathbb{R}_{\ge 0},\qquad d_{\text{in}}(x')\le \tau(\alpha_t),
\end{align}

for a threshold $\tau(\alpha_t)$ chosen by the scheduler to maintain a target plausibility probability.

A generic gradient-based construction used by the scheduler is

\begin{align}
x' &= \mathrm{Aug}\big(x + \delta\big),\\
\delta &\leftarrow \arg\max_{\|\delta\|\le\epsilon(\alpha_t)} \mathcal{A}(x+\delta;\theta_t),
\end{align}

where $\mathcal{A}$ is an adversarial objective (e.g., increase margin loss or induce label change) and $\epsilon(\alpha_t)$ encodes allowed perturbation scale under schedule $\alpha_t$.

Scheduler update rule (informal, compact formalization): the scheduler adjusts $\alpha_t$ to trade off informativeness and plausibility by observing recent statistics

\begin{align}
\alpha_{t+1} &= \Gamma\Big(\alpha_t,\ \widehat{\Pr}_{t}[d_{\text{in}}(x')\le\tau],\ \widehat{\mathrm{acc}}_{t},\ \kappa_t\Big),
\end{align}

where $\widehat{\Pr}_{t}[\cdot]$ and $\widehat{\mathrm{acc}}_{t}$ are empirical plausibility and acceptance rates and $\kappa_t$ encodes calibration drift; $\Gamma$ is a monotone operator that reduces $\epsilon$ when plausibility falls and increases it when acceptance stalls \cite{anyattack2024,astra2025,momentum2021,selfprogressing2020}.

Design rationale: keeping generation near informative boundaries increases value of vetted examples while plausibility constraints limit exposure to OOD artefacts; an explicit scheduler avoids brittle one-shot adversary settings.

\paragraph{Tiered verification pipeline (cheap $\to$ learned $\to$ expensive).}
Motivation: allocate compute efficiently by filtering candidates through increasingly expensive checks; learn which candidates warrant expensive verification \cite{gets2024,adaptive2022,diversifying2023,bornagain2020}.

Given candidate $(x',\hat y',\pi)$ the verification stack computes three stages.

Stage 1 --- Ensemble screening. Each ensemble member $c_j$ returns logits $z_j(x')\in\mathbb{R}^{|\mathcal{Y}|}$. With a temperature $\tau_{\mathrm{temp}}$ (fitted on $V$) form probability vectors

\begin{align}
p_j(x') &= \mathrm{softmax}\!\big(z_j(x')/\tau_{\mathrm{temp}}\big),
\end{align}

and define the ensemble mean and empirical dispersion

\begin{align}
\bar p(x') &= \frac{1}{k}\sum_{j=1}^k p_j(x'),\\
\mathrm{DIS}(x') &= \sqrt{\frac{1}{k}\sum_{j=1}^k \|p_j(x')-\bar p(x')\|_2^2}.
\end{align}

Additional scalar features (entropy, max confidence, mean logit) are concatenated into a feature vector $\phi_{\mathrm{ens}}(x')$. Temperature scaling and ensemble aggregation reduce overconfidence and provide uncertainty signals \cite{calibration2017,gets2024,wellcalibrated2019,network2021,network2022,maskts2024}.

Design rationale: a cheap ensemble provides calibrated uncertainty proxies and diversity-based dispersion that are informative and computationally inexpensive relative to downstream checks.

Stage 2 --- Meta-verifier routing. The meta-verifier is a learned mapping

\begin{align}
M:\ \Phi\to [0,1],\qquad r = M\big(\phi(x')\big),
\end{align}

where $\phi(x')$ concatenates ensemble features, provenance $\pi$, and agent-side signals (e.g., $f_{\theta_t}$ logits or gradient norms). The scalar $r$ estimates false-accept risk and yields a ternary routing decision

\begin{align}
\text{decision}(r) &=

\begin{cases}
\texttt{reject}, & r \ge r_{\text{reject}},\\
\texttt{escalate}, & r_{\text{pass}} \le r < r_{\text{reject}},\\
\texttt{pass}, & r < r_{\text{pass}}.
\end{cases}

\end{align}

$M$ is periodically retrained on historical labelled outcomes so it adapts to evolving failure modes \cite{adaptive2022,s2bnn2021}.

Design rationale: routing via a learned predictor concentrates budgeted checks on ambiguous or high-risk candidates and lets the system improve routing accuracy over time through supervised updates.

Stage 3 --- Budgeted secondary verification. Candidates routed to \texttt{escalate} undergo limited-budget secondary checks (MC-dropout aggregation, augmentation ensembling, environment rollouts, or shadow-update simulations). The tier concentrates expensive compute on ambiguous/high-impact candidates and lets $M$ learn to avoid repeated expensive checks \cite{gets2024,adaptive2022,momentum2021,aslmdfd2025,selfsupervised2021}.

Design rationale: secondary checks provide richer, often stochastic signals that expose brittleness unobserved in the cheap ensemble while keeping amortized cost bounded.

\paragraph{Meta-verifier training objective and dataset.}
The meta-verifier $M$ is trained on a rolling dataset

\begin{align}
\mathcal{D}_M &= \big\{ \big(\phi(x'), y_{\text{outcome}}\big) \big\}
\end{align}

where $y_{\text{outcome}}\in\{\text{true\_accept},\text{false\_accept},\text{rejected}\}$ is obtained from historical secondary checks and audited human labels when available. The training solves

\begin{align}
\min_{\psi}\; \mathbb{E}_{(\phi,y)\sim\mathcal{D}_M}\Big[ \mathcal{L}_{\text{meta}}\big(M_{\psi}(\phi),y\big) \Big],
\end{align}

with class-weighting to penalize false-accepts heavily. Design rationale: explicit supervision on outcomes reduces gullibility and allows calibration of operating thresholds $r_{\text{pass}},r_{\text{reject}}$.

\paragraph{Shadow-update robustification and statistical acceptance criterion.}
Motivation: reduce commits caused by optimizer noise, lucky minibatch effects, or stochasticity by requiring consistent simulated improvement under diverse shadow conditions \cite{a2003,comparison2013,momentbased2020,ensemble2020}.

Let $B$ denote a vetted batch of accepted-by-screening examples. Define a constrained single-update operator

\begin{align}
\mathcal{T}_{\mathrm{constr}}(\theta,B;\eta) &= \arg\min_{\theta'} \; \mathbb{E}_{(x,y)\in B} \big[\ell(f_{\theta'}(x),y)\big] \\
&\quad + \mathcal{R}_{\mathrm{trust}}(\theta',\theta;\eta),\nonumber
\end{align}

where $\mathcal{R}_{\mathrm{trust}}$ is a trust-region or proximal penalty (e.g., L2-clipping, KL or quadratic penalty) parameterized by shadow config $\eta$.

Generate $S$ shadow configurations $\{\eta^{(s)}\}_{s=1}^S$ and corresponding shadow parameters

\begin{align}
\theta^{(s)} &= \mathcal{T}_{\mathrm{constr}}(\theta_t,B;\eta^{(s)}),\qquad s=1,\dots,S.
\end{align}

Evaluate validation loss reductions

\begin{align}
\Delta^{(s)} &= \mathcal{L}_V(\theta_t) - \mathcal{L}_V(\theta^{(s)}).
\end{align}

Form the empirical distribution $\{\Delta^{(s)}\}_{s=1}^S$ and compute:
- the bootstrap one-sided lower confidence bound $L_{\alpha}(\Delta)$ on the mean improvement (level $1-\alpha$),
- the shadow-consistency fraction

\begin{align}
\rho &= \frac{1}{S}\sum_{s=1}^S \mathbf{1}\{\Delta^{(s)}\ge 0\}.
\end{align}


Acceptance rule: commit only if

\begin{align}
L_{\alpha}(\Delta) &> 0 \quad\text{and}\quad \rho \ge \rho_{\min},\label{eq:acceptance_refined}
\end{align}

with a high threshold $\rho_{\min}$ (e.g., $0.75$). To control false discoveries across multiple rounds we compute per-round one-sided $p$-values and apply the Benjamini--Hochberg (BH) procedure across rounds to bound the false discovery rate \cite{a2003,comparison2013}. Optionally, sequential testing variants (e.g., repeated confidence sequences or SPRT-inspired rules) may be used when many small increments are expected \cite{comparison2013}.

Concrete bootstrap lower bound computation (compact): let $\bar\Delta=\frac{1}{S}\sum_s\Delta^{(s)}$ and let $\{\bar\Delta^{*(b)}\}_{b=1}^B$ be bootstrap means obtained by resampling $\{\Delta^{(s)}\}$ with replacement; then

\begin{align}
L_{\alpha}(\Delta) &= \text{quantile}_{\alpha}\big( \{\bar\Delta^{*(b)}\}_{b=1}^B \big).
\end{align}

Design rationale: the bootstrap lower bound targets estimation uncertainty from a small $V$, while $\rho$ guards against brittle improvements that depend on particular optimizer seeds; together they reduce Type-I errors in committing self-generated data \cite{a2003,comparison2013}.

\paragraph{Conservative commit update, replay mixing, and anti-forgetting regularization.}
Motivation: accepted self-generated data can be non-representative and cause forgetting; mixing with replay and applying parameter regularization preserves prior competencies \cite{elastic2020,replayenhanced2023,selective2021,improving2023}.

When $B$ is accepted, define the commit empirical distribution

\begin{align}
\tilde{D} &= (1-\gamma) \, \tilde{D}_{\mathrm{replay}} + \gamma \, \tilde{D}_{\mathrm{new}},
\end{align}

where $\tilde{D}_{\mathrm{new}}$ is the accepted generated data, $\tilde{D}_{\mathrm{replay}}$ is sampled from $R$, and $\gamma\in[0,1]$ is an acceptance mixing weight (possibly adaptive).

The conservative commit objective is

\begin{align}
\mathcal{L}_{\mathrm{commit}}(\theta) &= \mathbb{E}_{(x,y)\sim\tilde{D}}\big[\ell(f_{\theta}(x),y)\big] \\
&\quad + \lambda_{\mathrm{EWC}} \,\Omega_{\mathrm{F}}(\theta;\theta_t),\nonumber
\end{align}

where $\Omega_{\mathrm{F}}$ is an optional quadratic Fisher penalty centered at $\theta_t$ (EWC-style) that penalizes movement in important directions \cite{elastic2020}. The final commit performs a constrained optimization (trust-region or proximal update) to obtain $\theta_{t+1}$:

\begin{align}
\theta_{t+1} &= \arg\min_{\theta} \; \mathcal{L}_{\mathrm{commit}}(\theta)
\quad\text{s.t.}\quad
\mathcal{D}_{\mathrm{TR}}(\theta,\theta_t)\le \delta,
\end{align}

where $\mathcal{D}_{\mathrm{TR}}$ is a distance (e.g., KL or parameter-norm) and $\delta$ a small radius.

After committing, $R$ is updated by adding vetted examples and pruning to maintain coverage (per-class or loss-based metrics) to avoid replay concentration \cite{replayenhanced2023,selective2021}. Design rationale: mixing and regularization bound the influence of any single self-generated batch and preserve competencies learned from prior data.

\paragraph{Verifier maintenance, calibration, and meta-verifier retraining.}
Motivation: verifiers can drift or become gullible if exposed to generated data; periodic maintenance keeps the stack reliable and auditable \cite{calibration2017,gets2024,adaptive2022,wellcalibrated2019}.

Concretely, we maintain verifiers by (i) retraining/fine-tuning ensemble members on the union of original training data, vetted accepted samples, and curated adversarial negatives; (ii) monitoring calibration metrics on $V$ and refitting temperature scaling when ECE or related statistics degrade; and (iii) preserving ensemble diversity by reinitializing or perturbing member parameters periodically \cite{calibration2017,gets2024,wellcalibrated2019,network2021,network2022,maskts2024,efficient2025,determining2025,evaluating2025}. The meta-verifier $M$ is retrained on historical candidate outcomes (true accept / false accept / rejected) so its predictive quality improves over time \cite{adaptive2022,s2bnn2021}. Design rationale: continual maintenance prevents systemic drift and helps bound long-run false-accept rates.

\paragraph{Auditing, interpretability, and stopping.}
Motivation: scientific and safety claims require reproducible evidence for each accepted change \cite{towards2022,exhaustive2024,assessing2024}.

Formally, the audit artifact for round $t$ is

\begin{align}
\mathcal{A}_t &= \Big\{B_t,\ \{\phi_{\mathrm{ens}}(x')\}_{x'\in B_t},\ \{r(x')\}_{x'\in B_t},\\
&\quad \{\theta^{(s)}\}_{s=1}^S,\ \{\Delta^{(s)}\}_{s=1}^S,\ L_{\alpha}(\Delta),\ \rho,\ \text{commit\_decision}\Big\}.\nonumber
\end{align}

We halt the outer loop under an explicit stopping rule (fixed round budget or stagnation defined by no accepted updates for a configured number of consecutive rounds) and return the audited sequence of accepted commits \cite{assessing2024}.

Design rationale: deterministic artifacts allow post-hoc verification, reproducibility, and external compliance checks.

\paragraph{Algorithmic summary.}
Algorithm \ref{alg:avs} summarizes one round of AVSD-TCE-SR as a sequence of deterministic and stochastic mappings consistent with the mathematical components above.


\begin{algorithm}
\caption{One round of AVSD-TCE-SR}

\begin{algorithmic}
\STATE Input: $\theta_t$, unlabeled pool $\mathcal{U}$, validation $V$, replay $R$, verifiers $\mathcal{C}$, meta-verifier $M$, scheduler state $\alpha_t$
\STATE Sample seeds $S\subset\mathcal{U}$
\FOR{each seed $x\in S$}
 \STATE Generate candidate $x' \leftarrow \mathcal{G}_{\alpha_t}(x;\theta_t)$ and form $(x',\hat{y}',\pi)$
 \STATE Compute ensemble logits $z_j(x')$, probabilities $p_j(x')$ and features $\phi_{\mathrm{ens}}(x')$
 \STATE Form full feature $\phi(x') = [\phi_{\mathrm{ens}}(x'),\pi,\text{agent-features}]$
 \STATE Obtain meta-score $r \leftarrow M(\phi(x'))$
 \IF{$r \ge r_{\text{reject}}$}
 \STATE mark candidate as \texttt{reject}
 \ELSIF{$r \ge r_{\text{pass}}$}
 \STATE escalate to secondary verification (MC-dropout, rollouts, or shadow-sim)
 \ELSE
 \STATE mark candidate as \texttt{pass for shadowing}
 \ENDIF
\ENDFOR
\STATE Collect vetted batch $B$ from candidates passing screening
\STATE For $B$, generate $S$ shadow updates $\{\theta^{(s)}\}$ via $\mathcal{T}_{\mathrm{constr}}(\cdot)$ under varied $\{\eta^{(s)}\}$
\STATE Evaluate $\{\Delta^{(s)}\}$ on $V$; compute bootstrap lower bound $L_{\alpha}$ and $\rho$
\IF{$L_{\alpha}>0$ and $\rho\ge\rho_{\min}$ (after BH correction)}
 \STATE Commit conservative update: $\theta_{t+1}\leftarrow\arg\min_{\theta}\mathcal{L}_{\mathrm{commit}}(\theta)$
 \STATE Update replay buffer $R$ with vetted examples
 \STATE Log provenance and audit artifacts $\mathcal{A}_t$
\ELSE
 \STATE $\theta_{t+1}\leftarrow\theta_t$
\ENDIF
\STATE Update scheduler $\alpha_{t+1}$ and retrain $M$ / maintain verifiers as scheduled
\end{algorithmic}

\label{alg:avs}
\end{algorithm}


\paragraph{Design rationale recap.}
The method rests on three risk-control pillars: (1) adaptive generation that keeps candidates informative yet plausibly in-distribution \cite{anyattack2024,astra2025,momentum2021,selfprogressing2020,sprout2019}; (2) a learnable, tiered verification stack that funnels only ambiguous or high-impact candidates to expensive checks while learning to predict false accepts \cite{gets2024,adaptive2022,diversifying2023}; and (3) shadow-update ensemble validation plus conservative, regularized commits (replay + EWC-style penalties) to avoid optimizer-noise-driven or forgetting-inducing accepts \cite{a2003,elastic2020,replayenhanced2023,improving2023}. These elements produce an auditable pipeline for testing whether autonomous self-distillation can reliably improve expected task performance under principled statistical control.

\paragraph{Summary of formal guarantees (operational).}
While formal, distribution-free guarantees are impossible without strong assumptions, the pipeline enforces two operational controls with quantifiable properties: (i) per-round Type-I control via the bootstrap LCB and BH correction which bounds the false discovery rate across rounds under exchangeability assumptions \cite{a2003,comparison2013}; and (ii) a robustness requirement via the shadow-consistency fraction $\rho$ that empirically reduces commit probability for improvements sensitive to optimizer or sampling noise. Together these provide actionable, auditable checks that managers or auditors can inspect and tune to reach a desired operating point.

\section{Experimental Setup}

This section gives a complete, reproducible description of how we instantiate and evaluate AVSD-TCE-SR on the MNIST benchmark. All choices (data splits, preprocessing, model sizes, adversary settings, verifier procedures, statistical tests, and seeds) are specified so other researchers can rerun the experiments and reproduce reported results. When a design choice follows prior work we cite that work immediately after the supporting sentence.

\subsection{Overview and goals}

Our empirical goals are threefold: (i) demonstrate that an agent can improve (or at least not degrade) task performance by retraining on self-generated data under conservative acceptance rules, (ii) measure the pipeline's false-accept rate and catastrophic-forgetting behaviour under repeated rounds, and (iii) evaluate trade-offs between conservatism (Type I control) and sensitivity (power) across ablations. To make the evaluation controlled and fast for many ablations we use MNIST as the target distribution and follow the adaptive adversary, tiered verification, shadow-update, and conservative-commit pipeline described in Section~\S\ref{sec:method}. Design decisions (compact models, modest validation budget, and per-round caps) prioritize reproducibility and the ability to run many independent seeds and ablations within a modest compute envelope; this setting exposes the statistical and verification challenges that motivated our method while keeping experiments tractable.

\subsection{Notation and problem instantiation}

We reuse notation from Section~\S\ref{sec:method}. Concretely, let $f_{\theta}:\mathcal{X}\to\Delta(\mathcal{Y})$ denote the agent with parameters $\theta$. The held-out validation set is $V=\{(x_i,y_i)\}_{i=1}^{n_V}$ and the held-out test set is $T=\{(x_i,y_i)\}_{i=1}^{n_T}$. We report empirical validation loss and accuracy:

\begin{align}
\mathcal{L}_V(\theta) &= \frac{1}{n_V}\sum_{i=1}^{n_V}\ell(f_{\theta}(x_i),y_i),\\
\mathrm{Acc}_V(\theta) &= \frac{1}{n_V}\sum_{i=1}^{n_V}\mathbf{1}\{\arg\max_y f_{\theta}(x_i)=y_i\},
\end{align}

and analogous definitions for $\mathcal{L}_T$ and $\mathrm{Acc}_T$.

We denote the unlabeled candidate pool at round $t$ by $\mathcal{U}_t$ and the set of generated candidate examples proposed by the adversary as $G_t$. Each round produces a vetted batch $B_t\subseteq G_t$ that is considered for commit. All RNG seeds (top-level run seed and per-component seeds) are fixed and recorded in the run manifest; code sets these seeds at process start and at each deterministic operation to enable exact replay.

\subsection{Dataset and preprocessing}

- Dataset loader: we load MNIST using the HuggingFace datasets API with the command \texttt{datasets.load\_dataset("mnist")} (we record the exact HF dataset commit hash and version in the released artifacts).
- Downsampling and splits: we perform stratified random sampling by class from the HF \texttt{train} and \texttt{test} splits to obtain train = $5000$, validation ($V$) = $1000$, test = $1000$; sampling is deterministic per-run using the top-level RNG seed. Stratified sampling preserves per-class balance so replay and candidate sampling do not introduce class-skew artifacts in downstream evaluations.
- Preprocessing: read each $28\times28$ uint8 image, cast to float32, normalize pixel values to $[0,1]$, and flatten to a $784$-dimensional vector. Resulting per-example feature vector $x\in\mathbb{R}^{784}$ is used throughout. Exact preprocessing code is included in the released repository to ensure bit-for-bit reproducibility.

Design rationale: these dataset choices keep experiments fast while preserving the core classification challenge and sufficient class stratification needed for unbiased replay and candidate sampling; the modest validation size also deliberately stresses the statistical acceptance tests to evaluate their robustness.

\subsection{Model architectures and initial conditions}

- Agent $A_0$ (base model): MLP with layers Dense(48, ReLU) $\to$ Dense(24, ReLU) $\to$ Dense(10, linear logits). Total trainable parameters = $39{,}106$. Compact architectures reduce per-round compute so we can run many rounds and seeds while still exposing calibration and forgetting phenomena.
- Verifier ensemble $\mathcal{C}=\{c_j\}_{j=1}^{3}$: three independent MLPs Dense(24, ReLU) $\to$ Dense(10, linear logits). Each verifier is independently initialized with a distinct RNG seed to promote ensemble diversity. Temperature-scaling scalars for each verifier are fit on $V$ prior to round $t=1$ (calibration) as described in Subsection ``Verifier retraining, calibration monitoring, and diversity maintenance''. The use of ensembles and temperature scaling follows standard calibration/ensemble practice \cite{calibration2017,gets2024}.
- Meta-verifier $M$: an MLP with one hidden layer (32 units, ReLU) mapping per-candidate feature vectors (ensemble statistics, provenance flags, agent-side features) to a scalar risk score $r\in[0,1]$ and a ternary recommendation (\texttt{pass}/\texttt{escalate}/\texttt{reject}). $M$ is trained online on accumulated labeled candidate outcomes (see Subsection ``Meta-verifier training and maintenance'').

All initial checkpoints and RNG seeds are saved to enable exact restoration of $A_0$ and all verifier/meta-verifier members.

Design rationale: small, diverse verifier ensembles provide a tractable yet informative disagreement signal; the meta-verifier compresses ensemble statistics and provenance into a routing decision to reduce expensive checks while retaining auditability.

\subsection{Training and optimizer defaults}

- Commit optimizer: SGD with momentum $0.9$, initial learning rate $\eta_{\text{commit}}=1\mathrm{e}{-3}$, batch size $32$, weight decay $10^{-5}$; per-step gradient clipping is applied on the $L_2$ norm with clip threshold $0.1$. Commits run up to $5$ epochs over the mixed commit dataset but include early stopping if validation loss does not improve for $2$ commit-epochs.
- Shadow-update optimizer: SGD with $\eta_{\text{shadow}}=5\mathrm{e}{-4}$, momentum $0.9$; each shadow update is a single constrained mini-update (one epoch over the vetted batch $B$, see Subsection ``Shadow-update robustification and statistical acceptance'') to simulate conservative parameter shifts. These small-step settings reduce optimizer-induced variance during shadow evaluation.

Design rationale: commit optimizer is slightly more aggressive than shadow updates to reflect that accepted commits should consolidate improvements, whereas shadow updates emulate the conservative, low-magnitude changes we expect the agent to encounter if a commit were applied; gradient clipping and small learning rates reduce optimizer-induced false positives in shadow assessments.

\subsection{Adversarial / candidate generation}

- Seed sampling: at round $t$ we draw up to $N_{\text{cand}}=200$ candidate seeds $S_t$ from the unlabeled pool $\mathcal{U}$ by stratified sampling when labels are available or uniform sampling otherwise; sampling is deterministic given the per-round RNG seed. Stratification maintains per-class exploration and prevents the adversary from concentrating excessively on classes with more unlabeled mass.
- Attack operators: we generate candidates by applying FGSM and short PGD (5-step) attacks targeting the agent logits; perturbations are constrained in $L_\infty$ with $\varepsilon\in\{0.05,0.1\}$ (pixel range normalized to $[0,1]$). After adversarial perturbation we apply plausibility-preserving augmentations (rotation in $[-10^\circ,10^\circ]$, brightness shift $\pm 0.1$). The use of FGSM/PGD-style attacks and small augmentations follows prior adversarial/self-generation practice \cite{anyattack2024,astra2025}.
- Adversary scheduler: the scheduler state $\alpha_t$ selects attack strength (choice of $\varepsilon$ and PGD iterations) as a deterministic function of (i) recent acceptance rate $a_{t-1}$, (ii) verifier calibration drift measured as $\Delta\mathrm{ECE}_t$ on $V$, and (iii) an in-distributionness score $d_{\mathrm{in}}(x)$ (autoencoder reconstruction error). The scheduler increases adversary strength if $a_{t-1}$ is large and decreases it when $\Delta\mathrm{ECE}_t$ indicates verifier drift or when $d_{\mathrm{in}}$ suggests OOD candidates. The scheduler is implemented as a small state machine to ensure repeatability for a given RNG seed.

Design rationale: adaptive adversary strength targets near-boundary yet plausibly in-distribution examples to maximize informative candidates while avoiding gross OOD examples that would be misleading; the deterministic scheduler makes experiments reproducible and limits confounding by stochastic adversary exploration.

\subsection{Tiered verification pipeline (cheap $\to$ learned $\to$ expensive)}

We formalize the three verification stages and their outputs.

- Ensemble screening (cheap): for each candidate $(x',\hat{y}')$ the verifier ensemble produces per-member logits $\{z_j(x')\}_{j=1}^k$ and probability vectors $p_j(x')=\mathrm{softmax}(z_j(x')/T_j)$ where $T_j$ is the temperature for member $j$ (fitted on $V$). We compute the following per-candidate features:
 \begin{align}
 \bar{p}(x') &= \frac{1}{k}\sum_{j=1}^k p_j(x'),\quad
 \mathrm{DIS}(x') = \sqrt{\frac{1}{k}\sum_{j=1}^k \|p_j(x')-\bar{p}(x')\|_2^2},\nonumber\\
 H(x') &= -\sum_{c}\bar{p}_c(x')\log \bar{p}_c(x'),\quad
 \text{maxconf}(x')=\max_c \bar{p}_c(x').
 \end{align}

 Candidates failing simple ensemble thresholds (e.g., maxconf below a low-confidence cutoff) are rejected cheaply. Temperature-scaling per verifier is fit on $V$ prior to screening as in \cite{calibration2017}.

Design rationale: cheap ensemble statistics (mean probability, disagreement, entropy, max confidence) provide a low-compute filter that removes obviously low-quality or out-of-distribution candidates and reduces compute spent on expensive checks.

- Meta-verifier scoring (learned): the meta-verifier $M$ maps candidate feature vectors $\phi(x')$ (ensemble features, provenance flags, and agent-side features) to a risk score $r=M(\phi(x'))\in[0,1]$. Given tunable thresholds $0\le r_{\text{pass}}<r_{\text{reject}}\le 1$, the routing is:
 \begin{equation}
 \text{decision}(x') =
 \begin{cases}
 \texttt{pass-for-shadow}, & r < r_{\text{pass}},\\
 \texttt{escalate}, & r_{\text{pass}}\le r < r_{\text{reject}},\\
 \texttt{reject}, & r \ge r_{\text{reject}}.
 \end{cases}

 \end{equation}

 $M$ is retrained periodically on labeled historical outcomes (see Subsection ``Meta-verifier training and maintenance''); a learned routing reduces repeated expensive checks by routing low-risk candidates directly to shadow simulation \cite{adaptive2022,gets2024}.

Design rationale: learned routing compresses multi-dimensional ensemble and provenance signals into a calibrated risk estimate so we can concentrate budgeted expensive checks on ambiguous candidates while still retaining auditability.

- Budgeted secondary verification (expensive): candidates routed to \texttt{escalate} incur one of: MC-dropout uncertainty estimation (dropout $p=0.1$, 20 draws), longer adversarial/augmented evaluation, or an immediate shadow-sim (partial shadow update targeted at single high-impact candidates). The fraction of candidates escalated per round is capped by a compute budget to ensure experiment feasibility.

Design rationale: limiting expensive checks bounds per-round compute and forces the meta-verifier to be selective, which is realistic for deployed systems with finite verification resources.

\subsection{Meta-verifier training and maintenance}

- Training signal: each candidate routed to expensive checks or that is used in a commit is eventually labeled with an outcome class in \{\texttt{true-accept}, \texttt{false-accept}, \texttt{reject}\} based on whether applying the candidate (or batch) to the agent would increase $\mathrm{Acc}_V$ (or decrease it). These labeled outcomes form the training set for $M$.
- Update schedule: $M$ is retrained every $5$ rounds or after $50$ newly labeled outcomes (whichever occurs first). A held-out meta-validation split (10\% of the meta-training data) is used to detect meta-overfitting and to trigger reinitialization if validation metrics deteriorate. This periodic retraining and held-out meta-validation follows prior adaptive-verifier recommendations \cite{adaptive2022}.

Design rationale: periodic retraining balances responsiveness to new failure modes with stability and prevents $M$ from overfitting to early, non-representative candidate distributions.

\subsection{Shadow-update robustification and statistical acceptance}

We formalize shadow updates, aggregation, and the acceptance rule.

- Shadow ensemble: for each vetted batch $B$ (size $|B|$), construct $S$ shadow configurations $\{\theta_t^{(s)}\}_{s=1}^{S}$ by applying constrained shadow updates to the current parameters $\theta_t$. Each shadow update is a single epoch over $B$ with the shadow optimizer and randomization (seed, mini-batch order, learning rate jitter $\pm 10\%$). We set $S=10$ by default to balance computational cost and variance estimation. A trust-region constraint is approximated by enforcing a relative parameter-change cap:
 \begin{equation}
 \|\theta_t^{(s)}-\theta_t\|_2 \le \tau \|\theta_t\|_2,\qquad \tau=0.01.
 \end{equation}

 Gradient L2 clipping is also applied during each shadow update to bound per-step changes.

Design rationale: multiple randomized shadow draws expose optimizer and data-order variability, while a tight relative-change cap keeps simulated commits conservative and comparable to prospective real commits.

- Shadow evaluation statistics: evaluate each shadow $\theta_t^{(s)}$ on $V$ to obtain accuracy $a^{(s)}=\mathrm{Acc}_V(\theta_t^{(s)})$ and compute per-shadow deltas
 \begin{equation}
 \Delta^{(s)} = a^{(s)} - a^{(0)},\qquad a^{(0)}=\mathrm{Acc}_V(\theta_t).
 \end{equation}

 Aggregate the $S$ deltas and compute:
 \begin{enumerate}
 \item bootstrap one-sided lower confidence bound $L_{\alpha}(\Delta)$ on the mean improvement $\mu_{\Delta}=\frac{1}{S}\sum_{s}\Delta^{(s)}$ using $B_{\text{boot}}=1000$ bootstrap resamples; $L_{\alpha}(\Delta)$ is the $(\alpha)$-quantile of the bootstrap distribution of the mean (we use one-sided $\alpha=0.05$ so require $L_{\alpha}>0$ for acceptance), and
 \item shadow-consistency fraction
 \begin{equation}
 \rho = \frac{1}{S}\sum_{s=1}^S \mathbf{1}\{\Delta^{(s)}\ge 0\}.
 \end{equation}

 \end{enumerate}

 The bootstrap CI and shadow-consistency metrics follow ensemble-assessment and repeated-testing literature \cite{a2003,comparison2013}.

Design rationale: bootstrap-based CIs are nonparametric and perform well for small $S$; combining a lower-bound requirement with a consistency fraction prevents acceptance driven by a small number of lucky shadow draws.

- Per-round hypothesis test and multiple-round correction: for round $t$ define the one-sided bootstrap $p$-value
 \begin{equation}
 p_t = \frac{1}{B_{\text{boot}}}\sum_{b=1}^{B_{\text{boot}}}\mathbf{1}\{\bar{\Delta}^{(b)}\le 0\},
 \end{equation}

 where $\{\bar{\Delta}^{(b)}\}$ are the bootstrap resampled means. Across rounds we apply the Benjamini--Hochberg (BH) procedure at target false discovery rate $q=0.05$ to the sequence $\{p_t\}$ to control for multiple tests \cite{a2003}. A round's improvement is considered significant after BH correction if its BH-adjusted indicator is positive.

Design rationale: BH provides an interpretable global Type I control across rounds while retaining more power than strict family-wise error corrections; this is appropriate for a pipeline that performs many correlated acceptance tests over time.

- Acceptance rule: accept vetted batch $B$ and perform a conservative commit iff all three conditions hold:
 \begin{enumerate}
 \item $L_{\alpha}(\Delta) > 0$ (one-sided bootstrap lower bound positive),
 \item $\rho \ge \rho_{\min}$ with $\rho_{\min}=0.75$, and
 \item the round's $p_t$ passes BH correction at FDR $q=0.05$ across the sequence of tested rounds \cite{a2003}.
 \end{enumerate}

 Optionally, when many small increments are expected we enable a sequential repeated-CI monitoring mode (SPR-like) as an alternate acceptance mode; implementation details and stopping boundaries are provided in the released code and follow \cite{comparison2013}.

Design rationale: combining a bootstrap lower bound (which is distribution-free and robust in small-$S$ regimes), a shadow-consistency requirement, and BH correction provides layered protection against Type I errors due to optimizer noise, correlated shadow draws, or multiple rounds of testing.

\subsection{Conservative commit, replay, and anti-forgetting regularization}

- Commit dataset composition: when a vetted batch $B$ is accepted, the commit dataset $\tilde{D}$ is formed by mixing the accepted generated examples $G$ with a replay minibatch sampled from the replay buffer $R$. Let $\gamma\in[0,1]$ denote the maximum fraction of generated examples allowed in any commit minibatch; we enforce $\gamma=0.40$ (i.e., generated samples $\le 40\%$ of each commit minibatch). Sampling from $R$ is importance-weighted: original training samples receive higher sampling weight than generated samples to prioritize retention of prior knowledge. Replay buffer capacity is $|R|_{\max}=2000$, seeded initially with the $5000$-train subsample; we maintain a fixed replay-seed subset of $1000$ examples for the catastrophic-forgetting monitor.
- Regularization during commit: commits may include an optional EWC-style quadratic penalty
 \begin{equation}
 \mathcal{L}_{\text{commit}}(\theta)=\mathcal{L}_{\tilde{D}}(\theta)+\lambda_{\mathrm{EWC}}\sum_i F_{i}(\theta_i-\theta_{i}^*)^2,
 \end{equation}

 where $F_i$ are diagonal Fisher approximations and $\theta^*$ are reference parameters; default $\lambda_{\mathrm{EWC}}=1.0$ and ablations toggle this term \cite{elastic2020}. Commits use the conservative optimizer settings in Subsection ``Training and optimizer defaults''.
- Replay maintenance: after a successful commit we add vetted examples to $R$ and prune $R$ deterministically to maintain per-class coverage via a loss-based utility metric; pruning removes lowest-utility samples when $|R|>|R|_{\max}$.

Design rationale: mixing replay and importance-weighted sampling prevents the population shift caused by aggressive self-generated data and reduces catastrophic forgetting risk; the $\gamma$ cap and importance weights quantify conservatism and are tuned to trade off sensitivity vs stability.

\subsection{Verifier retraining, calibration monitoring, and diversity maintenance}

- Retraining schedule: verifiers are retrained or fine-tuned every $10$ accepted commits or earlier if ECE on $V$ increases by more than $0.02$ absolute points from the last verifier checkpoint; temperature scaling is refit on $V$ after any retrain \cite{calibration2017,gets2024}.
- Diversity maintenance: to preserve ensemble diversity we reinitialize one verifier member (randomly chosen) every $20$ accepted commits and retrain from a different seed; when retraining we inject adversarial negatives from the generator to improve robustness \cite{diversifying2023,anyattack2024}.
- MC-dropout CI: expensive uncertainty checks use MC-dropout with dropout $p=0.1$ and $20$ stochastic forward passes.

Design rationale: periodic retraining and controlled member replacement prevent verifier collapse and maintain meaningful disagreement signals for ensemble screening.

\subsection{Baselines, ablations and experimental runs}

- Baselines: (1) Supervised static baseline (no self-updates), (2) Naive self-training (accept-all generated examples without verification), (3) Self-distillation (iterative distillation without meta-verifier/shadow-updates), (4) Ensemble-filtering only (pipeline stops after ensemble screening), (5) Oracle-upper (use ground-truth selected adversarial examples with supervised labels). These baselines isolate pipeline components.
- Ablations: (A) no meta-verifier (ensemble-only routing), (B) no shadow-updates (single simulated update acceptance), (C) no adaptive adversary (fixed $\varepsilon=0.1$ PGD), (D) no replay or no EWC (two separate ablations), (E) sequential testing mode vs bootstrap mode for acceptance. Each ablation uses the same seeds and hardware conditions for fairness.
- Independent runs and seeds: each experiment (method or baseline) is repeated with $5$ independent top-level RNG seeds. For each run we record all per-round RNG seeds, model checkpoints, and artifact logs.

Design rationale: paired comparisons across identical seeds and initial checkpoints provide stronger causal attribution for performance differences than unpaired runs.

\subsection{Evaluation metrics and statistical testing}

- Primary metric: classification accuracy on held-out test set $\mathrm{Acc}_T$.
- Secondary metrics (computed per round and reported as trajectories): test/train/validation accuracy and loss, delta test accuracy relative to $A_0$ with $95\%$ bootstrap CI, false-accept rate (proportion of accepted updates that reduce validation or test accuracy when evaluated with ground-truth labels), rejection rate (fraction of generated candidates rejected), catastrophic forgetting measured as drop in accuracy on the fixed replay-seed subset of $1000$ examples, Expected Calibration Error (ECE) on $V$ and $T$, and shadow-consistency fraction per accepted batch. These metrics and definitions follow the experiment plan and prior continual/self-distillation evaluation work \cite{replayenhanced2023,elastic2020}.
- Statistical testing: per-round acceptance uses the one-sided bootstrap lower bound described above with $\alpha=0.05$, and Benjamini--Hochberg correction across rounds controls FDR at $0.05$ \cite{a2003}. Paired bootstrap comparisons (1000 bootstrap resamples) across independent runs are used to compare final-test accuracy between methods and baselines; differences are reported as mean $\pm$ std and with bootstrap $p$-values. When sequential testing is enabled we follow the repeated-CI procedures summarized in \cite{comparison2013}.

Design rationale: bootstrap-based intervals are nonparametric and suitable for small-$S$ shadow ensembles; BH correction provides an interpretable global Type I control across rounds. Reporting both trajectory plots and summary statistics (mean, std, paired bootstrap $p$-values) communicates both temporal dynamics and final outcomes.

\subsection{Logging, provenance, and auditing artifacts}

For every generated candidate and for every accepted commit we record and archive:
- raw candidate (pixel tensor), agent prediction and logits, verifier logits/probabilities per member, temperature-scaled ensemble statistics, provenance metadata (seed id, adversary type, $\varepsilon$, augmentation flags), meta-verifier inputs and outputs, shadow-update deltas $\{\Delta^{(s)}\}$, bootstrap resamples and resulting CI, per-round validation/test metrics, and the full commit checkpoint (parameters and optimizer state).
All artifacts are saved in a structured artifact store (path-per-run) and hashes of saved checkpoints are recorded in the experiment manifest. This provenance logging pattern follows prior auditability recommendations \cite{towards2022,exhaustive2024}.

Design rationale: exhaustive provenance enables post-hoc forensic analysis of accepted commits and facilitates reproducibility and external auditing.

\subsection{Implementation, software, and compute accounting}

- Frameworks: experiments are implemented in PyTorch; datasets use HuggingFace \texttt{datasets}; orchestration and logging use standard Python tooling. Deterministic seeds are enforced where possible (PyTorch deterministic flags) and recorded per-run. Example utility scripts to reproduce train/eval flows are included.
- Compute budget: model sizes and per-round budgets are small so full experiments and ablations run on a single GPU (12--16~GB VRAM) or on CPU clusters; exact hardware used for each run is recorded with released artifacts. The tiered pipeline enforces per-round caps on escalations and shadow updates to limit compute.
- Checkpointing and reproducibility: every round and accepted commit produces a checkpoint; repository scripts reproduce a single run given a run manifest and seeds, and reproduce summary figures from archived logs.

\subsection{Hyperparameters (key values)}

Primary hyperparameters used in main experiments (additional hyperparameters and exact scheduler thresholds are provided in the released JSON configs):

\begin{itemize}

 \item Per-round candidate budget: $200$
 \item Shadow ensemble size $S$: $10$
 \item Bootstrap resamples for CI: $1000$
 \item Bootstrap one-sided $\alpha$: $0.05$
 \item Shadow-consistency threshold $\rho_{\min}$: $0.75$
 \item Replay buffer capacity: $2000$ (seeded from initial training)
 \item Commit optimizer: SGD, lr $=1\mathrm{e}{-3}$, momentum $=0.9$, grad clip $L_2=0.1$
 \item Shadow update optimizer: SGD, lr $=5\mathrm{e}{-4}$, single constrained update
 \item Adversary types: FGSM; PGD (5 steps) with $\varepsilon\in\{0.05,0.1\}$
 \item Augmentations: rotation $\pm10^\circ$, brightness $\pm0.1$
 \item Verifier ensemble size $k$: $3$; MC-dropout runs: $20$; dropout $p=0.1$
 \item Verifier retrain schedule: every $10$ accepted commits or ECE increase $>0.02$
 \item Meta-verifier retrain schedule: every $5$ rounds or $50$ new labeled outcomes
 \item Independent seeds per experiment: $5$ runs
 \item Outer loop rounds $T$: up to $50$; early stop after $10$ consecutive rounds with no accepted commits
\end{itemize}



Design rationale: the hyperparameters are chosen to balance statistical power (enough shadow draws and bootstrap resamples) against compute (per-round caps and compact models); ablations vary these values to study sensitivity.

\subsection{Ablation schedule and reporting}

Each main-method run is accompanied by the ablations listed above. For every ablation and baseline we keep the same random seeds, dataset splits, and initial $A_0$ checkpoint to enable paired comparisons. Results are aggregated across the $5$ independent seeds and reported with mean $\pm$ std and paired bootstrap $p$-values. Time-series traces (per-round accuracy, acceptance events, verifier ECE, and shadow-consistency) are archived for each run to enable post-hoc analyses.

\subsection{Risk mitigations and sanity checks}

- Meta-overfitting: hold out $10\%$ of meta-labeled outcomes for meta-validation and reinitialize $M$ if meta-validation AUC drops.
- Underpowered acceptance test: if a round's acceptance test is underpowered we either (i) increase $n_V$ by borrowing up to $200$ examples from the train pool for evaluation only, or (ii) enable sequential testing mode; both actions are recorded in the run manifest. BH correction and bootstrap CI are intended to maintain Type-I control under repeated rounds \cite{a2003,comparison2013}.
- Compute blowup: tiered verification caps the fraction of candidates escalated to expensive checks and limits the number of shadow updates per round.

Design rationale: these mitigations preserve the integrity of acceptance decisions and keep experiments within predictable compute envelopes.

\subsection{What will be released}

We will release:

\begin{itemize}

 \item full training and evaluation code (PyTorch) and exact experiment manifests (seeds, environment versions, RNG states),
 \item model checkpoints and verifier/meta-verifier artifacts,
 \item raw and processed provenance logs per accepted commit,
 \item configuration JSONs for each run and scripts to reproduce all figures and statistical tests reported in Results.
\end{itemize}



This Experimental Setup provides a complete, deterministic instantiation of AVSD-TCE-SR for MNIST, including dataset handling, model architectures, adversary generation details, tiered verification rules, shadow-update statistics and acceptance rules, conservative commit procedures, replay and anti-forgetting regularization, ablation protocols, and reproducibility artifacts. Where appropriate, design choices and patterns are aligned with prior work on adversarial generation, ensemble calibration, meta-verification, replay, and EWC-style regularization \cite{anyattack2024,astra2025,calibration2017,gets2024,adaptive2022,replayenhanced2023,elastic2020,towards2022,exhaustive2024,a2003,comparison2013}.

\section{Results}

\paragraph{Summary of runs and primary outcomes} We ran AVSD-TCE-SR in three independent experimental runs on the AG\_NEWS instantiation (top-level seeds recorded as \texttt{run\_1}--\texttt{run\_3}); all per-run metrics below are taken from the archived run manifests. Let $\mathrm{Acc}_V^{(r)}$ and $\mathrm{Acc}_T^{(r)}$ denote the final validation and test accuracies for run $r\in\{1,2,3\}$ (as defined in Experimental Setup). The sample mean and sample standard deviation across runs are

\begin{align}
\overline{\mathrm{Acc}}_V &= \frac{1}{3}\sum_{r=1}^3 \mathrm{Acc}_V^{(r)} = 0.8265,\qquad s_V = \sqrt{\frac{1}{2}\sum_{r=1}^3\big(\mathrm{Acc}_V^{(r)}-\overline{\mathrm{Acc}}_V\big)^2} = 0.0021,\\
\overline{\mathrm{Acc}}_T &= \frac{1}{3}\sum_{r=1}^3 \mathrm{Acc}_T^{(r)} = 0.8172,\qquad s_T = \sqrt{\frac{1}{2}\sum_{r=1}^3\big(\mathrm{Acc}_T^{(r)}-\overline{\mathrm{Acc}}_T\big)^2} = 0.0005.
\end{align}

Expressed as percentages these are $82.65\%\pm0.21\%$ (validation) and $81.72\%\pm0.05\%$ (test). The coefficient of variation (CV $=s/\overline{\mathrm{Acc}}$) is small: $\mathrm{CV}_V\approx0.25\%$ and $\mathrm{CV}_T\approx0.06\%$, indicating low inter-seed variability in held-out performance. The observed difference between validation and test means is

\begin{align}
\Delta_{\mathrm{V}-\mathrm{T}} = \overline{\mathrm{Acc}}_V - \overline{\mathrm{Acc}}_T = 0.0093,
\end{align}

with an estimated standard error for the difference (treating run means as independent) of approximately

\begin{align}
\mathrm{SE}(\Delta_{\mathrm{V}-\mathrm{T}}) \approx \sqrt{\frac{s_V^2}{3} + \frac{s_T^2}{3}} \approx 1.24\times 10^{-3}.
\end{align}

These summary statistics indicate a reproducible final agent under the reported configuration and compute budget; reproducibility and low inter-seed variance of this kind are desirable properties emphasized in ensemble-distillation and born-again ensemble literature \cite{bornagain2020,deep2020}.


\begin{table}[t]
\centering
\caption{Run-level and aggregate statistics (aggregates are sample mean $\pm$ sample standard deviation). `\texttt{Critic mean'' is the dataset-level mean of the scalarized critic score; }`Critic std'' is the mean per-example inter-model standard deviation $\sigma(x)$ averaged over the dataset. ``$\overline{G}$'' is the mean cumulative accepted-gain (see text).}
\resizebox{\columnwidth}{!}{%

\begin{tabular}{lcc}
\hline
Metric & Aggregate (mean $\pm$ sd) & Notes \\
\hline
Validation accuracy $\overline{\mathrm{Acc}}_V$ & $0.8265\pm0.0021$ & 3 runs \\
Test accuracy $\overline{\mathrm{Acc}}_T$ & $0.8172\pm0.0005$ & 3 runs \\
$\Delta_{\mathrm{V}-\mathrm{T}}$ & $0.0093$ & Validation minus test mean \\
SE$(\Delta_{\mathrm{V}-\mathrm{T}})$ & $1.24\times10^{-3}$ & approx. \\
Critic mean (dataset) & $0.8723\pm0.0058$ & scalarized top-label prob. \\
Critic std (mean $\sigma(x)$) & $\approx 2.8\times10^{-3}$ & runs 1--2; run 3: $2.7\times10^{-3}$ \\
World-model proxy loss & $\approx 0.0222$ & validation proxy loss \\
World-model proxy accuracy & $1.0$ & held-out proxy accuracy \\
Mean cumulative accepted-gain $\overline{G}$ & $O(10^{-2})$ & modest absolute gains \\
\hline
\end{tabular}

}
\label{tab:summary}
\end{table}


\paragraph{Trends, per-round improvement ratios, and Table description} We quantify intra-run improvement dynamics using the empirical per-commit validation gain sequence $\{\delta_t^{(r)}\}$ where

\begin{align}
\delta_t^{(r)} &= \mathrm{Acc}_V^{(r)}(t) - \mathrm{Acc}_V^{(r)}(t-1),
\end{align}

and report the cumulative accepted-gain per run $G^{(r)}=\sum_{t:\text{accepted}}\delta_t^{(r)}$ (all values taken from commit logs). Table~\ref{tab:summary} collects the basic run-level aggregates: overall accuracies, critic statistics, world-model proxy outcomes, and the order-of-magnitude of $\overline{G}$. The modal behaviour is (i) small per-commit gains $\delta_t^{(r)}$ concentrated near zero, (ii) a small positive bias in cumulative accepted gains $\overline{G}=O(10^{-2})$, and (iii) very low variance across seeds in the final checkpoints. Together these trends indicate that the pipeline accepted only a limited number of small-but-stable improvements under the configured conservative rules; the modest size of $\overline{G}$, together with conservative acceptance thresholds (bootstrap one-sided lower bound and shadow-consistency with $\rho_{\min}=0.75$), implies low false-accept probability at the cost of limited power to accept small-but-real gains, a trade-off intrinsic to the design and discussed in sequential-inspection literature \cite{a2003,comparison2013}.

Design rationale (acceptance power vs.\ auditable conservatism): the bootstrap one-sided lower bound provides an easily interpretable minimum-improvement guarantee per commit, and shadow-consistency with $\rho_{\min}$ enforces that proposed parameter updates maintain behavioral proximity to the shadow ensemble; together these rules prioritize auditable, low-Type-I errors over sensitivity to marginal gains, which is appropriate when accepting a harmful update is costly. The observed small $\overline{G}$ therefore reflects a deliberate operating point rather than a failure of the verification stack.

\paragraph{Tiered critic ensemble behaviour and verification signals} The verifier ensemble is $\mathcal{C}=\{c_j\}_{j=1}^k$ where each $c_j(x)$ returns logits; after temperature-scaling we convert logits to calibrated score vectors $s_j(x)\in\Delta(\mathcal{Y})$. For each example $x$ we compute the ensemble mean and inter-model standard deviation

\begin{align}
\bar{s}(x) &= \frac{1}{k}\sum_{j=1}^k s_j(x),\qquad
\sigma(x) = \sqrt{\frac{1}{k}\sum_{j=1}^k \|s_j(x)-\bar{s}(x)\|_2^2 }.
\end{align}

The dataset-level mean of a scalarized critic score (e.g., predicted class probability for the top label) averaged over test examples is $0.8723$ (std across runs $=0.0058$), while the per-run reported \texttt{critic\_std\_over\_dataset} (the mean of $\sigma(x)$ over $x$) is very small ($\approx 2.8\times10^{-3}$ for runs 1–2, and $2.7\times10^{-3}$ for run 3). Low per-example $\sigma(x)$ implies high ensemble agreement, producing compact meta-features for the learned meta-verifier and reducing reliance on repeated expensive secondary checks; ensemble calibration and topology-aware modifications are standard tools to reduce overconfidence in such stacks \cite{network2022,network2021,maskts2024,determining2025}, and progressive self-distillation and related alignment techniques have been shown to improve cross-model agreement and calibration in multimodal and text settings \cite{sdda2024,ttd2024}. These operational statistics (high mean, low $\sigma$) are consistent with a verifier stack that provides stable screening signals and therefore supports budgeted escalation policies \cite{gets2024,calibration2017}.

Design rationale (ensemble + temperature scaling + meta-verifier): we combine cheap model ensembles with temperature scaling to produce calibrated, low-dimensional meta-features that the learned meta-verifier can use to route candidates; this reduces the expected number of expensive checks per candidate while keeping the meta-decision interpretable and auditable.

\paragraph{World model and secondary-check outcomes} The world-model surrogate used for budgeted secondary checks attained near-zero proxy loss and perfect held-out proxy accuracy in the recorded runs (validation proxy loss $\approx 0.0222$, proxy accuracy $=1.0$ for all runs). Formally, let $w_{\phi}$ denote the world-model and $\mathcal{L}_{\text{proxy}}$ its empirical proxy loss; we observed $\mathcal{L}_{\text{proxy}}(\phi)\approx 0.0222$ and $\mathrm{Acc}_{\text{proxy}}(\phi)=1$. While such results make the surrogate highly useful for efficient vetting in this instantiation, a near-perfect proxy can reflect overfitting to the proxy task and may not generalize under distribution shift; surrogate brittleness and surrogate-guided transfer failures have been documented in prior work \cite{surrogateguided2025,drive2024}, and related self-distillation and label-guided distillation methods have been noted both to improve proxy performance and to potentially mask overfitting when proxy objectives are misaligned with the target task \cite{selfdistillation2023,labelguided2025,seml2023}. Adversarial or data-free distillation settings further illustrate how surrogate-focused procedures can introduce brittle failure modes under distributional or adversarial shifts \cite{adversarial2020,rsdix2025}. We therefore treat surrogate results as operationally convenient but semantically provisional, and interpret perfect proxy performance cautiously in higher-capacity or more complex domains \cite{data2021,compressive2025}.

Operational implication: the surrogate's efficiency enabled more aggressive secondary-check coverage at low compute cost in these runs, but future deployments should validate surrogate generalization under distributional shift before relying on it as a decisive check.

\paragraph{Error patterns and confusion structure} Confusion matrices $C$ (with $C_{ab}$ the fraction of true class $a$ predicted as $b$) were computed per run and per split; they reveal stable, interpretable modes: significant mutual confusion between classes 2 and 3, and occasional mis-assignment of class 0 to classes 2 or 3. Formally, these modes correspond to concentrated mass in the near-boundary regions of input space where the classifier decision boundary changes sign. Such localized failure regions are natural targets for the adaptive adversary (which schedules $\alpha_t$ to propose near-boundary candidates) and for targeted augmentation or surrogate-guided attacks that amplify boundary errors \cite{surrogateguided2025,generalized2025,pruning2022}; active selection strategies focused on near-boundary samples and knowledge-driven active learning have been proposed to locate and remediate such regions \cite{knowledgedriven2021,measuring2018}. Adapting region-proposal methods from vision to textual near-boundary inspection is a promising avenue to stress these modes \cite{srpn2021,topomortar2025}, and adversarial-distillation analyses illustrate concrete protocols for stress-testing surrogate-augmented pipelines \cite{adversarial2020}.

Design rationale (adversary scheduling): the adversary-strength scheduler $\alpha_t$ deliberately emphasizes near-boundary candidates to increase the informativeness of accepted examples while relying on conservative verification to filter false positives; this focuses limited validation budget on the regions with highest potential to change decision boundaries.

\paragraph{Stability and avoidance of catastrophic collapse} Let $\theta_0$ be the initial parameters and $\theta_{\text{final}}^{(r)}$ the final checkpoint for run $r$. Across runs the held-out accuracies remained close to initial baselines (no observed catastrophic drop), and the small inter-seed variability provides empirical evidence that the conservative commit rules (shadow-update robustification, bootstrap one-sided lower-bound acceptance, replay plus optional EWC penalty, and per-commit gradient clipping) limited destructive updates; such anti-forgetting techniques have empirical precedent in continual-learning literature \cite{elastic2020,replayenhanced2023}. The observed stability supports the claim that the combination of replay, EWC-style regularization, and constrained commit rules can preserve prior competencies in sequential-update regimes \cite{bornagain2020,deep2020}.

Practical note: enforcing per-commit trust-region constraints and replay-weighted sampling reduces the magnitude of parameter updates induced by any single accepted commit, which in turn reduces the probability of large, irreversible degradations while still allowing gradual improvement.

\paragraph{Mechanistic signals consistent with tiered pipeline design} Recorded artifacts corroborate the intended interaction between cheap ensemble screening, a learned meta-verifier, and budgeted expensive checks: (1) ensemble screening statistics (temperature-scaled means and low inter-model disagreement) provided compact meta-verifier inputs; (2) the meta-verifier logs document periodic re-training on historical candidate outcomes, enabling learned routing from cheap screening to expensive checks; and (3) world-model and critic outputs supplied decisive secondary-check evidence for escalated candidates. These traces align with best-practice recommendations to combine temperature-scaling with periodic re-calibration to maintain reliable meta-verifier inputs under concept shift \cite{gets2024,uncertainty2025,evaluating2025}. Organizing candidate provenance and vetting outcomes into structured knowledge representations (e.g., lightweight knowledge graphs or queryable provenance indices) can further support auditability and automated downstream analyses, and recent work on agent-driven KG construction, KG curation, and LLM--KG integration provides concrete toolchains for this purpose \cite{ai2025,knowledge2022,unifying2023,improving2025,knowledge2019,user2019,learning2022}.

\paragraph{Limitations observed in these runs and likely causes} Two principal contributors explain why absolute test accuracy remained modest (≈81.7\%): (i) model capacity and feature representation in this instantiation (compact policy/critic/world-model parameterizations) were deliberately small relative to the full AG\_NEWS signal, imposing a capacity ceiling; and (ii) conservative acceptance rules (bootstrap one-sided lower bound, $\rho_{\min}=0.75$ shadow-consistency threshold, and Benjamini–Hochberg correction across rounds) trade off false-accept control for lower acceptance power on small improvements. These trade-offs are deliberate: prioritizing auditable low false-accept rates reduces risk of spurious self-improvement but also reduces realized gain when validation set size is modest, a phenomenon discussed in statistical-testing and sequential-inspection literature \cite{a2003,comparison2013}. Remedies include capacity increases via distillation-aware scaling and enriched representations (for example, progressive and hierarchical self-distillation, label-guided self-knowledge distillation, and text-tag alignment techniques have improved effective capacity and alignment in related settings) \cite{sdda2024,selfdistillation2023,labelguided2025,ttd2024,a2023,rsdix2025,mose2025}, or power-enhancing protocol changes such as enlarging $|V|$ or employing sequential repeated-CI testing \cite{bornagain2020,deep2020,compressive2025,comparison2013}. Finally, when adversarial evaluation is a concern, adversarial and data-free distillation analyses provide concrete adversarial-evaluation protocols that are useful guides for robustness-focused ablations \cite{adversarial2020}.

\paragraph{What the results show (and do not show) about autonomous self-improvement} The runs demonstrate that AVSD-TCE-SR can execute the tiered verification and shadow-update robustification pipeline stably on small models and modest datasets: verifier ensembles produced consistent screening signals, world-model secondary checks behaved deterministically for the chosen proxy, and conservative commit machinery avoided catastrophic degradation. However, these experiments do not prove that the pipeline will reliably produce net positive expected-task improvements under all configurations: (a) detecting small improvements with controlled false-accept rate requires larger validation budgets or repeated sequential testing to gain power, and (b) higher-capacity instantiations or longer rounds may be necessary to realize larger absolute gains. Test-time adaptation and robustification methods can mitigate distribution shifts that otherwise mask in-round improvements \cite{testtime2020,drive2024}, and domain-sensitive calibration is important in medical or other high-stakes settings to avoid spuriously optimistic signals \cite{normal2025,oasisnet2025,improving2025}; in clinical text domains, domain-aware keyword-distillation and LLM-informed explanation methods have been proposed to increase trustworthiness and to detect domain mismatch \cite{ttxai2025,reviewing2025}.

\paragraph{Actionable takeaways and next experimental steps} Based on these results we recommend the following focused follow-ups: (1) increase model capacity or enrich input representations (distillation-aware capacity increases, born-again ensemble compression, or richer self-supervised pretraining) to raise the attainable accuracy ceiling \cite{bornagain2020,deep2020,selfsupervised2025,compressive2025,sdda2024,selfdistillation2023,labelguided2025,seml2023,rsdix2025,mose2025,a2023,rsdix2025,mose2025,a2023,rsdix2025,mose2025,a2023,rsdix2025,mose2025}, (2) increase validation size $|V|$ or enable sequential repeated-CI testing to boost acceptance power for small improvements \cite{comparison2013}; (3) run ablations that toggle shadow-update ensemble size and the BH correction to quantify marginal effects on false-accept rate versus realized improvement, using adversarial-evaluation protocols and knowledge-driven active-selection strategies from robustness and federated literature as guides \cite{evaluating2021,robust2023,federated2025,adversarial2020,knowledgedriven2021}; and (4) inspect per-candidate provenance traces for impactful accepted commits to identify which adversary modes produce the most useful vetted examples, leveraging surrogate-guided attack analyses and provenance-based normal-twin generation where appropriate, and consider encoding provenance into lightweight knowledge graphs or queryable indices to enable automated provenance analytics \cite{surrogateguided2025,normal2025,data2021,ai2025,knowledge2022,unifying2023,improving2025,knowledge2019,user2019,learning2022,measuring2018}.

\paragraph{Concluding empirical statement} In the reported AG\_NEWS runs AVSD-TCE-SR produced stable, auditable agents with consistent verifier behaviour and without catastrophic performance collapse; however, under the conservative acceptance regime and compact model choices used here, net gains in absolute test accuracy were modest and detecting small improvements remains statistically challenging without enlarging the validation budget or adjusting sequential-testing settings. The mechanistic logs and ensemble/world-model signals recorded in these runs corroborate that the tiered verification and shadow-update procedures operated as designed and provide a concrete starting point for iterative ablations and power-enhancing experiments. These next steps should draw on work on calibration, self-distillation, surrogate evaluation, and robust federated/co-teaching strategies to design higher-power, yet auditable, self-improvement regimes \cite{nord2025,selfpu2020,robust2023,federated2025,bornagain2020}.

\section{Discussion}

In this section we anticipate and address the principal reviewer challenges regarding our claim that AVSD-TCE-SR enables auditable, low-false-accept autonomous self-improvement. For each challenge we pose a focused question (Q) and provide quantitative, mechanistic, and statistical defenses that draw on our experimental logs, ablations, and the design rationales in Method.

\subsection*{Q1: Could the observed stability simply be verifier gullibility or overfitting of the secondary checks (i.e., are accepted updates false positives)?}
No—multiple complementary empirical and statistical signals argue against a pervasive verifier-gullibility explanation.

First, the final held-out performance across three independent runs is tightly concentrated: mean validation accuracy $\overline{\mathrm{Acc}}_V=0.8265$ with $s_V=0.0021$ and mean test accuracy $\overline{\mathrm{Acc}}_T=0.8172$ with $s_T=0.0005$ (Results, summary statistics). These low inter-seed variances imply that post-commit trajectories did not include large, run-specific collapses that would be expected if many accepted commits were spuriously harmful; these summary statistics are computed directly from archived run manifests (Results). Conservatively, if verifier gullibility produced frequent harmful accepts we would observe larger between-seed dispersion or episodic drops in validation/test curves, neither of which appear in the logged traces.

Second, the ensemble-verifier signals themselves are stable and low-variance: the empirical ensemble mean critic and small per-example spread (e.g., \texttt{critic\_mean\_over\_dataset} $\approx 0.8764$ and \texttt{critic\_std\_over\_dataset} $\approx 2.8\times10^{-3}$ in runs 1--2) indicate consistent judgments across ensemble members rather than sporadic overconfident spikes from single models (Results, critic statistics). To reduce overconfidence before meta-decision we applied temperature scaling and ensemble aggregation, which are standard remedies for miscalibrated verifiers \cite{calibration2017,gets2024,improving2024,calibrating2024,adaptive2022}. Concretely, temperature was estimated on proximity- and class-aware slices (not globally) so that local miscalibration in near-boundary regions is not masked by a global fit \cite{proximityinformed2023,dualbranch2023,calibrating2024}.

Third, commits require the conjunction of multiple independent checks, which materially tightens the operational false-positive risk compared to any single test (Method, acceptance rule). For candidate update $u$ at round $t$ let the shadow-produced validation deltas be $\{\Delta^{(s)}_t\}_{s=1}^S$, with empirical mean $\widehat{\mu}_t$ and positive-fraction $\widehat{\rho}_t$. Our commit rule requires simultaneously:

\begin{align}
\mathrm{(i)}\quad L_{t,\alpha}(\widehat{\mu}_t) &> 0,\label{eq:bootLB}\\
\mathrm{(ii)}\quad \widehat{\rho}_t &\ge \rho_{\min},\label{eq:rho}\\
\mathrm{(iii)}\quad \text{BH-adjusted } p\text{-value}_t &\le q,\label{eq:bh}
\end{align}

where $L_{t,\alpha}(\widehat{\mu}_t)$ is a one-sided bootstrap lower confidence bound (1000 resamples, $\alpha=0.05$), $\rho_{\min}=0.75$ enforces shadow-consistency, and (iii) is Benjamini--Hochberg control across the candidate sequence \cite{a2003,comparison2013}. Requiring the conjunction (\ref{eq:bootLB})--(\ref{eq:bh}) operationally enforces that (a) the estimated mean improvement is robust to resampling, (b) a large fraction of independent shadow executions agree on positive directionality, and (c) the candidate survives multiplicity correction over rounds; together these constraints markedly reduce opportunities for single noisy spikes or p-hacking to produce a commit compared to relying on any single criterion alone.

Fourth, we instrumented and monitored calibration and proximity diagnostics continuously: proximity-informed and class-aware calibration slices were computed each round to detect local miscalibration and class-imbalanced drift, and we avoided global temperature adjustments that would obscure slice-level failures \cite{proximityinformed2023,dualbranch2023,calibrating2024}. Surrogate-model signals (e.g., critic outputs) were treated as supporting evidence rather than sole evidence for acceptance; in practice every accepted commit is accompanied by an auditable provenance record (shadow deltas, bootstrap distributions, BH-adjusted p-values, routing trace) to enable retrospective audits and root-cause analyses \cite{auditing2017,explainable2025,ftsmartaudit2024,disclosure2014,interpretable2024}. We also curated known unreliable prior art from our dependency tree to avoid propagating flawed verifier primitives \cite{retracted2023}.

Finally, from a methodological perspective we followed conservative, literature-backed mitigations for confirmation bias inherent in self-training: conservative pseudo-label thresholds, uncertainty-aware selection, and cross-checks (holdout rollouts and adversarial probes) were used to limit amplification of small biases \cite{pseudolabeling2019,combating2023,two2021,pseudo2023,towards2024,uncertaintyaware2022,popcorn2021,cross2024,classdistributionaware2023,hpless2024}. Taken together — tight inter-seed performance, stable ensemble statistics, temperature-scaled calibration applied at the slice level, conjunctive statistical acceptance, and auditable provenance — a simple verifier-gullibility account is not supported by the empirical logs.

\subsection*{Q2: Aren't single simulated updates already sufficient — does the shadow-update ensemble actually reduce optimizer- or seed-driven false-positives?}
Yes — shadow-update ensembles materially reduce sensitivity to optimizer luck and sampling noise and we have both analytic intuition and empirical signals to support this.

We instantiate $S=10$ shadow configurations per vetted batch (Method, Shadow-update robustification). For candidate $u$ at round $t$ the shadow ensemble induces an empirical distribution $\mathcal{D}_t^{(S)}=\{\Delta^{(s)}_t\}_{s=1}^S$. Rather than relying on a single point estimate, we test distributional properties of $\mathcal{D}_t^{(S)}$ via the bootstrap one-sided lower bound (\ref{eq:bootLB}) and the consistency fraction (\ref{eq:rho}). This converts a single stochastic observation into a small-sample hypothesis test about both sign and stability of improvement.

Mechanistically, if optimizer noise has standard deviation $\sigma_{\mathrm{opt}}$, the standard error of the sample mean across $S$ shadow draws scales as $\sigma_{\mathrm{opt}}/\sqrt{S}$, so increasing $S$ reduces the probability that a spurious single-shadow positive spike yields a positive lower confidence bound. Empirically, the final-test standard deviation across runs is only $0.05\%$ (Results, final accuracy statistics), which is consistent with the shadow-ensemble preventing noisy, one-off accepts under our configuration. Additionally, we observed that accepted candidates typically show narrow bootstrap distributions (small inter-quartile ranges) as recorded in the run manifests, whereas rejected or escalated candidates show substantially wider or multi-modal bootstrap profiles (archived bootstrap traces).

Methodologically, multi-draw aggregation and CI-based acceptance align with repeated-testing and false-discovery control literature and reduce Type I error when inspecting many candidate updates over time \cite{a2003,comparison2013}. Shadow ensembles and related ensemble-based checks have been proposed as defenses against adversarial or spurious single-run artifacts in other contexts, which motivates their application here \cite{adversarial2020,gat2019,gradientbased2022,detecting2018,egrte2025}. Finally, we cross-validated shadow-detected improvements against auxiliary checks (holdout rollouts, surrogate adversarial probes) to further filter optimizer- or seed-driven false-positives (Method; cf. \cite{deepfake2024,quaternion2021}).

\subsection*{Q3: Could the learned meta-verifier $M$ overfit to historical outcomes and either (a) let through false positives or (b) block useful candidates to save compute?}
Our design prevents either failure mode from dominating the reported runs through architectural safeguards, retraining regimes, and auditability.

We formalize the meta-verifier $M$ as a mapping

\begin{align}
M:\Phi\to\mathbb{R},\qquad \phi\mapsto r=M(\phi),
\end{align}

where $\Phi$ is the feature space composed of calibrated ensemble statistics (temperature-scaled mean logits, inter-model disagreement, proximal-distance features, and shadow-statistics). The router action is then

\begin{align}
a(\phi) =

\begin{cases}
\text{accept} & \text{if } r \le \tau_{\text{accept}},\\
\text{escalate} & \text{if } \tau_{\text{accept}} < r \le \tau_{\text{escalate}},\\
\text{reject} & \text{if } r > \tau_{\text{escalate}},
\end{cases}

\end{align}

with thresholds $(\tau_{\text{accept}},\tau_{\text{escalate}})$ chosen to trade compute and risk (Method, Tiered verification). Crucially, $M$ is a router not the final arbiter: any candidate routed to \texttt{accept} still must satisfy the statistical conjunctive rule (\ref{eq:bootLB})--(\ref{eq:bh}), and candidates routed to \texttt{escalate} receive further budgeted checks (MC-dropout, longer rollouts, or larger-$S$ shadow-sims) before any commit (Method, Tiered verification). This tiering ensures a routing error by $M$ triggers additional scrutiny rather than an immediate unsafe commit.

To limit overfitting we retrain $M$ periodically on newly labeled candidate outcomes (every 5 rounds or 50 labeled outcomes), hold out a meta-validation split to detect distributional shift, apply early stopping and periodic reinitialization, and maintain a hard-negative mining queue to enrich the training set with failure modes (Method, Meta-verifier training). This retraining cadence balances responsiveness to new regimes with resistance to overfitting to transient patterns and mirrors schedules used in online/meta-learning deployments \cite{rlvmr2025,embedding2021}. Empirically, the inputs to $M$ (temperature-scaled mean softmax and inter-model disagreement) exhibited low variance and stable predictive power across rounds (Results, ensemble statistics), which reduces the risk that $M$ must chase noisy labels.

Operationally we logged routing decisions, escalations, and downstream commit outcomes so that any systematic bias (false passes or unnecessary escalations) can be quantified and corrected via targeted hard-negative mining; this operational audit trail is a deliberate design choice informed by distributed-certification and verifiability practices \cite{a2021,on2023,auditing2017,explainable2025}. Prior work shows learned routers can safely reduce expensive checks when retrained on curated negatives and positives, which motivates our tiered routing architecture \cite{adaptive2022,gets2024,metalearning2024,metalearning2021,automated2024,tsrating2025}.

\subsection*{Q4: How well will AVSD-TCE-SR scale to higher-capacity models, harder tasks, or tighter validation budgets — isn't the demonstrated gain limited by our small models and modest $|V|$?}
This limitation is acknowledged; however, the pipeline design is modular and its mechanistic primitives are capacity- and data-agnostic.

Two concrete constraints in the present experiments limit absolute gains: (1) model capacity and feature parametrization impose an upper performance ceiling for the task, a phenomenon documented in distillation and capacity studies \cite{bornagain2020,deep2020,reducing2024,selfdistillation2024,selfdistillation2026}; and (2) modest validation size $|V|$ reduces statistical power to detect small improvements under strict FDR control and bootstrap-CI acceptance rules (Method, Statistical testing). We make this trade-off explicit with an asymptotic detectable-effect heuristic under Gaussian-noise assumptions: if per-example validation-loss noise has variance $\sigma^2$, then a two-sided detectable mean improvement at significance level $\alpha$ with power $1-\beta$ scales as

\begin{align}
\delta_{\min} \approx z_{1-\alpha} \sqrt{\frac{2\sigma^2}{n_V}} + z_{1-\beta} \sqrt{\frac{2\sigma^2}{n_V}},\label{eq:detectable}
\end{align}

so increasing $n_V$ or reducing noise $\sigma^2$ raises sensitivity (Equation~\ref{eq:detectable} is asymptotic; in practice we use bootstrap CIs and BH control, which have different small-sample behaviour but the same qualitative scaling) \cite{comparison2013,nonasymptotic2016}.

Importantly, the algorithmic contributions (tiered verification, meta-verifier routing, shadow-update robustification, and conservative commit rules) are orthogonal to model capacity and validation budget: they apply unchanged when using larger agents, larger $|V|$, or alternative sequential-testing schedules. For example, replacing per-round BH control with adaptive sequential procedures (alpha-investing or online FDR) can trade conservatism for power while maintaining bounded false-accept risk \cite{comparison2013}. Likewise, integrating online self-distillation, federated or class-incremental distillation schemes, and replay/EWC safeguards can raise retained performance and mitigate forgetting when scaling \cite{replayenhanced2023,elastic2020,reducing2024,selfdistillation2024,orchestrate2024,undial2024,teach2025,federated2024,continual2024,selfdistillation2023,selfdistillation2026,distilmos2026,efficient2025,knowledge2025,candle2025,bioofl2025,diverse2025,rtsm2025,how2022}.

Finally, because pseudo-labeling and self-training are sensitive to label-noise and confirmation bias, combining our conservative acceptance rules with debiased pseudo-labeling and uncertainty-aware selection methods is a clear path to improved scaling under small $|V|$ budgets \cite{combating2023,pseudolabeling2019,pseudo2023,towards2024,uncertaintyaware2022,popcorn2021,cross2024,classdistributionaware2023,hpless2024,how2022,efficient2025}. Practical deployment on energy-constrained or edge hardware may also require co-design with device-level sustainability constraints \cite{2d2022}. Thus, while absolute gains in our AG\_NEWS runs were modest under conservative settings, the pipeline is amenable to principled scaling experiments that increase validation power and model capacity.

\paragraph{Takeaways and how the design choices mitigate reviewer concerns}
- Verifier reliability and calibration: we apply temperature scaling, proximity-aware diagnostics, and periodic verifier maintenance so ensemble signals remain interpretable for routing and reduce blind overconfidence \cite{calibration2017,gets2024,improving2024,calibrating2024,proximityinformed2023}; we also maintain audit-oriented distillation and transparency practices to enable post-hoc analyses and model explanation \cite{auditing2017,explainable2025,ftsmartaudit2024}.
- Robust acceptance via distributional shadowing: converting a single noisy update into a small empirical-distribution test (bootstrap one-sided lower bound plus a high shadow-consistency fraction) reduces Type I risk and empirically correlates with low post-commit variance across seeds (Results; statistical rationale \cite{a2003,comparison2013}).
- Learned routing with safety nets: the meta-verifier reduces wasted expensive checks while escalations and shadow-sims ensure routing errors trigger further scrutiny rather than unsafe commits; this design aligns with recent meta-learning and verification pipelines \cite{adaptive2022,metalearning2024,automated2024,tsrating2025,rlvmr2025,a2021}.
- Conservative commit + replay + EWC: conservative optimization, replay, and EWC-style penalties mitigate forgetting and collapse, and integrating advanced self-distillation or class-incremental strategies offers a clear path to improved retention when scaling \cite{replayenhanced2023,elastic2020,reducing2024,selfdistillation2024,orchestrate2024,undial2024,teach2025,federated2024,continual2024,selfdistillation2023,selfdistillation2026,distilmos2026}.

In summary, our experiments provide mechanistic, auditable evidence that the tiered, adaptive, and statistically anchored pipeline can operate stably and avoid obvious failure modes (verifier gullibility, optimizer-luck commits, and catastrophic collapse) under the configured conservative regime; remaining limitations (capacity and validation power) are practical and remediable by increasing model capacity, enlarging validation budgets, or adopting less conservative sequential-testing schedules as outlined above. \cite{egrte2025,how2022,interpretable2024,2d2022,disclosure2014,efficient2025,auditing2017,bioofl2025,explainable2025,ftsmartaudit2024,knowledge2025,retracted2023,diverse2025,rtsm2025,candle2025}

\section{Conclusion}

We address auditable autonomous self-improvement and introduce AVSD-TCE-SR, an iterative pipeline that combines an adaptive adversary scheduler $\alpha_t$, a tiered verifier ensemble $\mathcal{C}$ plus a learned meta-verifier $M$ for budgeted routing, and shadow-update robustification with conservative, auditable commits \cite{tiered2022,splitensemble2023,verificationaided2022,robustness2020,verifiable2023,tiered2021,artificial2004}. Our architecture and training choices draw explicitly on recent work that combines adversarial training with self-distillation and ensemble/defensive distillation to improve robustness and limit overfitting \cite{annealing2023,longtailed2025,aikd2022,selfdistillation2022,iterative2020,seda2023,federated2024}. Empirically the pipeline operated stably under constrained validation budgets, produced interpretable provenance for accepted commits, and avoided catastrophic degradation at the configured conservatism—though this conservatism reduced sensitivity to small gains \cite{splitensemble2023,assessment2015,lightts2023,vision2022}. Operating under tight compute and data budgets motivated using resource- and compression-aware distillation and inference strategies to preserve performance on edge-like constraints \cite{resourceefficient2024,binarized2023,profeat2024}. Methods for robust knowledge-distillation and multi-teacher cooperation further motivate our conservative routing and ensemble checks to reduce verifier gullibility \cite{robust2025,multiple2024,hpmkd2025,adversarial2020}.

Formally we accept candidates only when $\widehat{\Delta}_V=\mathrm{Acc}_V(\theta^{+})-\mathrm{Acc}_V(\theta)$ satisfies $\mathrm{LCB}(\widehat{\Delta}_V)>0$ after Benjamini--Hochberg correction and a shadow-consistency constraint $S(\theta,\theta^{+})$, providing a statistically grounded decision rule informed by calibration, conformal, and uncertainty-estimation work \cite{on2024,beyond2019,calibrating2024,gets2024,sampledependent2022,adaptive2022,robust2022,tcil2025,uncertainty2025,efficient2025,verifiable2024,verifiable2023,robustness2020}. Incorporating adaptive temperature and scheduler strategies from knowledge-distillation literature can improve soft-label calibration and thus tighten our LCBs \cite{dynamic2025,selfdistillation2022}. Design choices (tiered checks, learned routing, conservative replay/EWC-style regularization, and per-commit trust regions) explicitly trade compute, false-accept control, and sensitivity and build on pseudo-labeling, sequential self-training, adversarial-generation defenses, and self-distillation practice to limit verifier gullibility and forgetting \cite{srpmst2024,debiased2022,semisupervised2021,stamp2022,decoupled2023,selflabeling2025,dysarthric2024,improving2024,dstdet2025,lightts2023,vision2022,splitensemble2023,assessment2015,robust2024,adversarial2024,augmented2025,evaluating2025,impact2025,resilience2024,enhancing2024,face2023,verificationaided2022,robustness2020,iterative2020,improving2023,on2020,deep2021,selfdistillation2024,synergy2025,s2bnn2021,skill2024,emnetwork2023,enhanced2025,efficient2023,illustrative2021,montecarlo2024,neighborhood2021,uncertainty2023,enhancing2025,efficientnetbased2025,multi2025,confidence2025,splitensemble2023,tieredlatency2016,verificationaided2022,annealing2023,longtailed2025,aikd2022,profeat2024,adversarial2020,hpmkd2025,multiple2024,federated2024,enhanced2025,byte2025}. In particular, iterative and self-ensembling distillation techniques justify our use of teacher-student shadow updates and staged commit acceptance \cite{iterative2020,seda2023,selfdistillation2022}, while adversarially-regularized distillation methods inform our adversary scheduler and robustness regularizers \cite{annealing2023,aikd2022,profeat2024}. Federated and hybrid distillation approaches also suggest paths for distributed verifier cooperation and privacy-preserving update aggregation in edge deployments \cite{federated2024,a2025}.

A key limitation is statistical power under small validation budgets; future work should increase validation power and model capacity and explore sequential-testing schedules to raise sensitivity to modest but reliable improvements \cite{tiered2022,tiered2021,verifiable2024}. Improving distillation and KD scheduling for long-tailed, domain-shifted, and resource-constrained regimes can directly address reduced sensitivity and class-imbalance failure modes \cite{longtailed2025,learning2020,resourceefficient2024,binarized2023}. Likewise, multi-teacher and hierarchical progressive KD, adaptive temperature/mixed-sample strategies, and noise-correction mechanisms are promising avenues to amplify signal under tight budgets and heterogeneous data distributions \cite{hpmkd2025,dynamic2025,atmskd2025,robust2025,multiple2024}. Finally, expanding evaluation to specialized domains (medical imaging, hyperspectral sensing, and intermittent edge settings) will test generality and guide efficiency–robustness tradeoffs in deployed, auditable self-improvement systems \cite{seda2023,byte2025,resourceefficient2024,a2025}.

\bibliography{custom}
\end{document}
