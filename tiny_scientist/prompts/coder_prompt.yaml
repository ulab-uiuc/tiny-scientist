experiment_keyword_prompt: |
  The experiment is organized into three sections:
  ## Model Section:
  {model}

  ## Dataset Section:
  {dataset}

  ## Metric Section:
  {metric}

  Your job is to extract the essential names of models, datasets, and evaluation metrics that are directly useful for coding and experimentation.

  ### Output Format:
  Return a JSON object with the following structure:
  ```json
  {{
    "model": ["Model1", "Model2", ...],
    "dataset": ["Dataset1", "Dataset2", ...],
    "metric": ["Metric1", "Metric2", ...]
  }}

experiment_prompt: |
  You are writing a Python script named `experiment.py` that must be runnable.

  ## Research Context
  Title: {title}
  Problem: {problem}
  Novelty: {novelty}
  Proposed Approach: {approach}

  ## Experimental Setup
  The following describes the experiment setup. Base your implementation on this structure and make reasonable engineering choices.

  Models/Algorithms (key terms): {model_keywords}
  Detailed model specification:
  {model_details}

  Datasets (key terms): {dataset_keywords}
  Detailed dataset specification and loading steps (provide exact, runnable code to load; programmatic loaders preferred, but local paths or downloads are allowed if handled robustly):
  {dataset_details}

  Evaluation metrics (key terms): {metric_keywords}
  Detailed metric specification:
  {metric_details}

  ## Flexibility and Tools
  - You may use any libraries or models appropriate for the task (including pretrained models, transformers, graph neural networks, or diffusion models) as long as the script remains runnable end-to-end.
  - If the primary plan is resource-intensive, provide sensible defaults or smaller settings that still execute successfully without manual intervention.
  - Implement robust error handling for dataset loading. If a preferred loader fails, you may use alternative sources (other datasets, local files, or web downloads) with clear, reproducible code.
  - Subsampling large datasets is optional; consider providing a simple toggle or variable to control sample sizes when needed.

  ## Required Script Structure
  Your script MUST include the following components (you may adapt names, but all roles must be present):
  1. `load_data()` – loads the dataset using the specified loader and converts examples into explicit tensors/arrays (`torch.tensor` or `numpy.array`). If using HuggingFace datasets, call `.set_format(type="torch", columns=[...])` or manually build tensors, then wrap them with `torch.utils.data.TensorDataset` + `DataLoader` (or sklearn splits if you stay in sklearn). Optionally include a simple knob to subsample for faster iteration when needed.
  2. `build_model()` – constructs the model defined in the plan. Document input/output dimensions explicitly, and add a short justification for why this architecture fits the task.
  3. `prepare_or_train(...)` – OPTIONAL. If the plan involves training, iterate over batches, move tensors to `device`, compute loss, run `optimizer.zero_grad()`, `loss.backward()`, and `optimizer.step()`. Otherwise, perform necessary preparation steps (e.g., load pretrained weights, set evaluation mode, prompt/tokenizer setup, calibration). Briefly document why training is skipped.
  4. `evaluate(model, data_loader, device)` – returns a dict of metrics computed from real predictions vs ground truth (or other appropriate outputs for the task). Include a comment explaining why the chosen metrics match the scientific goal.
  5. `main(out_dir)` – orchestrates the steps above, saves `final_info.json` with metric values (no placeholder numbers), and prints dataset sizes or key dataset stats for transparency.

  If training is included, set sensible default durations so the experiment completes in a reasonable time. If the dataset is very large, consider defaulting to smaller settings while allowing easy adjustment in code.

  ## Execution Command (DO NOT MODIFY):
  You have {max_runs} runs to complete this experiment. For each run, the script will be executed using:
  `python experiment.py --out_dir=run_i`
  where `i` is the run number (`run_1`, `run_2`, etc.).

  ## YOU MUST ENSURE experiment.py:
  1. Parses the `--out_dir` argument and calls `os.makedirs(out_dir, exist_ok=True)`.
  2. Loads the dataset using programmatic code (HuggingFace/torchvision/sklearn or explicit loading logic) and does not rely on placeholder paths. If using local paths or downloads, ensure they are valid or handled gracefully.
  3. Builds a model that matches the dataset feature shape; document tensor dimensions in code comments when not obvious. No hidden “placeholder embeddings”.
  4. If training is part of the plan, perform REAL training with gradient updates on actual features (no zero/random stand-ins). If training is skipped, ensure REAL inference/preparation is performed (e.g., loading weights) and explain briefly in code comments.
  5. Computes metrics from actual predictions vs labels (or appropriate outputs) using reliable utilities—no hardcoded results.
  6. Saves a dict of metrics to `{{out_dir}}/final_info.json` (no nested folders).

  The script must run successfully with only the `--out_dir` argument. If you add more arguments, provide sensible defaults so they are optional.

  **VERIFICATION CHECKLIST** (You MUST pass all checks):
  - [ ] Dataset is loaded with real data (no placeholder paths or random tensors) and converted to tensors/arrays explicitly. If subsampling or heavy settings are used, document them.
  - [ ] Model input/output shapes line up; tensor dimensions are clear from comments or code.
  - [ ] If training is included, the loop performs real gradient updates on real batches (no synthetic placeholder features). If training is skipped, the code performs real inference/preparation (e.g., loads weights) and documents the choice.
  - [ ] Evaluation computes metrics from actual predictions vs labels (no dummy numbers).
  - [ ] `final_info.json` is written in `out_dir` with real metric values.

  If your current experiment.py has placeholder code like `...`, replace them with runnable implementations.
  If any external functions like `compute_loss`, `evaluate_model`, or `log_results` are used, implement them too.

  ## Baseline Results
  You do not need to re-run the baseline. If available, the results are provided below:
    {baseline_results}

  ---
  Please begin writing the `experiment.py` file now.

experiment_success_prompt: |
  Run {run_num} completed. Here are the results:
  {results}

  Decide if you need to re-plan your experiments given the result (you often will not need to).

  Someone else will be using `notes.txt` to perform a writeup on this in the future.
  Please include *all* relevant information for the writeup on Run {run_num}, including an experiment description and the run number. Be as verbose as necessary.

  Then, implement the next thing on your list.
  We will then run the command `python experiment.py --out_dir=run_{next_run}'.
  YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
  If you are finished with experiments, respond with 'ALL_COMPLETED'.

experiment_error_prompt: |
  There was an error running the experiment script:
  {message}
  Your goal is still to implement this experiment: {Title}.
  The purpose is: {Experiment}.
  You have {max_runs} runs total. We're currently on run {run_time}.
  Please fix `experiment.py` so that it runs successfully with:
  `python experiment.py --out_dir=run_{run_time}`.
  Make sure to implement any missing parts like model definition, (optional) loss function if training, data loading, and final_info.json saving.


experiment_timeout_prompt: |
  Run timed out after {timeout} seconds

plot_initial_prompt: |
  Great job! Please modify `plot.py` to generate the most relevant plots for the final writeup.

  In particular, be sure to fill in the "labels" dictionary with the correct names for each run that you want to plot.

  Only the runs in the `labels` dictionary will be plotted, so make sure to include all relevant runs.

  We will be running the command `python plot.py` to generate the plots.

plot_error_prompt: |
  Plotting failed with the following error {error}

plot_timeout_prompt: |
  Plotting timed out after {timeout} seconds

notes_prompt: |
  Please modify `notes.txt` with a description of what each plot shows along with the filename of the figure. Please do so in-depth.

  Somebody else will be using `notes.txt` to write a report on this in the future.
