experiment_plan_prompt: |
  You are a research experiment planner. Given a research idea, produce a concrete implementation checklist for `experiment.py`.

  ## Research Context
  Title: {title}
  Problem: {problem}
  Proposed Approach: {approach}

  ## Experimental Setup
  Model/Algorithm details:
  {model_details}

  Dataset details:
  {dataset_details}

  Evaluation metric details:
  {metric_details}

  Experiment blueprint table (authoritative; you must follow it):
  {experiment_table}

  ## Your Task
  Break down the implementation of `experiment.py` into 4–6 ordered steps. Each step should map to one or more rows in the experiment blueprint table and be directly implementable.

  Return ONLY a JSON array with no surrounding text, in this exact format:
  ```json
  [
    {{"step": 1, "name": "Data Loading", "description": "Concise instruction for what to implement in this step", "row_refs": ["Dataset", "Training Setup"]}},
    {{"step": 2, "name": "Model Architecture", "description": "...", "row_refs": ["Model Architecture"]}},
    {{"step": 3, "name": "Training Loop", "description": "...", "row_refs": ["Training Setup", "Hyperparameters"]}},
    {{"step": 4, "name": "Evaluation", "description": "...", "row_refs": ["Evaluation Metrics"]}},
    {{"step": 5, "name": "Main Orchestration", "description": "...", "row_refs": ["Sanity Checks"]}}
  ]
  ```

  Keep each description concrete and aligned to the table rows. `row_refs` must reference real row names from the blueprint table.

experiment_step_prompt: |
  You are implementing step {step_num} of {total_steps} for a machine learning experiment script.

  ## Step to Implement
  Step {step_num}: {step_name}
  {step_description}

  ## Research Context
  Title: {title}
  Problem: {problem}
  Proposed Approach: {approach}

  ## Experimental Setup
  Model/Algorithm details:
  {model_details}

  Dataset details:
  {dataset_details}

  Evaluation metric details:
  {metric_details}

  Experiment blueprint table (authoritative):
  {experiment_table}

  ## TODO Plan (from TODO.md)
  {todo_plan}

  ## Current experiment.py content
  {current_code}

  ## Hard Constraints (apply to all steps)
  - Lightweight architectures only: bag-of-words/logistic regression, shallow MLP (≤2 hidden layers, ≤128 units), shallow CNN (≤3 conv, ≤32 filters), linear SVM, single-layer GRU (≤64 units)
  - FORBIDDEN: heavyweight foundation-model classes, graph neural networks, diffusion models, pretrained checkpoints
  - Subsample datasets: ≤5,000 train, ≤2,000 val/test
  - Training: ≤5 epochs, batch_size ≤64
  - Script must accept --out_dir argument and save final_info.json there

  ## Instructions
  1. Read the current experiment.py using the read_file tool (if it exists)
  2. Implement ONLY the current step — do not skip ahead or leave placeholders
  3. Write the complete updated experiment.py using the write_file tool
  4. The file must remain runnable after each step (add pass stubs for functions not yet implemented)
  5. Follow TODO.md ordering strictly and do not work on unchecked future steps

  After writing, respond with "STEP_DONE".

experiment_keyword_prompt: |
  The experiment is organized into three sections:
  ## Model Section:
  {model}

  ## Dataset Section:
  {dataset}

  ## Metric Section:
  {metric}

  Your job is to extract the essential names of models, datasets, and evaluation metrics that are directly useful for coding and experimentation.

  ### Output Format:
  Return a JSON object with the following structure:
  ```json
  {{
    "model": ["Model1", "Model2", ...],
    "dataset": ["Dataset1", "Dataset2", ...],
    "metric": ["Metric1", "Metric2", ...]
  }}

experiment_prompt: |
  You are writing a Python script named `experiment.py` that must be runnable.

  ## Research Context
  Title: {title}
  Problem: {problem}
  Novelty: {novelty}
  Proposed Approach: {approach}

  ## Experimental Setup
  The following describes the experiment setup. You must base your implementation strictly on this structure and keep it lightweight.

  Models/Algorithms (key terms): {model_keywords}
  Detailed model specification:
  {model_details}

  Datasets (key terms): {dataset_keywords}
  Detailed dataset specification (use this loader exactly — do not invent file paths):
  {dataset_details}

  Evaluation metrics (key terms): {metric_keywords}
  Detailed metric specification:
  {metric_details}

  ## Hard Constraints
  - Use only lightweight architectures. Total parameter count should stay under ~100k.
  - ABSOLUTELY FORBIDDEN: large pretrained foundation-model backbones, graph neural networks, diffusion models, random placeholder features (`torch.rand`, `np.random`), manual placeholder paths, loading huge checkpoints.
  - Dataset must be loaded via the provided command exactly. If loading fails, stop and report the error; do not replace the dataset automatically. NEVER fall back to local files. For text, prefer simple vectorizers (`CountVectorizer`, `TfidfVectorizer`) over tokenizers.
  - Always subsample large datasets: cap training examples at ≤5,000 and validation/test at ≤2,000 each (use `select(range(...))` or appropriate slicing). Document this in code comments.

  ## Required Script Structure
  Your script MUST include the following components (you may adapt names, but all roles must be present):
  1. `load_data()` – downloads the dataset using the loader above, subsamples to the limits (≤5k train / ≤2k val/test), and converts examples into explicit tensors/arrays (`torch.tensor` or `numpy.array`). If using HuggingFace datasets, call `.set_format(type="torch", columns=[...])` or manually build tensors, then wrap them with `torch.utils.data.TensorDataset` + `DataLoader` (or sklearn splits if you stay in sklearn). Include a brief comment explaining why this preprocessing keeps the experiment lightweight.
  2. `build_model()` – constructs the lightweight model defined in the plan. Document input/output dimensions explicitly, and add a short justification for why this architecture is the simplest option that still addresses the task.
  3. `train(model, train_loader, optimizer, criterion, device)` – iterates over batches, moves tensors to `device`, computes loss, `loss.backward()`, `optimizer.step()`. Log the epoch count and explain in a comment why the number of epochs is sufficient.
  4. `evaluate(model, data_loader, device)` – returns a dict of metrics computed from real predictions vs ground truth. Include a comment explaining why the chosen metrics match the scientific goal.
  5. `main(out_dir)` – orchestrates the steps above, saves `final_info.json` with metric values (no placeholder numbers), and prints dataset sizes so we can verify subsampling.

  Keep training loops short (≤5 epochs, batch_size ≤64) so the experiment finishes quickly on CPU. If the dataset is extremely small, 3 epochs are usually enough.

  ## Execution Command (DO NOT MODIFY):
  Use a single execution target directory. The script will be executed using:
  `python experiment.py --out_dir=run`

  ## YOU MUST ENSURE experiment.py:
  1. Parses the `--out_dir` argument and calls `os.makedirs(out_dir, exist_ok=True)`.
  2. Loads the dataset using the specified loader with NO manual file paths.
  3. Builds a lightweight model that matches the dataset feature shape; document tensor dimensions in code comments when not obvious. No hidden “placeholder embeddings”.
  4. Performs REAL training with gradient updates (`optimizer.zero_grad()`, `loss.backward()`, `optimizer.step()`) on actual features (no zero/random stand-ins).
  5. Computes metrics from real predictions (e.g., accuracy/F1) using sklearn/torch utilities—no hardcoded results.
  6. Saves a dict of metrics to `{{out_dir}}/final_info.json` (no nested folders).

  ## Computational Constraints
  - Ensure the code is computationally affordable to run on a single GPU or CPU machine.
  - Avoid large model families or full-scale benchmark training runs.
  - Prefer small-scale tasks and low-cost benchmarks aligned with the thinker-provided blueprint.
  - Do not use complex distributed training or multi-GPU setups.

  Do not add extra command-line arguments.

  **VERIFICATION CHECKLIST** (You MUST pass all checks):
  - [ ] Dataset is downloaded via library loader (no placeholder paths, no mocks, no random tensors) and converted to tensors/arrays explicitly. Document any subsampling (≤5k train / ≤2k val/test).
  - [ ] Model is a lightweight architecture (no heavyweight pretrained backbones or graph models) and input/output shapes line up.
  - [ ] Training loop performs `optimizer.zero_grad()`, `loss.backward()`, `optimizer.step()` on real batches (no synthetic placeholder features).
  - [ ] Evaluation computes metrics from actual predictions vs labels (no dummy numbers).
  - [ ] `final_info.json` is written in `out_dir` with real metric values.
  - [ ] Sanity check: print or log dataset sizes and confirm they respect the limits; mention parameter count estimate in a comment.

  If your current experiment.py has placeholder code like `...`, replace them with runnable implementations.
  If any external functions like `compute_loss`, `evaluate_model`, or `log_results` are used, implement them too.

  ## Baseline Results
  You do not need to re-run the baseline. If available, the results are provided below:
    {baseline_results}

  ---
  Please begin writing the `experiment.py` file now.

experiment_success_prompt: |
  Run completed. Here are the results:
  {results}

  Decide if you need to re-plan your experiments given the result (you often will not need to).

  Someone else will be using `notes.txt` to perform a writeup on this in the future.
  Please include *all* relevant information for the writeup on this run, including an experiment description. Be as verbose as necessary.

  Then, implement the next thing on your list.
  If further fixes are needed, the command remains `python experiment.py --out_dir=run`.
  YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.

experiment_error_prompt: |
  There was an error running the experiment script:
  {message}
  Your goal is still to implement this experiment: {Title}.
  The purpose is: {Experiment}.
  Use the single run target `run`.
  Please fix `experiment.py` so that it runs successfully with:
  `python experiment.py --out_dir=run`.
  Make sure to implement any missing parts like model definition, loss function, data loading, and final_info.json saving.


experiment_timeout_prompt: |
  Run timed out after {timeout} seconds

plot_initial_prompt: |
  Great job! Please modify `plot.py` to generate the most relevant plots for the final writeup.

  In particular, be sure to fill in the "labels" dictionary with the correct names for each run that you want to plot.

  Only the runs in the `labels` dictionary will be plotted, so make sure to include all relevant runs.

  We will be running the command `python plot.py` to generate the plots.

plot_error_prompt: |
  Plotting failed with the following error {error}

plot_timeout_prompt: |
  Plotting timed out after {timeout} seconds

notes_prompt: |
  Please modify `notes.txt` with a description of what each plot shows along with the filename of the figure. Please do so in-depth.

  Somebody else will be using `notes.txt` to write a report on this in the future.
