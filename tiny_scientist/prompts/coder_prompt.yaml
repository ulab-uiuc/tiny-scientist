experiment_keyword_prompt: |
  The experiment is organized into three sections:
  ## Model Section:
  {model}

  ## Dataset Section:
  {dataset}

  ## Metric Section:
  {metric}

  Your job is to extract the essential names of models, datasets, and evaluation metrics that are directly useful for coding and experimentation.

  ### Output Format:
  Return a JSON object with the following structure:
  ```json
  {{
    "model": ["Model1", "Model2", ...],
    "dataset": ["Dataset1", "Dataset2", ...],
    "metric": ["Metric1", "Metric2", ...]
  }}

experiment_prompt: |
  You are writing a Python script named `experiment.py` that must be runnable.
  Apply Adaptive Prompt Decomposition principles for coherent long-range code and DO NOT TRAIN:
  - Lead with data structures and explicit interfaces; minimize conditionals.
  - Keep functions short, single-purpose, and <= 3 levels of indentation.
  - Prefer additive changes and stable boundaries; no placeholders like `...`.
  - Never break userspace behavior silently; if change is required, document migration.
  - Absolutely no training loops or fine-tuning; inference-only. If plan mentions training, ignore it and document that you are using inference.

  ## Research Context
  Title: {title}
  Problem: {problem}
  Novelty: {novelty}
  Proposed Approach: {approach}

  ## Experimental Setup
  The following describes the experiment setup. Base your implementation on this structure and make reasonable engineering choices.

  Models/Algorithms (key terms): {model_keywords}
  Detailed model specification:
  {model_details}

  Datasets (key terms): {dataset_keywords}
  Detailed dataset specification and loading steps (provide exact, runnable code to load; programmatic loaders preferred, but local paths or downloads are allowed if handled robustly):
  {dataset_details}

  Evaluation metrics (key terms): {metric_keywords}
  Detailed metric specification:
  {metric_details}

  ## Flexibility and Tools
  - You may use any libraries or models appropriate for the task (including pretrained models, transformers, graph neural networks, or diffusion models) as long as the script remains runnable end-to-end.
  - If the primary plan is resource-intensive, provide sensible defaults or smaller settings that still execute successfully without manual intervention.
  - Implement robust error handling for dataset loading. If a preferred loader fails, you may use alternative sources (other datasets, local files, or web downloads) with clear, reproducible code.
  - Subsampling large datasets is optional; consider providing a simple toggle or variable to control sample sizes when needed.
  - Prefer small pretrained models (< 100M parameters) or lightweight API endpoints (e.g., OpenAI gpt-4o-mini) to keep inference fast and demos cheap.
  - Avoid constructing ad-hoc MLP/GRU networks from scratch. Use established small pretrained models instead (e.g., `prajjwal1/bert-tiny`, `distilbert-base-uncased`, `sshleifer/tiny-distilroberta-base`, `Salesforce/codet5-small`).
  - Strongly prefer the OpenAI Python SDK for inference: `from openai import OpenAI; client = OpenAI(); client.chat.completions.create(model="gpt-4o-mini", ...)`.
  - Forbidden imports: `torch`, `transformers`, `tensorflow`, `sklearn`. Do not use them.

  ## Required Script Structure
  Your script MUST include the following components (you may adapt names, but all roles must be present):
  1. `load_data()` – loads the dataset using the specified loader and converts examples into explicit tensors/arrays (`torch.tensor` or `numpy.array`). If using HuggingFace datasets, call `.set_format(type="torch", columns=[...])` or manually build tensors, then wrap them with `torch.utils.data.TensorDataset` + `DataLoader` (or sklearn splits if you stay in sklearn). Optionally include a simple knob to subsample for faster iteration when needed.
  2. `build_model()` – constructs the model defined in the plan. Prefer small pretrained models (< 100M) or pipelines; avoid training from scratch. Document input/output dimensions explicitly, and add a short justification for why this architecture fits the task.
  3. `prepare_or_run_inference(...)` – Perform necessary preparation steps (e.g., load pretrained weights, set evaluation mode, prompt/tokenizer setup, calibration) and then run inference on the dataset. Do NOT implement any training loops, gradient updates, optimizers, or epochs. If the plan mentions training, explicitly skip it and explain briefly.
  4. `evaluate(model, data_loader, device)` – returns a dict of metrics computed from real predictions vs ground truth (or other appropriate outputs for the task). Include a comment explaining why the chosen metrics match the scientific goal.
  5. `main(out_dir)` – orchestrates the steps above, saves `final_info.json` with metric values (no placeholder numbers), and prints dataset sizes or key dataset stats for transparency.

  No training is permitted. If the dataset is very large, consider defaulting to a tiny subsample for quick inference.

  ## Execution Command (DO NOT MODIFY):
  You have {max_runs} runs to complete this experiment. For each run, the script will be executed using:
  `python experiment.py --out_dir=run_i`
  where `i` is the run number (`run_1`, `run_2`, etc.).

  ## YOU MUST ENSURE experiment.py:
  1. Parses the `--out_dir` argument and calls `os.makedirs(out_dir, exist_ok=True)`.
  2. Loads the dataset using programmatic code (HuggingFace/torchvision/sklearn or explicit loading logic) and does not rely on placeholder paths. If using local paths or downloads, ensure they are valid or handled gracefully.
  3. Builds a model that matches the dataset feature shape; document tensor dimensions in code comments when not obvious. No hidden “placeholder embeddings”.
  4. Do NOT perform training. Ensure REAL inference/preparation is performed (e.g., loading weights) and explain briefly in code comments.
  5. Computes metrics from actual predictions vs labels (or appropriate outputs) using reliable utilities—no hardcoded results.
  6. Saves a dict of metrics to `{{out_dir}}/final_info.json` (no nested folders).
  7. Avoid special-case branches when a small data structure change can unify logic.
  8. Do NOT import or call `torch.optim`, `loss.backward()`, or `optimizer.step()`.
  9. Does NOT import `torch`, `transformers`, `tensorflow`, or `sklearn`.

  The script must run successfully with only the `--out_dir` argument. If you add more arguments, provide sensible defaults so they are optional.

  **VERIFICATION CHECKLIST** (You MUST pass all checks):
  - [ ] Dataset is loaded with real data (no placeholder paths or random tensors) and converted to tensors/arrays explicitly. If subsampling or heavy settings are used, document them.
  - [ ] Model input/output shapes line up; tensor dimensions are clear from comments or code.
  - [ ] No training code present; the script performs real inference/preparation (e.g., loads weights) and documents the choice.
  - [ ] Evaluation computes metrics from actual predictions vs labels (no dummy numbers).
  - [ ] `final_info.json` is written in `out_dir` with real metric values.

  If your current experiment.py has placeholder code like `...`, replace them with runnable implementations.
  If any external functions like `compute_loss`, `evaluate_model`, or `log_results` are used, implement them too.

  ## Baseline Results
  You do not need to re-run the baseline. If available, the results are provided below:
    {baseline_results}

  ---
  Please begin writing the `experiment.py` file now.

experiment_success_prompt: |
  Run {run_num} completed. Here are the results:
  {results}

  Decide if you need to re-plan your experiments given the result (you often will not need to).

  Someone else will be using `notes.txt` to perform a writeup on this in the future.
  Please include *all* relevant information for the writeup on Run {run_num}, including an experiment description and the run number. Be as verbose as necessary.

  Then, implement the next thing on your list.
  We will then run the command `python experiment.py --out_dir=run_{next_run}'.
  YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
  If you are finished with experiments, respond with 'ALL_COMPLETED'.

experiment_error_prompt: |
  There was an error running the experiment script:
  {message}
  Your goal is still to implement this experiment: {Title}.
  The purpose is: {Experiment}.
  You have {max_runs} runs total. We're currently on run {run_time}.
  Please fix `experiment.py` so that it runs successfully with:
  `python experiment.py --out_dir=run_{run_time}`.
  Make sure to implement any missing parts like model definition, data loading, inference path, and final_info.json saving. Do NOT add training code or import local ML libraries (torch/transformers/tensorflow/sklearn). Prefer OpenAI Chat Completions (gpt-4o-mini).


experiment_timeout_prompt: |
  Run timed out after {timeout} seconds

plot_initial_prompt: |
  Great job! Please modify `plot.py` to generate the most relevant plots for the final writeup.

  In particular, be sure to fill in the "labels" dictionary with the correct names for each run that you want to plot.

  Only the runs in the `labels` dictionary will be plotted, so make sure to include all relevant runs.

  We will be running the command `python plot.py` to generate the plots.

plot_error_prompt: |
  Plotting failed with the following error {error}

plot_timeout_prompt: |
  Plotting timed out after {timeout} seconds

notes_prompt: |
  Please modify `notes.txt` with a description of what each plot shows along with the filename of the figure. Please do so in-depth.

  Somebody else will be using `notes.txt` to write a report on this in the future.
