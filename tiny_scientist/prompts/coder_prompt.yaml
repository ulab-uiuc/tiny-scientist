experiment_keyword_prompt: |
  The experiment is organized into three sections:
  ## Model Section:
  {model}

  ## Dataset Section:
  {dataset}

  ## Metric Section:
  {metric}

  Your job is to extract the essential names of models, datasets, and evaluation metrics that are directly useful for coding and experimentation.

  ### Output Format:
  Return a JSON object with the following structure:
  ```json
  {{
    "model": ["Model1", "Model2", ...],
    "dataset": ["Dataset1", "Dataset2", ...],
    "metric": ["Metric1", "Metric2", ...]
  }}

experiment_prompt: |
  You are writing a Python script named `experiment.py` that must be runnable.

  ## Research Context
  Title: {title}
  Problem: {problem}
  Novelty: {novelty}
  Proposed Approach: {approach}

  ## Experimental Setup
  The following describes the experiment setup. You must base your implementation strictly on this structure and keep it lightweight.

  Models/Algorithms (key terms): {model_keywords}
  Detailed model specification:
  {model_details}

  Datasets (key terms): {dataset_keywords}
  Detailed dataset specification (use this loader exactly — do not invent file paths):
  {dataset_details}

  Evaluation metrics (key terms): {metric_keywords}
  Detailed metric specification:
  {metric_details}

  ## Hard Constraints
  - Use only lightweight architectures: bag-of-words/logistic regression, shallow MLP (≤2 hidden layers, ≤128 units), shallow CNN (≤3 conv layers, ≤32 filters), linear SVM, single-layer GRU (≤64 hidden units). Total parameter count should stay under ~100k.
  - ABSOLUTELY FORBIDDEN: importing `transformers`, `BertTokenizer`, any BERT/GPT/ViT class, graph neural networks, diffusion models, random feature generators (`torch.rand`, `np.random` placeholders for embeddings), manual placeholder paths, loading huge pretrained checkpoints.
  - Dataset must be loaded via the provided command. If it fails, switch to one of: `datasets.load_dataset("ag_news")`, `datasets.load_dataset("imdb")`, `datasets.load_dataset("yelp_polarity")`, `datasets.load_dataset("cifar10")`, `datasets.load_dataset("mnist")`, `datasets.load_dataset("fashion_mnist")`, `datasets.load_dataset("amazon_polarity")`, or an equivalent auto-downloadable dataset. NEVER fall back to local files. For text, prefer simple vectorizers (`CountVectorizer`, `TfidfVectorizer`) over tokenizers.
  - Always subsample large datasets: cap training examples at ≤5,000 and validation/test at ≤2,000 each (use `select(range(...))` or appropriate slicing). Document this in code comments.

  ## Required Script Structure
  Your script MUST include the following components (you may adapt names, but all roles must be present):
  1. `load_data()` – downloads the dataset using the loader above, subsamples to the limits (≤5k train / ≤2k val/test), and converts examples into explicit tensors/arrays (`torch.tensor` or `numpy.array`). If using HuggingFace datasets, call `.set_format(type="torch", columns=[...])` or manually build tensors, then wrap them with `torch.utils.data.TensorDataset` + `DataLoader` (or sklearn splits if you stay in sklearn). Include a brief comment explaining why this preprocessing keeps the experiment lightweight.
  2. `build_model()` – constructs the lightweight model defined in the plan. Document input/output dimensions explicitly, and add a short justification for why this architecture is the simplest option that still addresses the task.
  3. `train(model, train_loader, optimizer, criterion, device)` – iterates over batches, moves tensors to `device`, computes loss, `loss.backward()`, `optimizer.step()`. Log the epoch count and explain in a comment why the number of epochs is sufficient.
  4. `evaluate(model, data_loader, device)` – returns a dict of metrics computed from real predictions vs ground truth. Include a comment explaining why the chosen metrics match the scientific goal.
  5. `main(out_dir)` – orchestrates the steps above, saves `final_info.json` with metric values (no placeholder numbers), and prints dataset sizes so we can verify subsampling.

  Keep training loops short (≤5 epochs, batch_size ≤64) so the experiment finishes quickly on CPU. If the dataset is extremely small, 3 epochs are usually enough.

  ## Execution Command (DO NOT MODIFY):
  You have {max_runs} runs to complete this experiment. For each run, the script will be executed using:
  `python experiment.py --out_dir=run_i`
  where `i` is the run number (`run_1`, `run_2`, etc.).

  ## YOU MUST ENSURE experiment.py:
  1. Parses the `--out_dir` argument and calls `os.makedirs(out_dir, exist_ok=True)`.
  2. Loads the dataset using the specified loader (or the fallback whitelist above) with NO manual file paths.
  3. Builds a lightweight model that matches the dataset feature shape; document tensor dimensions in code comments when not obvious. No hidden “placeholder embeddings”.
  4. Performs REAL training with gradient updates (`optimizer.zero_grad()`, `loss.backward()`, `optimizer.step()`) on actual features (no zero/random stand-ins).
  5. Computes metrics from real predictions (e.g., accuracy/F1) using sklearn/torch utilities—no hardcoded results.
  6. Saves a dict of metrics to `{{out_dir}}/final_info.json` (no nested folders).

  ## Computational Constraints
  - Ensure the code is computationally affordable to run on a single GPU or CPU machine.
  - Avoid using large models like GPT, T5, BERT-large, or full ImageNet training.
  - Prefer small-scale tasks, toy models, or low-cost benchmarks (e.g., MNIST, UCI datasets, small MLPs or ResNet18).
  - Do not use complex distributed training or multi-GPU setups.

  Do not add extra command-line arguments.

  **VERIFICATION CHECKLIST** (You MUST pass all checks):
  - [ ] Dataset is downloaded via library loader (no placeholder paths, no mocks, no random tensors) and converted to tensors/arrays explicitly. Document any subsampling (≤5k train / ≤2k val/test).
  - [ ] Model is a lightweight architecture (no transformer/graph/pretrained behemoth) and input/output shapes line up. Forbidden imports (`transformers`, BERT classes) are absent.
  - [ ] Training loop performs `optimizer.zero_grad()`, `loss.backward()`, `optimizer.step()` on real batches (no synthetic placeholder features).
  - [ ] Evaluation computes metrics from actual predictions vs labels (no dummy numbers).
  - [ ] `final_info.json` is written in `out_dir` with real metric values.
  - [ ] Sanity check: print or log dataset sizes and confirm they respect the limits; mention parameter count estimate in a comment.

  If your current experiment.py has placeholder code like `...`, replace them with runnable implementations.
  If any external functions like `compute_loss`, `evaluate_model`, or `log_results` are used, implement them too.

  ## Baseline Results
  You do not need to re-run the baseline. If available, the results are provided below:
    {baseline_results}

  ---
  Please begin writing the `experiment.py` file now.

experiment_success_prompt: |
  Run {run_num} completed. Here are the results:
  {results}

  Decide if you need to re-plan your experiments given the result (you often will not need to).

  Someone else will be using `notes.txt` to perform a writeup on this in the future.
  Please include *all* relevant information for the writeup on Run {run_num}, including an experiment description and the run number. Be as verbose as necessary.

  Then, implement the next thing on your list.
  We will then run the command `python experiment.py --out_dir=run_{next_run}'.
  YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
  If you are finished with experiments, respond with 'ALL_COMPLETED'.

experiment_error_prompt: |
  There was an error running the experiment script:
  {message}
  Your goal is still to implement this experiment: {Title}.
  The purpose is: {Experiment}.
  You have {max_runs} runs total. We're currently on run {run_time}.
  Please fix `experiment.py` so that it runs successfully with:
  `python experiment.py --out_dir=run_{run_time}`.
  Make sure to implement any missing parts like model definition, loss function, data loading, and final_info.json saving.


experiment_timeout_prompt: |
  Run timed out after {timeout} seconds

plot_initial_prompt: |
  Great job! Please modify `plot.py` to generate the most relevant plots for the final writeup.

  In particular, be sure to fill in the "labels" dictionary with the correct names for each run that you want to plot.

  Only the runs in the `labels` dictionary will be plotted, so make sure to include all relevant runs.

  We will be running the command `python plot.py` to generate the plots.

plot_error_prompt: |
  Plotting failed with the following error {error}

plot_timeout_prompt: |
  Plotting timed out after {timeout} seconds

notes_prompt: |
  Please modify `notes.txt` with a description of what each plot shows along with the filename of the figure. Please do so in-depth.

  Somebody else will be using `notes.txt` to write a report on this in the future.
