experiment_plan_prompt: |
  You are a research experiment planner. Given a research idea, produce a concrete implementation checklist for `experiment.py`.

  ## Research Context
  Title: {title}
  Problem: {problem}
  Proposed Approach: {approach}

  ## Experimental Setup
  Model/Algorithm details:
  {model_details}

  Dataset details:
  {dataset_details}

  Evaluation metric details:
  {metric_details}

  ## Your Task
  Break down the implementation of `experiment.py` into 4–6 ordered steps. Each step should be atomic and directly implementable.

  Return ONLY a JSON array with no surrounding text, in this exact format:
  ```json
  [
    {{"step": 1, "name": "Data Loading", "description": "Concise instruction for what to implement in this step"}},
    {{"step": 2, "name": "Model Architecture", "description": "..."}},
    {{"step": 3, "name": "Training Loop", "description": "..."}},
    {{"step": 4, "name": "Evaluation", "description": "..."}},
    {{"step": 5, "name": "Main Orchestration", "description": "..."}}
  ]
  ```

  Keep each description concrete: specify dataset loader, model size, optimizer, loss, metrics, and output file path where relevant.

experiment_step_prompt: |
  You are implementing step {step_num} of {total_steps} for a machine learning experiment script.

  ## Step to Implement
  Step {step_num}: {step_name}
  {step_description}

  ## Research Context
  Title: {title}
  Problem: {problem}
  Proposed Approach: {approach}

  ## Experimental Setup
  Model/Algorithm details:
  {model_details}

  Dataset details:
  {dataset_details}

  Evaluation metric details:
  {metric_details}

  ## TODO Plan (from TODO.md)
  {todo_plan}

  ## Current experiment.py content
  {current_code}

  ## Hard Constraints (apply to all steps)
  - Lightweight architectures only: bag-of-words/logistic regression, shallow MLP (≤2 hidden layers, ≤128 units), shallow CNN (≤3 conv, ≤32 filters), linear SVM, single-layer GRU (≤64 units)
  - FORBIDDEN: transformers, BERT/GPT/ViT classes, graph neural networks, diffusion models, pretrained checkpoints
  - Subsample datasets: ≤5,000 train, ≤2,000 val/test
  - Training: ≤5 epochs, batch_size ≤64
  - Script must accept --out_dir argument and save final_info.json there

  ## Instructions
  1. Read the current experiment.py using the read_file tool (if it exists)
  2. Implement ONLY the current step — do not skip ahead or leave placeholders
  3. Write the complete updated experiment.py using the write_file tool
  4. The file must remain runnable after each step (add pass stubs for functions not yet implemented)
  5. Follow TODO.md ordering strictly and do not work on unchecked future steps

  After writing, respond with "STEP_DONE".

experiment_keyword_prompt: |
  The experiment is organized into three sections:
  ## Model Section:
  {model}

  ## Dataset Section:
  {dataset}

  ## Metric Section:
  {metric}

  Your job is to extract the essential names of models, datasets, and evaluation metrics that are directly useful for coding and experimentation.

  ### Output Format:
  Return a JSON object with the following structure:
  ```json
  {{
    "model": ["Model1", "Model2", ...],
    "dataset": ["Dataset1", "Dataset2", ...],
    "metric": ["Metric1", "Metric2", ...]
  }}

experiment_prompt: |
  You are writing a Python script named `experiment.py` that must be runnable.

  ## Research Context
  Title: {title}
  Problem: {problem}
  Novelty: {novelty}
  Proposed Approach: {approach}

  ## Experimental Setup
  The following describes the experiment setup. You must base your implementation strictly on this structure and keep it lightweight.

  Models/Algorithms (key terms): {model_keywords}
  Detailed model specification:
  {model_details}

  Datasets (key terms): {dataset_keywords}
  Detailed dataset specification (use this loader exactly — do not invent file paths):
  {dataset_details}

  Evaluation metrics (key terms): {metric_keywords}
  Detailed metric specification:
  {metric_details}

  ## Hard Constraints
  - Use only lightweight architectures: bag-of-words/logistic regression, shallow MLP (≤2 hidden layers, ≤128 units), shallow CNN (≤3 conv layers, ≤32 filters), linear SVM, single-layer GRU (≤64 hidden units). Total parameter count should stay under ~100k.
  - ABSOLUTELY FORBIDDEN: importing `transformers`, `BertTokenizer`, any BERT/GPT/ViT class, graph neural networks, diffusion models, random feature generators (`torch.rand`, `np.random` placeholders for embeddings), manual placeholder paths, loading huge pretrained checkpoints.
  - Dataset must be loaded via the provided command. If it fails, switch to one of: `datasets.load_dataset("ag_news")`, `datasets.load_dataset("imdb")`, `datasets.load_dataset("yelp_polarity")`, `datasets.load_dataset("cifar10")`, `datasets.load_dataset("mnist")`, `datasets.load_dataset("fashion_mnist")`, `datasets.load_dataset("amazon_polarity")`, or an equivalent auto-downloadable dataset. NEVER fall back to local files. For text, prefer simple vectorizers (`CountVectorizer`, `TfidfVectorizer`) over tokenizers.
  - Always subsample large datasets: cap training examples at ≤5,000 and validation/test at ≤2,000 each (use `select(range(...))` or appropriate slicing). Document this in code comments.

  ## Required Script Structure
  Your script MUST include the following components (you may adapt names, but all roles must be present):
  1. `load_data()` – downloads the dataset using the loader above, subsamples to the limits (≤5k train / ≤2k val/test), and converts examples into explicit tensors/arrays (`torch.tensor` or `numpy.array`). If using HuggingFace datasets, call `.set_format(type="torch", columns=[...])` or manually build tensors, then wrap them with `torch.utils.data.TensorDataset` + `DataLoader` (or sklearn splits if you stay in sklearn). Include a brief comment explaining why this preprocessing keeps the experiment lightweight.
  2. `build_model()` – constructs the lightweight model defined in the plan. Document input/output dimensions explicitly, and add a short justification for why this architecture is the simplest option that still addresses the task.
  3. `train(model, train_loader, optimizer, criterion, device)` – iterates over batches, moves tensors to `device`, computes loss, `loss.backward()`, `optimizer.step()`. Log the epoch count and explain in a comment why the number of epochs is sufficient.
  4. `evaluate(model, data_loader, device)` – returns a dict of metrics computed from real predictions vs ground truth. Include a comment explaining why the chosen metrics match the scientific goal.
  5. `main(out_dir)` – orchestrates the steps above, saves `final_info.json` with metric values (no placeholder numbers), and prints dataset sizes so we can verify subsampling.

  Keep training loops short (≤5 epochs, batch_size ≤64) so the experiment finishes quickly on CPU. If the dataset is extremely small, 3 epochs are usually enough.

  ## Execution Command (DO NOT MODIFY):
  You have {max_runs} runs to complete this experiment. For each run, the script will be executed using:
  `python experiment.py --out_dir=run_i`
  where `i` is the run number (`run_1`, `run_2`, etc.).

  ## YOU MUST ENSURE experiment.py:
  1. Parses the `--out_dir` argument and calls `os.makedirs(out_dir, exist_ok=True)`.
  2. Loads the dataset using the specified loader (or the fallback whitelist above) with NO manual file paths.
  3. Builds a lightweight model that matches the dataset feature shape; document tensor dimensions in code comments when not obvious. No hidden “placeholder embeddings”.
  4. Performs REAL training with gradient updates (`optimizer.zero_grad()`, `loss.backward()`, `optimizer.step()`) on actual features (no zero/random stand-ins).
  5. Computes metrics from real predictions (e.g., accuracy/F1) using sklearn/torch utilities—no hardcoded results.
  6. Saves a dict of metrics to `{{out_dir}}/final_info.json` (no nested folders).

  ## Computational Constraints
  - Ensure the code is computationally affordable to run on a single GPU or CPU machine.
  - Avoid using large models like GPT, T5, BERT-large, or full ImageNet training.
  - Prefer small-scale tasks, toy models, or low-cost benchmarks (e.g., MNIST, UCI datasets, small MLPs or ResNet18).
  - Do not use complex distributed training or multi-GPU setups.

  Do not add extra command-line arguments.

  **VERIFICATION CHECKLIST** (You MUST pass all checks):
  - [ ] Dataset is downloaded via library loader (no placeholder paths, no mocks, no random tensors) and converted to tensors/arrays explicitly. Document any subsampling (≤5k train / ≤2k val/test).
  - [ ] Model is a lightweight architecture (no transformer/graph/pretrained behemoth) and input/output shapes line up. Forbidden imports (`transformers`, BERT classes) are absent.
  - [ ] Training loop performs `optimizer.zero_grad()`, `loss.backward()`, `optimizer.step()` on real batches (no synthetic placeholder features).
  - [ ] Evaluation computes metrics from actual predictions vs labels (no dummy numbers).
  - [ ] `final_info.json` is written in `out_dir` with real metric values.
  - [ ] Sanity check: print or log dataset sizes and confirm they respect the limits; mention parameter count estimate in a comment.

  If your current experiment.py has placeholder code like `...`, replace them with runnable implementations.
  If any external functions like `compute_loss`, `evaluate_model`, or `log_results` are used, implement them too.

  ## Baseline Results
  You do not need to re-run the baseline. If available, the results are provided below:
    {baseline_results}

  ---
  Please begin writing the `experiment.py` file now.

experiment_success_prompt: |
  Run {run_num} completed. Here are the results:
  {results}

  Decide if you need to re-plan your experiments given the result (you often will not need to).

  Someone else will be using `notes.txt` to perform a writeup on this in the future.
  Please include *all* relevant information for the writeup on Run {run_num}, including an experiment description and the run number. Be as verbose as necessary.

  Then, implement the next thing on your list.
  We will then run the command `python experiment.py --out_dir=run_{next_run}'.
  YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
  If you are finished with experiments, respond with 'ALL_COMPLETED'.

experiment_error_prompt: |
  There was an error running the experiment script:
  {message}
  Your goal is still to implement this experiment: {Title}.
  The purpose is: {Experiment}.
  You have {max_runs} runs total. We're currently on run {run_time}.
  Please fix `experiment.py` so that it runs successfully with:
  `python experiment.py --out_dir=run_{run_time}`.
  Make sure to implement any missing parts like model definition, loss function, data loading, and final_info.json saving.


experiment_timeout_prompt: |
  Run timed out after {timeout} seconds

plot_initial_prompt: |
  Great job! Please modify `plot.py` to generate the most relevant plots for the final writeup.

  In particular, be sure to fill in the "labels" dictionary with the correct names for each run that you want to plot.

  Only the runs in the `labels` dictionary will be plotted, so make sure to include all relevant runs.

  We will be running the command `python plot.py` to generate the plots.

plot_error_prompt: |
  Plotting failed with the following error {error}

plot_timeout_prompt: |
  Plotting timed out after {timeout} seconds

notes_prompt: |
  Please modify `notes.txt` with a description of what each plot shows along with the filename of the figure. Please do so in-depth.

  Somebody else will be using `notes.txt` to write a report on this in the future.
