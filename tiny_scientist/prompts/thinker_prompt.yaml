# thinker_prompt.yaml

# System prompts
idea_system_prompt: >
  You are an ambitious AI PhD student who is looking to publish a paper that will contribute significantly to the field.
  You want to generate creative and impactful research ideas that can be feasibly investigated with the code provided.
  Be critical and realistic in your assessments.
evaluation_system_prompt: >
  You are an expert research reviewer who evaluates scientific ideas with rigor and fairness.
  Your role is to comparatively evaluate multiple research ideas and score them based on their feasibility, novelty, impact, and alignment with the original research intent.
  Be thoughtful, objective, and provide clear justifications for your scoring.
dimension_suggestion_prompt: |
  Based on the following research intent, suggest 5 pairs of contrasting evaluation dimensions that would be most relevant for comparing research ideas.

  RESEARCH INTENT:
  ```
  {intent}
  ```

  Each dimension pair should represent TWO OPPOSING ENDS OF THE SAME SPECTRUM. These are trade-offs, not independent dimensions.

  EXAMPLES OF DIMENSION PAIRS:
  - (HCI-oriented, AI-oriented): Interaction design vs. algorithmic innovation
  - (High Risk High Reward, Low Risk Incremental): Bold bets vs. safe iterations
  - (Theoretical Depth, Practical Application): Theory vs. real-world utility
  - (Computational Efficiency, Model Accuracy): Fast/lightweight vs. accurate/resource-intensive

  REQUIREMENTS:
  1. Each pair must be a single spectrum with two opposing ends (not two independent axes).
  2. The pairs must be relevant to the intent and represent meaningful trade-offs.
  3. Provide short, concrete descriptions for both ends.

  Respond in the following format:

  THOUGHT:
  <THOUGHT>

  DIMENSION PAIRS JSON:
  ```json
  <JSON>
  ```

  In <THOUGHT>, justify why these spectrums are the right trade-offs for this intent (2-4 sentences).

  In <JSON>, provide exactly 5 dimension pairs with the following structure:
  {{
    "dimension_pairs": [
      {{
        "dimensionA": "First end of the spectrum (e.g., HCI-oriented)",
        "dimensionB": "Opposite end of the spectrum (e.g., AI-oriented)",
        "descriptionA": "Brief description of what dimensionA means",
        "descriptionB": "Brief description of what dimensionB means"
      }}
    ]
  }}

  CRITICAL: Each pair represents ONE spectrum (later scored on a -50..+50 scale: -50 = dimensionA, +50 = dimensionB, 0 = balanced).

single_dimension_evaluation_prompt: |
  You are tasked with evaluating research ideas on a SINGLE dimension.

  RESEARCH INTENT:
  ```
  {intent}
  ```

  IDEAS TO EVALUATE:
  ```
  {ideas}
  ```

  DIMENSION CRITERIA:
  **{dimension_a} vs {dimension_b}**
  - **{dimension_a} (Score: -50)**: {description_a}
  - **{dimension_b} (Score: +50)**: {description_b}

  This is a SPECTRUM where -50 means completely aligned with {dimension_a}, +50 means completely aligned with {dimension_b}, and 0 means balanced between both.

  OUTPUT REQUIREMENTS:
  - For each idea, provide a score (-50 to +50) and brief reasoning
  - Preserve exact titles from the input
  - Be consistent in your scoring scale
  - Consider how each idea positions itself on this specific dimension spectrum
  - JSON numeric values must be strict: use 30 not +30; only negative numbers use a leading "-"

  Respond in the following format:

  THOUGHT:
  <THOUGHT>

  EVALUATION JSON:
  ```json
  {{
    "scored_ideas": [
      {{
        "Title": "exact idea title",
        "Score": <number from -50 to +50>,
        "Reason": "brief explanation of why this score (1-2 sentences)"
      }}
    ]
  }}
  ```

  In <THOUGHT>, briefly describe how you interpret this spectrum and ensure consistent scoring (1-2 sentences).

  IMPORTANT: The JSON inside the ```json block must be the evaluation object itself (do NOT wrap it inside another object with keys like "THOUGHT").

idea_evaluation_prompt: |
  You are tasked with evaluating and scoring multiple research ideas generated for the following research intent:

  RESEARCH INTENT:
  ```
  {intent}
  ```

  RESEARCH IDEAS TO EVALUATE:
  ```
  {ideas}
  ```
  {modify_context}
  Please evaluate these ideas comparatively across THREE custom dimension pairs chosen specifically for this research context.

  Some ideas include a field "problem_highlights": a short list of key phrases copied verbatim from the Problem. Treat these as critical anchors: factor them into scoring where relevant and explicitly reference them in your reasoning when they influence the score.

  **DIMENSION PAIR 1: {dimension_pair_1_name}**
  - **{dimension_1_a} (Score: -50)**: {dimension_1_a_desc}
  - **{dimension_1_b} (Score: +50)**: {dimension_1_b_desc}

  **DIMENSION PAIR 2: {dimension_pair_2_name}**
  - **{dimension_2_a} (Score: -50)**: {dimension_2_a_desc}
  - **{dimension_2_b} (Score: +50)**: {dimension_2_b_desc}

  **DIMENSION PAIR 3: {dimension_pair_3_name}**
  - **{dimension_3_a} (Score: -50)**: {dimension_3_a_desc}
  - **{dimension_3_b} (Score: +50)**: {dimension_3_b_desc}

  SCORING SCALE (all three pairs): -50 = fully aligned with the first endpoint (A), +50 = fully aligned with the second endpoint (B), 0 = balanced.

  CRITICAL REQUIREMENTS:
  1. For EACH idea, output ALL fields: "Title", "Dimension1Score", "Dimension2Score", "Dimension3Score", "Dimension1Reason", "Dimension2Reason", "Dimension3Reason".
  2. Scores are integers in -50..+50 (-50 = {dimension_1_a}/{dimension_2_a}/{dimension_3_a}, +50 = {dimension_1_b}/{dimension_2_b}/{dimension_3_b}, 0 = balanced).
  3. Ensure meaningful differentiation between ideas (avoid identical score triplets).
  4. JSON numeric values must be strict: use 30 not +30; only negative numbers use a leading "-".

  Respond in the following format:

  THOUGHT:
  <THOUGHT>

  COMPARATIVE ANALYSIS:
  <ANALYSIS>

  EVALUATION JSON:
  ```json
  <JSON>
  ```

  In <THOUGHT>, briefly state your comparative evaluation strategy (1-2 sentences).
  In <ANALYSIS>, provide a brief comparative summary highlighting the key differences across the three spectrums (3-6 sentences).
  In <JSON>, provide the evaluation results in JSON format with the following structure:
  "scored_ideas": A list of scored idea objects, each containing:
  - "Title": The EXACT original title of the idea as provided in the input JSON - DO NOT MODIFY OR CHANGE THE TITLE IN ANY WAY
  - "Dimension1Score": A number from -50 to +50
  - "Dimension2Score": A number from -50 to +50
  - "Dimension3Score": A number from -50 to +50
  - "Dimension1Reason": A brief explanation (1-2 sentences) - REQUIRED
  - "Dimension2Reason": A brief explanation (1-2 sentences) - REQUIRED
  - "Dimension3Reason": A brief explanation (1-2 sentences) - REQUIRED

  IMPORTANT: The reasoning fields are MANDATORY and must be included for every idea. Do not omit these fields.

  NOTE FOR ROBUSTNESS: If you are unable to provide a concise reason for a given dimension for any idea, you MUST still include the corresponding "Dimension1Reason" / "Dimension2Reason" / "Dimension3Reason" fields in the JSON and set their value to an empty string (""). Do not omit the keys. This helps downstream parsers reliably find the fields.

  CRITICAL: You MUST preserve the exact original titles from the input. Do not change, modify, or improve the titles in any way.
  Ensure your evaluation is fair, comprehensive, and based solely on the scientific and practical merits of each idea.

idea_partial_evaluation_prompt: |
  You are tasked with assigning scores ONLY to the newly added research ideas while being given the full previously scored idea list for calibration.

  RESEARCH INTENT:
  ```
  {intent}
  ```

  FULL IDEA SET (context + new):
  ```
  {ideas}
  ```
  {modify_context}
  Some ideas include a field "problem_highlights": a short list of key phrases copied verbatim from the Problem. Treat these as critical anchors: factor them into scoring where relevant and explicitly reference them in your reasoning when they influence the score.

  PARTIAL SCORING PROTOCOL:
  - Some ideas include "AlreadyScored": true along with existing *Score and *Reason fields; these are CONTEXT ONLY and must NOT be rescored or output again.
  - Only ideas without "AlreadyScored" true (either missing the field or explicitly false) are NEW TARGETS requiring fresh scores.
  - Calibrate new scores relative to existing contextual scores so the global scale remains consistent (avoid inflating or compressing scale unnecessarily).
  - Do NOT repeat or copy existing contextual ideas in the output.
  - If all ideas are new (no AlreadyScored true), then score them all (treat as first evaluation batch).

  DIMENSION CRITERIA:
  **DIMENSION PAIR 1: {dimension_pair_1_name}**
  - **{dimension_1_a} (Score: -50)**: {dimension_1_a_desc}
  - **{dimension_1_b} (Score: +50)**: {dimension_1_b_desc}

  **DIMENSION PAIR 2: {dimension_pair_2_name}**
  - **{dimension_2_a} (Score: -50)**: {dimension_2_a_desc}
  - **{dimension_2_b} (Score: +50)**: {dimension_2_b_desc}

  **DIMENSION PAIR 3: {dimension_pair_3_name}**
  - **{dimension_3_a} (Score: -50)**: {dimension_3_a_desc}
  - **{dimension_3_b} (Score: +50)**: {dimension_3_b_desc}

  OUTPUT REQUIREMENTS:
  1. Provide distinct, fine‑grained scores (-50 to +50) for each new idea: "Dimension1Score", "Dimension2Score", "Dimension3Score".
  2. Provide brief reasoning for each score: "Dimension1Reason", "Dimension2Reason", "Dimension3Reason".
  NOTE FOR ROBUSTNESS: If you cannot provide a reason for a given score, include the corresponding reason field and set it to an empty string ("") rather than omitting the key.
  3. DO NOT include any context (AlreadyScored true) ideas in the output JSON.
  4. Preserve exact titles.
  5. JSON numeric values must be strict: use 30 not +30; only negative numbers use a leading "-".

  Respond in the following format:

  THOUGHT:
  <THOUGHT>

  CALIBRATION ANALYSIS:
  <ANALYSIS>

  EVALUATION JSON:
  ```json
  <JSON>
  ```
  In <THOUGHT>, briefly describe how you will calibrate scores against the context ideas (1-2 sentences).
  In <ANALYSIS>, provide a concise calibration explanation describing how the contextual (AlreadyScored) ideas and their existing scores informed the scale you used for the NEW ideas.

  In <JSON>, provide ONLY the newly scored idea objects in JSON format. Use the following structure:
  "scored_ideas": A list of newly scored idea objects, each containing:
  - "Title": The EXACT original title of the new idea (preserve exact text)
  - "Dimension1Score": A number from -50 to +50
  - "Dimension2Score": A number from -50 to +50
  - "Dimension3Score": A number from -50 to +50
  - "Dimension1Reason": A brief explanation (1-2 sentences) of the score - REQUIRED
  - "Dimension2Reason": A brief explanation (1-2 sentences) of the score - REQUIRED
  - "Dimension3Reason": A brief explanation (1-2 sentences) of the score - REQUIRED

  IMPORTANT: The reasoning fields (Dimension1Reason, Dimension2Reason, Dimension3Reason) are MANDATORY and must be included for every idea. Do not omit these fields.

  CRITICAL:
  - Do NOT include any context ideas (those with "AlreadyScored": true) inside "scored_ideas".
  - Do NOT invent or modify titles.
  - Maintain meaningful differentiation between the numeric scores.
modify_idea_prompt: |
  Given a research idea and score adjustments based on user evaluation, generate a modified version of the idea that aligns with the specified target scores.

  ORIGINAL RESEARCH IDEA:
  ```
  {idea}
  ```

  SCORE ADJUSTMENTS:
  ```
  {modifications}
  ```

  RESEARCH INTENT:
  ```
  {intent}
  ```

  EVALUATION CRITERIA:
  **DIMENSION PAIR 1: {dimension_pair_1_name}**
  - **{dimension_1_a} (Score: -50)**: {dimension_1_a_desc}
  - **{dimension_1_b} (Score: +50)**: {dimension_1_b_desc}

  **DIMENSION PAIR 2: {dimension_pair_2_name}**
  - **{dimension_2_a} (Score: -50)**: {dimension_2_a_desc}
  - **{dimension_2_b} (Score: +50)**: {dimension_2_b_desc}

  **DIMENSION PAIR 3: {dimension_pair_3_name}**
  - **{dimension_3_a} (Score: -50)**: {dimension_3_a_desc}
  - **{dimension_3_b} (Score: +50)**: {dimension_3_b_desc}

  IMPORTANT: Use the score adjustments as concrete targets. For example, if a dimension moves from -30 to -5, the idea should shift slightly toward the opposite end but remain broadly similar; if it moves from -45 to +40, expect a major redesign.

  Guidelines for modification:
  1. Preserve the core contribution unless the target shifts are extreme (|score| >= 40 or a sign flip with large magnitude).
  2. Change the parts that justify the new positions (methodology, supervision signal, constraints, evaluation, resources).
  3. Keep it aligned with the original intent and keep changes concrete (not cosmetic).

  **CRITICAL**: When modifying the idea:
  - Keep the "Approach" field focused on the CORE NOVEL ALGORITHM (not basic preprocessing)
  - The "Approach" should describe YOUR algorithmic innovation with mathematical/technical detail
  - Maintain detailed comparison with existing work in "NoveltyComparison"
  - AVOID filling "Approach" with standard data processing, tokenization, or routine training procedures
  - AVOID ethical considerations, societal impact, or bias discussions in "Approach" - focus ONLY on the technical algorithm

  Respond in the following format:

  THOUGHT:
  <THOUGHT>

  MODIFIED IDEA JSON:
  ```json
  <JSON>
  ```

  In <THOUGHT>, explain how you interpreted the score adjustments and what specific changes you made to achieve the target scores.

  In <JSON>, provide the modified idea preserving ALL original fields:

  **REQUIRED FIELDS** (must be present in output):
  - "Name": Lowercase identifier with underscores
  - "Title": Full title for the idea
  - "Description": Brief 1-2 sentence summary
  - "Problem": The research question being addressed
  - "Importance": Why this problem matters
  - "Difficulty": Technical challenges
  - "NoveltyComparison": Detailed comparison with existing work (≥150 words)
  - "Approach": Core algorithm/mechanism (CRITICAL - focus on novel component)
  - "is_experimental": Boolean (true/false)
  - "Interestingness": Score 1-10
  - "Feasibility": Score 1-10
  - "Novelty": Score 1-10
  - "IntentAlignment": Score 1-10
  - "Score": Overall score 1-10

  **PRESERVE IF PRESENT** (copy from original if exists):
  - "Experiment": Experimental plan (if already generated)
  - "ResearchPlan": Alternative to Experiment
  - Any evaluation scores: NoveltyScore, FeasibilityScore, ImpactScore, Dimension1Score, Dimension2Score, Dimension3Score, scores dict, etc.

  **IMPORTANT**:
  - If the original idea has an "Experiment" field, copy it exactly unless modifications specifically target the experimental design
  - Preserve all fields even if not modified
  - The "Approach" field is CRITICAL for downstream code generation

merge_ideas_prompt: |
  You are tasked with merging two research ideas into a single, stronger idea that combines the best aspects of both.

  IDEA A:
  ```
  {idea_a}
  ```

  IDEA B:
  ```
  {idea_b}
  ```

  RESEARCH INTENT:
  ```
  {intent}
  ```

  Create a new research idea that:
  1. Preserves the most valuable aspects of both original ideas
  2. Resolves any contradictions or tensions between them
  3. Maintains strong alignment with the original research intent
  4. Is more novel, feasible, or impactful than either original idea alone

  Respond in the following format:
  THOUGHT:
  <THOUGHT>

  MERGED IDEA JSON:
  ```json
  <JSON>
  ```

  In <THOUGHT>, explain your reasoning for how you combined elements from both ideas and why the merged result is stronger.
  In <JSON>, provide the merged idea with ALL required fields:

  **REQUIRED FIELDS** (must be present in output):
  - "Name": Lowercase identifier with underscores (create new merged name)
  - "Title": New title combining aspects of both ideas
  - "Description": Brief 1-2 sentence summary
  - "Problem": Merged research question
  - "Importance": Why the merged problem matters
  - "Difficulty": Combined technical challenges
  - "NoveltyComparison": Detailed comparison explaining how merged idea differs from existing work (≥150 words)
  - "Approach": Core algorithm/mechanism combining both ideas (CRITICAL - describe the merged technical approach)
  - "is_experimental": Boolean (true if either idea is experimental)
  - "Interestingness": Score 1-10 (estimate for merged idea)
  - "Feasibility": Score 1-10 (estimate for merged idea)
  - "Novelty": Score 1-10 (estimate for merged idea)
  - "IntentAlignment": Score 1-10 (estimate for merged idea)
  - "Score": Overall score 1-10 (estimate for merged idea)

  **PRESERVE IF PRESENT** (copy from most relevant source idea or merge intelligently):
  - "Experiment": If one or both ideas have experimental plans, merge or choose the more comprehensive one
  - "ResearchPlan": Alternative to Experiment

  **IMPORTANT**:
  - The "Approach" field is CRITICAL and must describe the merged technical approach
  - If both ideas have "Experiment" fields, intelligently merge them or choose the more comprehensive one
  - Create new Name/Title that reflects the merged nature
  - Do not copy evaluation scores from either idea (NoveltyScore, FeasibilityScore, etc.) - the merged idea will be re-evaluated

novelty_system_prompt: |
  You are an ambitious AI PhD student who is looking to publish a paper that will contribute significantly to the field.
  You have an idea and you want to check if it is novel or not. I.e., not overlapping significantly with existing literature or already well explored.
  Be a harsh critic for novelty, ensure there is a sufficient contribution in the idea for a new conference or workshop paper.
  You are analyzing search results to determine if your idea has already been explored in existing literature.

  Decide a paper idea is novel if after sufficient searching, you have not found a paper that significantly overlaps with your idea.
  Decide a paper idea is not novel if you have found a paper that significantly overlaps with your idea.

query_prompt: |
  Based on the following research intent:

  ```
  {intent}
  ```

  **YOUR TASK**: Read the intent above and extract 2-4 SPECIFIC technical terms FROM THE INTENT to create a search query.

  **CRITICAL STEP-BY-STEP**:
  1. READ the intent carefully
  2. IDENTIFY the specific technique/algorithm mentioned (e.g., "latent reasoning", "graph attention", "knowledge distillation")
  3. IDENTIFY the specific model/domain mentioned (e.g., "BERT", "transformers", "large language models")
  4. COMBINE them with AND: "technique AND model/domain"
  5. DO NOT use examples below - they are just to show the FORMAT, not content to copy!

  **TYPES OF SPECIFIC TECHNICAL TERMS TO EXTRACT FROM INTENT**:
  ✅ Specific algorithms: "graph attention network", "knowledge distillation", "contrastive learning"
  ✅ Specific mechanisms: "tree-based reasoning", "causal intervention", "hierarchical attention"
  ✅ Specific models: "BERT", "GPT", "transformer", "large language models"
  ✅ Specific architectures: "encoder-decoder", "autoregressive", "diffusion model"
  ✅ Specific tasks: "link prediction", "question answering", "sentiment analysis"

  **WHAT TO AVOID** (general/vague terms):
  ❌ "machine learning", "deep learning", "AI"
  ❌ "neural networks" (too general - specify which type!)
  ❌ "NLP", "computer vision" (too broad)
  ❌ "improve performance", "better results"

  **FORMAT EXAMPLES** (showing query structure, NOT content to copy):
  - If intent mentions "causal graphs for GNN link prediction" → query: "causal graph AND graph neural network AND link prediction"
  - If intent mentions "knowledge distillation for BERT" → query: "knowledge distillation AND BERT"
  - If intent mentions "latent reasoning in LLMs" → query: "latent reasoning AND large language models"
  - If intent mentions "contrastive learning on graphs" → query: "contrastive learning AND graph representation"

  **REMEMBER**: Extract terms FROM THE INTENT above, not from these examples!

  Respond in the following format:

  RESPONSE:
  ```json
  <JSON>
  ```

  In <JSON>, respond in JSON format with ONLY the following field:
  - "Query": The query you just generated (3-5 concepts, use AND/OR strategically)

  **CRITICAL JSON FORMAT**:
  - Do NOT use quotes around individual terms inside the Query value
  - CORRECT: {{"Query": "causal graph AND graph neural networks"}}
  - WRONG: {{"Query": "\"causal graph\" AND \"graph neural networks\""}}

rethink_query_prompt: |
  Based on the following research idea and the original intent:

  ORIGINAL INTENT:
  ```
  {intent}
  ```

  CURRENT IDEA:
  ```
  {idea}
  ```

  **YOUR TASK**: Read the CURRENT IDEA above and extract 2-3 SPECIFIC technical terms FROM IT to create a search query.

  **CRITICAL STEP-BY-STEP**:
  1. READ the current idea's "Approach" section carefully
  2. IDENTIFY the specific technique/algorithm in the approach (e.g., "hierarchical attention", "causal intervention")
  3. IDENTIFY the specific task/application (e.g., "text classification", "bias mitigation")
  4. COMBINE them with AND: "technique AND task"
  5. DO NOT copy from examples - extract from the IDEA above!

  **TYPES OF TERMS TO EXTRACT FROM THE IDEA**:
  ✅ Specific techniques in the approach: "hierarchical attention", "knowledge distillation", "causal intervention"
  ✅ Specific models mentioned: "BERT", "GPT", "transformer"
  ✅ Specific tasks: "text classification", "question answering", "node classification"

  **WHAT TO AVOID**:
  ❌ Generic terms: "deep learning", "machine learning"
  ❌ Vague terms: "neural networks" (specify which!)
  ❌ Abbreviations without context: "RL", "NLP"

  **FORMAT EXAMPLES** (showing structure only, NOT content to copy):
  - If idea uses "hierarchical attention for text classification" → query: "hierarchical attention AND text classification"
  - If idea uses "causal intervention for bias mitigation" → query: "causal intervention AND bias mitigation"
  - If idea uses "knowledge distillation on BERT" → query: "knowledge distillation AND BERT"

  **REMEMBER**: Extract terms FROM THE CURRENT IDEA above!

  Respond in the following format:

  RESPONSE:
  ```json
  <JSON>
  ```

  In <JSON>, respond in JSON format with ONLY the following field:
  - "Query": The query you just generated 1-3 concepts, use AND/OR strategically)

  **CRITICAL JSON FORMAT**:
  - Do NOT use quotes around individual terms inside the Query value
  - CORRECT: {{"Query": "hierarchical attention AND dynamic weighting"}}
  - WRONG: {{"Query": "\"hierarchical attention\" AND \"dynamic weighting\""}}

novelty_query_prompt: |
  Based on the following research idea and the original intent:

  ORIGINAL INTENT:
  ```
  {intent}
  ```

  CURRENT IDEA:
  ```
  {idea}
  ```

  **YOUR TASK**: Read the CURRENT IDEA's "Approach" and extract the MOST SPECIFIC technical terms of the CORE NOVEL ALGORITHM to check if it already exists.

  **CRITICAL STEP-BY-STEP FOR NOVELTY CHECK**:
  1. READ the "Approach" section in the CURRENT IDEA
  2. IDENTIFY what makes it novel (e.g., "causal" + "graph attention", "adversarial" + "contrastive learning")
  3. EXTRACT all distinctive modifiers + the base technique (e.g., "causal graph attention", not just "attention")
  4. COMBINE all novel components with AND
  5. Goal: Find papers with the EXACT SAME combination
  6. DO NOT use examples below - extract from the IDEA!

  **HOW TO EXTRACT THE NOVEL COMPONENTS FROM THE IDEA**:
  ✅ If idea uses "causal graph attention" → extract: "causal AND graph attention network"
  ✅ If idea uses "adversarial contrastive learning" → extract: "adversarial AND contrastive learning"
  ✅ If idea uses "hierarchical policy gradient for dialogue" → extract: "hierarchical AND policy gradient AND dialogue"
  ✅ If idea uses "tree-based reasoning with transformers" → extract: "tree-based AND reasoning AND transformer"

  **KEY PRINCIPLE**:
  - Include ALL distinctive modifiers that make it novel (e.g., "causal", "adversarial", "hierarchical")
  - Use complete technical phrases (e.g., "graph attention network", not just "attention")
  - Be as specific as the idea describes

  **FORMAT EXAMPLES** (structure only, extract YOUR content from idea above):
  - If idea's novelty is "X + Y technique for Z" → query: "X AND Y technique AND Z"
  - If idea combines "A method with B architecture" → query: "A method AND B architecture"

  **REMEMBER**: Extract the EXACT novel combination FROM THE IDEA's "Approach" above!

  Respond in the following format:

  RESPONSE:
  ```json
  <JSON>
  ```

  In <JSON>, respond in JSON format with ONLY the following field:
  - "Query": The query you just generated (3-5 distinctive concepts, use AND/OR strategically)

  **CRITICAL JSON FORMAT**:
  - Do NOT use quotes around individual terms inside the Query value
  - CORRECT: {{"Query": "causal AND attention AND graph neural networks"}}
  - WRONG: {{"Query": "\"causal\" AND \"attention\" AND \"graph neural networks\""}}

# Complete prompts that combine all necessary information
idea_first_prompt: |
  Generate a **creative and impactful research idea** based on:
  ```
  {intent}
  ```

  Related works:
  ```
  {related_works_string}
  ```

  Please make sure the following requirements are kept:
  1. **Problem**
    * The problem should be a precise, not too large and not ambiguous problem.
    * Think from demand: what are the attractive/necessary demands from the research community that have not been fully satisfied?
    * A question that can be answered with yes/no + some reason. For example, Can LLM do well on NER task?
  2. **Importance**
    * Explain why the problem matters and is interesting.
    * Use recent literature to support your points.
    * Relate to the community consensus/trend to show your importance. We point out the gap/potential/opportunities between our idea and the current consensus.
    * Recent example: Scalable LLM ranker? Graph based ranker
  3. **Difficulty**
    * Explain why the problem is technically challenging and why simple methods fail.
    * Cite prior works that highlight these challenges.
    * List 2-4 reason why this is very challenging. List with (1) (2) (3)
  4. **NoveltyComparison**
    * Write a **detailed comparison (≥150 words)** explaining how your idea differs from existing work.
    * Discuss related works, their approaches, and limitations. Analyze your prior work’s limitations, and argue that yours is better
    * Explain why past methods don’t fully solve the problem and what’s new about yours.
    * Prove your novelty. No one has solved XYZ, but we did. These limitations can be perfectly solved after our paper.
  5. **Approach**
    * Describe the **core algorithm/mechanism** of your proposed method.
    * Need to explain each point in the difficulty why our proposed method can solve them. List explanation for each point in the difficulty
    * Focus on your novel component, not standard preprocessing or ethics.
    * IMPORTANT! You need to make sure that your proposed method can solve the problem you propose. Need to include the reason why such method can solve.


  The output should be this format:
  ```json
  {{
    "Name": "...",
    "Title": "...",
    "Description": "...",
    "Problem": "...",
    "Importance": "...",
    "Difficulty": "...",
    "NoveltyComparison": "...",
    "Approach": "...",
    "is_experimental": true,
    "Interestingness": 1-10,
    "Feasibility": 1-10,
    "Novelty": 1-10,
    "IntentAlignment": 1-10,
    "Score": 1-10
  }}
  ```

idea_reflection_prompt: |
  Round {current_round}/{num_reflections}

  Original Intent:
  ```
  {intent}
  ```

  Current Idea:
  ```
  {current_idea}
  ```

  Refer to the following related works from recent literature that might help inform your reflection:
  ```
  {related_works_string}
  ```

  Your goal is to turn this idea into a technically strong, and novel research proposal that could be published.

  Key Reflection Priorities
  1. **Intent Alignment** – Make sure the idea directly solves the original intent. If not, refocus.
  2. **Problem** – Add concrete examples, numbers, and proper citations. Show clearly why this problem matters and persists.
  3. **Importance** – Strengthen with real evidence (scale, impact, urgency).
  4. **Difficulty** – List more bullet points why ours is better
  5. **Novelty Comparison** – Include more papers for comparison
  6. **Approach (Most Important)** List more bullet points corresponding to Difficulty and check how to design methods that ours is better

  Please make sure the following requirements are kept:
  1. **Problem**
    * The problem should be a precise, not too large and not ambiguous problem.
    * Think from demand: what are the attractive/necessary demands from the research community that have not been fully satisfied?
    * A question that can be answered with yes/no + some reason. For example, Can LLM do well on NER task?
  2. **Importance**
    * Explain why the problem matters and is interesting.
    * Use recent literature to support your points.
    * Relate to the community consensus/trend to show your importance. We point out the gap/potential/opportunities between our idea and the current consensus.
    * Recent example: Scalable LLM ranker? Graph based ranker
  3. **Difficulty**
    * Explain why the problem is technically challenging and why simple methods fail.
    * Cite prior works that highlight these challenges.
    * List 2-4 reason why this is very challenging. List with (1) (2) (3)
  4. **NoveltyComparison**
    * Write a **detailed comparison (≥150 words)** explaining how your idea differs from existing work.
    * Discuss related works, their approaches, and limitations. Analyze your prior work’s limitations, and argue that yours is better
    * Explain why past methods don’t fully solve the problem and what’s new about yours.
    * Prove your novelty. No one has solved XYZ, but we did. These limitations can be perfectly solved after our paper.
  5. **Approach**
    * Describe the **core algorithm/mechanism** of your proposed method.
    * Need to explain each point in the difficulty why our proposed method can solve them. List explanation for each in the difficulty
    * Focus on your novel component, not standard preprocessing or ethics.
    * IMPORTANT! You need to make sure that your proposed method can solve the problem you propose. Need to include the reason why such method can solve.


  The output should be in this form
  ```json
  {{
    "Name": "",
    "Title": "",
    "Problem": "",
    "Importance": "",
    "Difficulty": "",
    "NoveltyComparison": "",
    "Approach": "",
    "Interestingness": 0,
    "Feasibility": 0,
    "Novelty": 0,
    "IntentAlignment": 0,
    "Score": 0
  }}
  ```

novelty_prompt: |
  Round {current_round}/{num_rounds}.
  You are assessing the novelty of the following research idea in the context of the original intent:

  ORIGINAL INTENT:
  ```
  {intent}
  ```

  CURRENT IDEA:
  ```
  {idea}
  ```

  SEARCH RESULTS FROM PREVIOUS QUERY:
  ```
  {last_query_results}
  ```

  Respond in the following format:

  THOUGHT:
  <THOUGHT>

  DECISION:
  <DECISION>

  In <THOUGHT>, carefully analyze the idea's novelty by:
  1. First explicitly assess how well the idea aligns with the original intent
  2. Compare the idea against the search results to identify similarities and differences
  3. Determine if any existing work already implements the core approach for the same intent
  4. Consider if the idea offers meaningful innovation beyond existing approaches
  5. Assess whether minor variations from existing work constitute sufficient novelty

  In <DECISION>, write either:
  - "NOVELTY CHECK: CONTINUE" if you need more information to make a decision. In this case, explain what specific information you need.
  - "NOVELTY CHECK: NOVEL" if you've determined the idea is novel. Briefly explain why.
  - "NOVELTY CHECK: NOT NOVEL" if you've determined the idea is not novel. Briefly explain why and cite the specific paper(s) that demonstrate lack of novelty.

experiment_plan_prompt: |
  Given the following research idea:

  ```
  {idea}
  ```

  And the ORIGINAL INTENT:
  ```
  {intent}
  ```

  Develop a comprehensive experimental plan. You must provide TWO separate outputs: a JSON object and a Markdown table.

  IMPORTANT: Your experiment plan MUST directly support investigation of the original intent. Ensure that your experimental design directly addresses the research question posed in the original intent and builds upon the idea that was generated. Your response should heavily rely on literature search; use the approaches from relevant papers as references.

  Respond in the following format:

  THOUGHT:
  <THOUGHT>

  EXPERIMENT PLAN JSON:
  ```json
  <JSON>
  ```
  EXPERIMENT PLAN MARKDOWN TABLE:
  ```markdown
  <MARKDOWN_TABLE>
  ```
  In <THOUGHT>, briefly discuss your reasoning behind the chosen approach.

  In <JSON>, provide the plan in JSON format with these fields:
  - "Model": Detailed description of model architecture with specific components. You must stay within LIGHTWEIGHT architectures:
    * Allowed families: bag-of-words/logistic regression, shallow MLP/FFN (≤2 hidden layers, ≤128 units), shallow CNN (≤3 convolutional layers, ≤32 filters), linear SVM, single-layer GRU (≤64 hidden units).
    * Forbidden: Transformers (BERT, GPT, ViT, etc.), large pretrained backbones, graph neural networks, diffusion models, custom random feature generators.
    * Document input/output dimensions so coder can wire tensors without guessing. Provide literal numeric values (e.g., use 3072 instead of `32 * 32 * 3`).
    * Absolutely NO comments (`//` or `/* */`) inside JSON values.
    * Keep parameter counts tiny: total hidden units per layer ≤ 128, total trainable parameters ≤ ~100k.
  - "Dataset": Detailed description of dataset including:
    * "Name": MUST be a real dataset available on HuggingFace Datasets (verify it exists!)
    * "Size": Number of samples (limit each split to ≤ 5,000 training examples and ≤ 2,000 for validation/test in the initial plan to keep runtime low)
    * "Splits": Train/validation/test split ratios (explicitly state how you will downsample large datasets to the limits above)
    * "Preprocessing": Required preprocessing steps (e.g., lowercasing text, CountVectorizer/TF-IDF, image normalization). Specify resulting feature shapes.
    * Absolutely NO comments (`//` or `/* */`) or inline expressions inside the JSON.
    * "Load_Command": The exact Python call that loads the dataset programmatically (e.g., `datasets.load_dataset("ag_news")`, `datasets.load_dataset("imdb")`, `sklearn.datasets.load_iris()`). The identifier MUST be valid on the target library—generic names like `'coco'` or any path placeholders are forbidden. If a dataset needs manual downloads, select a different public dataset that can be fetched automatically.
    * CRITICAL: Stay within this downloadable shortlist unless the intent explicitly demands otherwise: `{{"ag_news", "imdb", "yelp_polarity", "cifar10", "mnist", "fashion_mnist", "amazon_polarity"}}`. If none fit, clearly justify an alternative that is still auto-downloadable.
  - "Metric": Description of evaluation metrics with justification (use key name "Metric", NOT "Evaluation_Metrics")
    * Include a short self-check note confirming that dataset size limits, model simplicity, and lack of comments are satisfied.

  In <MARKDOWN_TABLE>, provide a DETAILED experimental plan as a Markdown table. This table MUST include the following columns and rows:
  - "Component": The part of the experiment
  - "Specification": A DETAILED description with specifics (not vague descriptions)
  - "Justification / Rationale": Why this choice is appropriate, citing relevant literature
  - "Status": Leave this column EMPTY
  - Include an additional row titled **Sanity Checks** listing:
    * Dataset subsampling strategy confirming ≤5,000 train / ≤2,000 val/test examples.
    * Model parameter count estimate (≤100k parameters).
    * Confirmation that JSON contains no inline comments or expressions.

  **REQUIRED ROWS** (at minimum):
  1. **Model Architecture**: Specify exact architecture, layers, dimensions, parameters
  2. **Dataset**: Name, size, splits (train/val/test), preprocessing steps
     - **CRITICAL DATASET REQUIREMENT**: You MUST use a dataset that is:
       a) Publicly available on HuggingFace Datasets Hub (https://huggingface.co/datasets) and can be loaded with `datasets.load_dataset()`, OR
       b) A standard benchmark dataset with clear download instructions (e.g., via torchvision, tensorflow_datasets)
       - ❌ DO NOT invent dataset names (e.g., "Gutenberg Dataset" does not exist on HuggingFace)
       - ✅ Prefer the shortlist `ag_news`, `imdb`, `yelp_polarity`, `cifar10`, `mnist`, `fashion_mnist`, `amazon_polarity`. If intent requires a different dataset, confirm it is auto-downloadable and provide the exact `datasets.load_dataset` call.
       - ❌ DO NOT use placeholder file paths such as `path/to/...`; rely on library loaders only
       - ✅ Verify the dataset exists (including the config/split name) before specifying it
       - Include the exact load command: e.g., `datasets.load_dataset("wikitext", "wikitext-2-raw-v1")`
  3. **Baselines**: List 3-5 specific baseline methods to compare against (with citations)
  4. **Training Setup**: Optimizer, learning rate, batch size, epochs, hardware
  5. **Evaluation Metrics**: Primary and secondary metrics with justification
  6. **Hyperparameters**: Key hyperparameters and their values/ranges

  Each specification should be concrete and implementation-ready, not high-level or vague.

non_experiment_plan_prompt: |
  Given the following non-experimental research idea:

  ```
  {idea}
  ```

  And the ORIGINAL INTENT:
  ```
  {intent}
  ```

  Develop a comprehensive research plan. You must provide TWO separate outputs: a JSON object and a Markdown table.

  IMPORTANT: Your research plan MUST directly support the investigation of the original intent.

  Respond in the following format:

  THOUGHT:
  <THOUGHT>

  EXPERIMENT PLAN JSON:
  ```json
  <JSON>
  ```
  EXPERIMENT PLAN MARKDOWN TABLE:
  ```markdown
  <MARKDOWN_TABLE>
  ```

  In <THOUGHT>, briefly discuss your reasoning behind the chosen approach.

  In <JSON>, provide a concise research plan in JSON format with a SINGLE field:
  - "Research_Plan": A detailed paragraph describing the complete research plan.

  In <MARKDOWN_TABLE>, provide a more detailed plan as a Markdown table. This table MUST include the following columns:
  - "Phase": The stage of the research (e.g., Literature Review, Data Collection, Analysis).
  - "Methodology": The specific methods to be used in this phase.
  - "Expected Outcome": The deliverable or result of this phase.
  - "Status": Leave this column EMPTY.
