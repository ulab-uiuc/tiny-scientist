write_system_prompt: |
  You are an ambitious AI PhD student who is looking to publish a paper that will contribute significantly to the field.
  You have already figured out the research idea and the experiments you want to run.
  Now, you need to write the paper draft based on the template provided in `latex/template.tex`.

  ⚠️ Do not include any citations or \cite{{}} commands in the content.
  Just focus on writing clear and coherent content that explains the motivation, methodology, experiments, and results.
  
  **IMPORTANT - Citation Principles** (when citations are added later):
  - Each citation must support a specific claim, statement, or finding in that sentence
  - Citations should be placed right after the claim they support
  - Do NOT add citations randomly or at the end of unrelated sentences
  - Only cite papers that are directly relevant to the specific content of that sentence
  
  **CRITICAL - LaTeX Format Only (NO Markdown)**
  - NEVER mix Markdown and LaTeX syntax (e.g., **text\textbf{{ is WRONG)
  - Use \textbf{{text}} for bold, NEVER **text**
  - Use \textit{{text}} for italic, NEVER *text*
  - Use \texttt{{text}} for code/monospace, NEVER `text`
  - Example: \textbf{{Data Collection}}: not **Data Collection**: or **Data Collection\textbf{{
  - When including tables, always use proper LaTeX tabular format (not Markdown)
  - Avoid using Markdown-style tables (e.g., those starting with `| Column |`) — they are not compatible with LaTeX rendering and will break the document
  - For algorithms, use \begin{{algorithmic}}...\end{{algorithmic}} with \STATE, \FOR{{...}}, \IF{{...}}, \WHILE{{...}}, \ENDFOR, \ENDIF, \ENDWHILE (uppercase commands)

  [LaTeX Formatting Reminder]
  - Use `\%` to indicate percentage values (e.g., 93\%)
  - Do not escape comment `%` symbols (e.g., `% comment`)
  - Wrap math expressions with `$...$`
  - Escape special characters, `_` as `\_`, `&` as `\&`, `#` as `\#`, etc.
  - For algorithms, use uppercase commands like:
    \begin{{algorithm}}
    \caption{{Algorithm Name}}
    \begin{{algorithmic}}
    \STATE Initialize parameters $\theta$
    \FOR{{each iteration $t = 1$ to $T$}}
        \STATE Update $\theta$ using gradient
    \ENDFOR
    \end{{algorithmic}}
    \end{{algorithm}}

  The purpose of this draft is to flesh out the content. Citations will be added later during the refinement process.
  Your goal is to make the paper look and feel like a real submission to NeurIPS or Nature. All figures, tables, and text must be cleanly formatted and publication-ready.

  Here's an example of a well-written paper draft to guide your writing style, tone, and section lengths
  {example_paper_draft}

write_system_prompt_related_work:
  You are an ambitious AI PhD student who is looking to publish a paper that will contribute significantly to the field.
  You have already figured out the research idea and the experiments you want to run.
  Now, you need to write the paper draft based on the template provided in `latex/template.tex`.

section_tips:
  Abstract: |
    - TL;DR of the paper
    - What are we trying to do and why is it relevant?
    - Why is this hard?
    - How do we solve it (i.e. our contribution!)
    - How do we verify that we solved it (e.g. Experiments and results)

    Please make sure the abstract reads smoothly and is well-motivated. This should be one continuous paragraph with no breaks between the lines. Should not be too long, try to grab its important points. Each point should only have one or two sentences.

  Introduction: |
    - Write 5 paragraphs for the instructions. Each paragraph should answer one question.
    - paragraph1: What is the problem?
      * Start broad with background and context
      * Narrow down to the specific gap or unsolved issue
      * End with a sharp statement of the problem
      * probably end the paragraph with a question like: Can LLM simulate social interactions?

    - paragraph2: Why is it interesting and important?
      * Connect to research community demand
      * Tie to trends or consensus in the field
      * Show broader impact or urgency

    - paragraph3: Why is it hard? (E.g., why do naive approaches fail?)
      * Explain inherent difficulty of the problem
      * Give examples of why naive approaches fail
      * Clarify how these challenges block real progress

    - paragraph4: Why hasn't it been solved before? 
          (Or, what's wrong with previous proposed solutions? How does mine differ?)
      * Acknowledge prior work and attempts
      * Point out their shortcomings or gaps
      * Clearly state how your approach differs and improves

    - paragrap5: What are the key components of my approach and results? 
      * Introduce your core approach in one sentence
      * Break into 2–3 main components
      * Summarize key results

  Related_Work: |
    - Academic siblings of our work, i.e. alternative attempts in literature at trying to solve the same problem.
    - Goal is to "Compare and contrast" - how does their approach differ in either assumptions or method? If their method is applicable to our Problem Setting I expect a comparison in the experimental section. If not, there needs to be a clear statement why a given method is not applicable.
    - Note: Just describing what another paper is doing is not enough. We need to compare and contrast.
    - **STRUCTURE REQUIREMENT**: Write **3 paragraphs** (at most 4 paragraphs). Each paragraph should focus on a distinct category or theme of related work.

  Method: |
    - The first paragraph in the method should be problem definition. provide math notation and basic definition for the problem
    - We need to use math to define and describe methods clearly
    - You need to focus on describing different parts of your CORE method
    - Do you mention concrete implementation details and experimental setup
    - Need to use math to describe the whole process of your algorithm that you propose
    - If necessary, have a pseudo code or algorithm description to describe your algorithm. But in mose cases, it is not necessary. It depends on the topic and research question you want to answer.
    - Very important!!! You need to explain your motivation for any methodology design. Any design is not from scratch, you need to have a good reason (very brief reason) for that before you start talking about the details of each part of your methods.

  Experimental_Setup: |
    - How do we test that our stuff works? Introduces a specific instantiation of the Problem Setting and specific implementation details of our Method for this Problem Setting.
    - Do not imagine unknown hardware details.
    - Includes a description of the dataset, evaluation metrics, important hyperparameters, and implementation details.

  Results: |
    - Shows the results of running Method on our problem described in Experimental Setup.
    - Statement should be emphasized and as the paragraph title
    - You need to first describe the trends and improvement ratio and change in the Table. include all the basic information
    - Furthermore, you need to analyze and provide reasons about why this works or not

  Discussion: |
    
    **CRITICAL - ANTICIPATE REVIEWER CHALLENGES**: 
    You MUST proactively identify and address potential weaknesses that reviewers might challenge. For EACH subsection, pose a research question (RQ) that questions the validity of your method, then provide evidence-based defense.
    
    **Required Challenge Categories** (address at least 3-4):
    1. **Data Integrity**: Could the results be due to data leakage, train-test overlap, or contamination?
    2. **Baseline Fairness**: Are baselines implemented fairly? Could they be stronger with better hyperparameters?
    3. **Ablation Validity**: Does the improvement really come from your claimed innovation, or from other factors (e.g., more parameters, longer training, different data preprocessing)?
    4. **Generalization**: Do results hold across different datasets, domains, or settings? Or only in cherry-picked scenarios?
    5. **Statistical Significance**: Are improvements statistically significant? Could they be due to random variance?
    6. **Evaluation Metrics**: Are the evaluation metrics appropriate? Could they be gamed or misleading?
    7. **Implementation Details**: Are there hidden implementation tricks that aren't disclosed?
    8. **Computational Cost**: Is the improvement worth the added complexity/cost?
    
    **Format for Each Challenge**:
    \subsection*{{RQ#: [Challenging question that makes your method look fake/weak]?}}
    \textit{{[Short answer defending your method]}}
    
    [2-3 paragraphs with concrete evidence from experiments, ablations, or additional analysis that addresses the challenge]
    
    - Each subsection should be a research question that challenges your method
    - Be specific and quantitative in your defense (cite figures, tables, numbers)

  Conclusion: |
    - **KEEP IT SHORT**: ONE single paragraph only (~100-150 words)
    - Briefly restate the problem and key contribution (1-2 sentences)
    - Summarize main findings (1-2 sentences)
    - Mention one limitation or future direction (1 sentence)
    - Do NOT repeat detailed content from Discussion or Results
    - Academic papers favor concise, single-paragraph conclusions

error_list: |
  - Unenclosed math symbols
  - Only reference figures that exist in our directory
  - LaTeX syntax errors
  - Numerical results that do not come from explicit experiments and logs
  - Repeatedly defined figure labels
  - References to papers that are not in the .bib file, DO NOT ADD ANY NEW CITATIONS!
  - Unnecessary verbosity or repetition, unclear text
  - Results or insights in the `notes.txt` that have not yet need included
  - Any relevant figures that have not yet been included in the text
  - Closing any \\begin{{figure}} with a \\end{{figure}} and \\begin{{table}} with a \\end{{table}}, etc.
  - Duplicate headers, e.g. duplicated \\section{{Introduction}} or \\end{{document}}
  - Unescaped symbols, e.g. shakespeare_char should be shakespeare\\_char in text
  - Incorrect closing of environments, e.g. </end{{figure}}> instead of \\end{{figure}}

multi_round_refinement_prompt: |
  You are refining the {section} section (Round {round_num}/{total_rounds}).

  **Current Focus for This Round**: {focus}

  **Current Section Content**:
  {section_content}

  **Section Guidelines**:
  {section_tips}
  
  **SPECIAL INSTRUCTION for Conclusion**: If this is the Conclusion section, you MUST keep it as ONE single paragraph (~100-150 words). Do NOT expand it. Do NOT add multiple paragraphs.

  **Full Paper Context** (all other sections for coherence check):
  {other_sections_context}

  **Refinement Instructions**:
  1. {method_specific_instruction}
  2. Improve mathematical notation and formalism
  3. Add design rationale for key choices
  4. **CRITICAL**: Read the Full Paper Context above to ensure your refined section is coherent with other sections
  5. Ensure terminology, notation, and narrative flow are consistent across the paper
  
  **CRITICAL - CITATION PRESERVATION**:
  6. **YOU MUST PRESERVE ALL EXISTING \\cite{{}} COMMANDS EXACTLY AS THEY APPEAR**
  7. Count all \\cite{{}} in the current content and ensure the same number (or more) in your output
  8. Do NOT remove, modify, or relocate existing citations unless absolutely necessary
  9. If you add new content, you may add NEW citations, but keep all existing ones
  10. Do NOT remove any existing content unless it's redundant or contradicts other sections
  
  **MEANINGFUL CITATION PLACEMENT**:
  - Each citation must support a specific claim, statement, or finding in that sentence
  - Citations should be placed right after the claim they support
  - Do NOT add citations randomly or where they don't support the sentence content

  **Common Errors to Avoid**:
  {error_list}

  **CRITICAL - LaTeX Format Only (NO Markdown)**:
  - Use \textbf{{text}} for bold, NOT **text**
  - Use \textit{{text}} for italic, NOT *text*
  - Use \texttt{{text}} for code/monospace, NOT `text`
  - For algorithms, use \STATE, \FOR{{...}}, \ENDFOR, \IF{{...}}, \ENDIF (uppercase)

  Output the refined section directly (no explanations).

add_new_citations_prompt: |
  You are adding NEW citations to the {section} section.

  **Current section content**:
  {section_content}

  **NEW papers to cite** (cite ALL of these):
  {paper_context}

  **CRITICAL INSTRUCTIONS**:
  1. Each paper is formatted as: [CITE_KEY: bibtex_key] Title (authors, venue, year)
  2. You MUST extract the bibtex_key from [CITE_KEY: bibtex_key] and use it for citations
  3. Add citations using \\cite{{bibtex_key}} format (use EXACTLY the bibtex_key shown)
  4. Cite ALL {num_papers} new papers provided above
  5. Keep ALL existing citations and content intact
  6. Add new citations in contextually appropriate places
  7. If needed, expand 1-2 sentences to naturally incorporate citations
  8. Do NOT remove or change existing citations
  9. Example: If you see [CITE_KEY: Smith2024], cite it as \\cite{{Smith2024}}

  **MEANINGFUL CITATION PLACEMENT**:
  - Each citation MUST directly support the specific claim or statement in that sentence
  - Place citations immediately after the claim they support (e.g., "Recent work shows improvement~\\cite{{Smith2024}}.")
  - Read each paper's abstract to understand what it claims, then cite it where that claim is relevant
  - Do NOT add citations randomly or at the end of unrelated sentences
  - If you need to cite a paper but no suitable sentence exists, add a new sentence that makes a claim the paper supports
  - Example: If a paper proposes a new architecture, cite it when discussing that architecture, NOT when discussing datasets

  Output the enhanced section with all citations.

refinement_prompt: |
  Great job! Now criticize and refine only the {section} that you just wrote.
  Make this complete in this pass, do not leave any placeholders.

  Pay particular attention to fixing any errors such as:
  {error_list}

  Here is the corresponding section tips:
  {section_tips}

  Here is the section to refine:
  """
  {section_content}
  """

citation_system_prompt: |
  You are an academic writing assistant helping add and embed citation coverage in a research paper.

  Your role:
  - When asked to suggest citations, return only real, published academic paper titles that are highly relevant to the given content.
  - When asked to embed citations, you will be provided with papers in the format: [CITE_KEY: bibtex_key] Paper Title (authors, venue, year)
  - You MUST extract and use the EXACT bibtex_key from [CITE_KEY: bibtex_key] when inserting `\cite{{bibtex_key}}` commands.
  - For example, if you see [CITE_KEY: smith2023nlp] Natural Language Processing..., you must write \cite{{smith2023nlp}}
  - NEVER invent citation keys - only use the keys explicitly provided in [CITE_KEY: ...]

  **CRITICAL - MEANINGFUL CITATION PLACEMENT**:
  - Each citation MUST support a specific claim, method, finding, or statement in that sentence
  - Citations should be placed immediately after the claim they support
  - Do NOT randomly insert citations without clear relevance to the sentence content
  - If a paper is not directly relevant to a specific claim, do NOT force it into that sentence
  - Each citation should have a clear purpose: supporting a claim, referencing a method, citing prior work, etc.

  Do not invent or fabricate any citations.
  Do not output BibTeX, author names, or publication details.
  
  **CRITICAL - LaTeX Format Only (NO Markdown)**:
  - Use \textbf{{text}} for bold, NOT **text**
  - Use \textit{{text}} for italic, NOT *text*
  - Use \texttt{{text}} for code/monospace, NOT `text`
  Be thorough - missing citations is not acceptable
  Always follow the expected output format (JSON array or updated LaTeX content), with no extra commentary or explanation.

citation_search_query_prompt: |
  You will generate short keyword search queries optimized for Semantic Scholar (and OpenAlex) search.

  Inputs:
  - Idea Title: {idea_title}
  - Problem: {problem}
  - Novelty: {novelty}
  - Experiment: {experiment}
  - Section: {section}
  - Snippet: {snippet}

  Requirements:
  - Return ONLY a JSON array of strings. No extra text.
  - Each query should be 3–8 words, concise, human-readable.
  - Use ASCII letters/numbers/spaces only. No punctuation except spaces.
  - Do not include quotes inside items, markdown, or explanations.
  - Focus on core entities: tasks, methods, datasets, domains, techniques.
  - Provide 6–10 high-quality, diverse queries.

  Example output:
  [
    "graph neural network link prediction",
    "llm reinforcement learning distillation",
    "transformer pruning computer vision",
    "contrastive learning medical imaging",
    "few shot domain adaptation nlp",
    "retrieval augmented generation evaluation"
  ]

abstract_prompt: |
  You are writing the Abstract section of a top-tier AI research paper.
  Some tips are provided below:
  {abstract_tips}

  Here is the research idea that the paper is based on:

  - Title: **{title}**
  - Research Problem: **{problem}**
  - Importance: **{importance}**
  - Difficulty: **{difficulty}**
  - Novelty: **{novelty}**
  - Experiment Plan: **{experiment}**

  IMPORTANT: The full paper has already been written. Below is the complete content of all sections.
  Your task is to write an abstract that accurately summarizes what was actually written in the paper,
  not just the initial idea. Pay attention to the actual methods used, experiments conducted, and results obtained.

  === FULL PAPER CONTENT ===
  {full_paper_content}
  === END OF PAPER CONTENT ===

  Based on the full paper content above, write a concise abstract that:
  1. States the problem addressed in the paper
  2. Explains why it's important and challenging
  3. Describes the proposed solution/method
  4. Summarizes the key experiments and results
  5. Highlights the main contributions

  The output must be pure LaTeX and enclosed with \begin{{abstract}} ... \end{{abstract}}.
  Be sure to first name the file and use *SEARCH/REPLACE* blocks to perform these edits.


section_prompt:
  Introduction: |
    You are writing the Introduction section of a top-tier AI research paper.

    An example for an introduction should look like:

    LLMs have proved to be powerful copilots in scientific research~\citep{{AI4Science2023TheIO}}, demonstrating their great potential for accelerating scientific discovery.
    Despite the promising finding, a more ambitious question remains: \textit{{Can we simulate the human research community with LLMs}}? Answering such a question has multiple benefits: (1) simulating the human research community helps understand the underlying process behind the discovery of existing research ideas; (2) it can further help democratize and accelerate the discovery process of new research ideas.

    However, simulating the human research community is challenging, as it involves leveraging multiple LLM agents to interact with complex research data. While existing multi-agent LLM frameworks have been successfully applied to areas like social simulation~\citep{{zhou2023sotopia,Gao2023S3SS}} and game simulation~\citep{{hua2023war,xu2023language}}, they are not well-suited for simulating research communities due to the complexity of collaborative research activities like paper writing and review writing. Although recent efforts have explored research automation using LLMs, these frameworks are typically limited to specific research tasks, such as idea generation~\citep{{girotra2023ideas, baek2024researchagent}} or code experimentation~\citep{{huang2024mlagentbench}}, or focus on simulating single-agent workflows~\citep{{lu2024ai}}. These frameworks cannot simulate collaborative research activities where researchers with diverse backgrounds work together to brainstorm ideas, review papers, etc—processes that are fundamental to modern human research.

    \paragraph{{Research community as graph}}
    Our key observation is that the deeply interconnected research community can be naturally represented as graphs. Indeed, similar graph structures like citation networks~\citep{{newman2001structure}} and academic social networks~\citep{{Tang2008ArnetMinerEA}} have been extensively studied within data mining research, with proven values in applications such as citation prediction~\citep{{holm2020longitudinal}}, recommendation~\citep{{West2016ARS}}, and community detection~\citep{{Yang2012DefiningAE}}.
    However, introducing LLMs to a graph-structured research community can extend these previous works from prediction and analysis with existing data to dynamic simulation and real-time forecasting.

    \paragraph{{Novel framework for research simulation}}
    In this work, we propose \envname, a simulator of the human research community. To bridge the gap between existing multi-agent simulation frameworks and the complexity of research activities, we propose a graph-based framework, inspired by the message-passing mechanism in Graph Neural Networks (GNNs), for multi-agent simulation.
    Concretely, as shown in Figure \ref{{fig:community-graph}}, we propose a new concept of \textit{{agent-data graph}} with 2 generic types of nodes: (1) \textit{{agent}} nodes, suitable for entities like agents; (2) \textit{{data}} nodes, suitable for entities such as papers, reviews, and blogs. 
    Agent-data graphs are unique from standard heterogeneous graphs; here, the key conceptual difference between agent and data nodes is that an agent node can be considered a function over data nodes.
    To inference on agent-data graphs, we propose a \textit{{TextGNN}} framework where message-passing processes are defined based on text-form information processing with LLMs, thanks to their strong in-context learning~\citep{{wei2023larger}} and reasoning~\citep{{lee2024reasoning}} ability. 
    We apply the proposed agent-data graph and TextGNN to the research simulation. Here, a research community can be regarded as a special form of agent-data graph, called \textit{{community graph}}, with research agents and research papers as two types of nodes, and we consider three types of edges (review, author, and cite) in the graph. Different community activities, such as paper writing and review writing, can be modeled as special message-passing processes on the community graph.

    \paragraph{{Novel evaluation for research simulation}} 
    With \envname for research simulation, a further research question is to evaluate the quality of that. Prior works primarily use human evaluation with breakdown metrics such as novelty, excitement, feasibility, and expected effectiveness~\citep{{si2024can,hu2024nova}}. These approaches inevitably suffer from subjectiveness and high costs. In our work, since \envname functions as a simulator, our primary focus is on measuring how closely its outputs align with those of the real-world research community. Community graphs naturally provide a similarity-based evaluation method by masking a given paper node in the community graph and evaluating whether a simulator can reconstruct the masked nodes. This definition focuses on simulation similarity, making it scalable and objective. Based on such a node masking prediction task, we build a benchmark called \benchname with 1,000 paper writing tasks and 200 review writing tasks requiring multi-agent collaboration.

    \paragraph{{Main discoveries}} Based on the evaluation results from \benchname, we highlight three key findings: (1) \envname effectively simulates collaborative research activities, achieving an average similarity score of 0.68 for paper writing and 0.49 for review writing, as measured by the state-of-the-art text embedding model; (2) \envname demonstrates robustness and effectiveness in research simulation, showing improvement when more agents are added and maintaining performance when including unrelated papers; (3) \envname inspires interdisciplinary research, generating innovative ideas that combine insights from NLP, criminology, and astronomy and does not exist in the real-world research.


    Some tips are provided below:
    {section_tips}

    The following is the idea that you need to write introduction for:

    - Title: **{title}**
    - Research Problem: **{problem}**
    - Importance: **{importance}**
    - Difficulty: **{difficulty}**
    - Novelty: **{novelty}**
    - Experiment Plan: **{experiment}**

    **IMPORTANT**: You have access to the Abstract and Related Work sections that were already written. Make sure your Introduction builds upon and expands the Abstract naturally, while incorporating insights from the Related Work to position your contribution in the broader research context.

    **Abstract content:**
    {abstract_content}

    **Related Work content (for background context):**
    {related_work_content}

    **Your goal is to write a compelling, multi-paragraph introduction** that logically develops the background, motivation, and research gap. The tone should be formal, informative, and engaging.

    Be sure to use \cite or \citet where relevant, referring to the works provided in the file.
    Do not cite anything that is not already in `references.bib`. Do not add any new entries to this.
    
    **Citation Usage**: Each citation must support a specific claim in that sentence (e.g., cite prior work when stating what has been done before, cite foundational work when introducing concepts).

    Use at **5 paragraphs**, and aim for **250–300 words**.

    Keep the experimental results (figures and tables) only in the Results section, and make sure that any captions are filled in.
    In this pass, do not reference anything in later sections of the paper.

    Begin with:
    \section{{Introduction}}
    ...

  Method: |
    Please fill in the Method section of the writeup. The Method section should **clearly define the approach taken in this study**, ensuring that readers can understand and, if needed, replicate the implementation. This section must be based on **both the proposed experiment and the provided code**.

    **EXPERIMENT DESIGN REFERENCE** (Use this to inform your writing, do NOT copy the table directly):
    {experiment_table}

    CRITICAL REQUIREMENTS:
    1. **Problem Formulation (MANDATORY)**: Start with a formal mathematical definition
       - Define the problem space: $\mathcal{{X}}, \mathcal{{Y}}$
       - Define the objective function with proper notation
       - State any assumptions clearly
    
    2. **Multi-Level Structure (MANDATORY)**: Break down into subsections:
       - Use \paragraph{{}} for main components (3-5 paragraphs minimum)
       - Each paragraph should focus on ONE specific aspect
       - Follow a logical flow: Overview → Components → Integration
    
    3. **Mathematical Rigor (MANDATORY)**: 
       - Use equations for key transformations: $f(x) = ...$
       - Number important equations: \begin{{equation}}...\end{{equation}}
       - Define all variables and notations clearly
    
    4. **Algorithm Description (if applicable)**:
       - Include pseudo-code using \begin{{algorithm}} or \begin{{algorithmic}}
       - Or describe step-by-step process with clear enumeration
    
    5. **Architectural Details**:
       - Describe model architecture with dimensions
       - Specify layer configurations, activation functions
       - Reference to figures if available
    
    6. **Design Justification**:
       - For EACH major design choice, explain WHY (1-2 sentences)
       - Connect to existing literature when possible
    
    An example for method writing is as follow:

    \paragraph{{Definition of artifact graphs}}
    We further define the artifact (model-data) graph as a bipartite graph $\mathcal{{G}} = (\mathcal{{V}}, \mathcal{{E}})$, where the node set $\mathcal{{V}} = \mathcal{{V}}_m \cup \mathcal{{V}}d$ consists of two disjoint types of nodes: model nodes and dataset nodes. The edge set $\mathcal{{E}} = \mathcal{{E}}{{md}}$ represents evaluation relationships between models and datasets. Each dataset node $v \in \mathcal{{V}}_d$ is associated with attributes such as metadata and task descriptions, while each model node $u \in \mathcal{{V}}_m$ is associated with model specifications such as architecture, parameters, and configuration.

    \paragraph{{Uniqueness of artifact graphs}}
    Unlike general bipartite graphs, the edges in a artifact graph carry rich semantic information: each edge $(u, v) \in \mathcal{{E}}_{{md}}$ is annotated with evaluation results such as accuracy, F1, perplexity, or other task-specific metrics. This allows the graph to capture fine-grained performance relationships between models and datasets. Such a structure enables downstream applications like model routing, automatic benchmarking, and meta-analysis of model generalization across diverse tasks.

    \paragraph{{Edge collection}}
    To construct the edges in our artifact graph, we extract ground-truth evaluation metrics directly from the HuggingFace model cards. Specifically, we parse the README files to identify model–dataset pairs together with their reported performance scores. After matching model and dataset names with their canonical entries on the HuggingFace platform, we obtain a clean set of evaluation edges. In total, this process yields 2,561 perfectly matched evaluation records, which serve as the edge attributes in our graph.

    \paragraph{{Node collection}}
    To construct the node set, we collect information for both models and datasets. For each model, we crawl its README and metadata fields (e.g., architecture, parameters, tags), and for each dataset, we extract its description and metadata (e.g., domain, size, license). We retain only nodes that participate in at least one evaluation edge, filtering out isolated artifacts. The resulting graph contains 2,144 nodes in total, consisting of 1,757 models and 387 datasets. On average, each model connects to 1.46 datasets, while each dataset connects to 6.62 models, indicating that the graph is relatively sparse.

    \section{{Linking Scientific artifacts for automatic discovery}}
    In this section, we first formalize the problem definition of automatic discovery using the artifact-graph formulation. We then introduce our scalable solution, which addresses this problem via a two-stage \textit{{prediction–verification}} framework.

    \subsection{{Definition of Automatic Discovery}}
    Let $\mathcal{{G}} = (\mathcal{{V}}, \mathcal{{E}})$ denote the artifact graph, where $\mathcal{{V}} = \mathcal{{V}}_m \cup \mathcal{{V}}_d$ consists of model nodes $\mathcal{{V}}_m$ and dataset nodes $\mathcal{{V}}_d$, and $\mathcal{{E}}_{{md}} \subseteq \mathcal{{V}}_m \times \mathcal{{V}}_d$ represents evaluation edges. Each edge $(m,d) \in \mathcal{{E}}_{{md}}$ is annotated with a performance score $s(m,d)$ (e.g., accuracy, F1). We define the automatic discovery problem as finding a missing link $(m,d) \notin \mathcal{{E}}_{{md}}$ such that the predicted score $\hat{{s}}(m,d)$ exceeds the best observed score on dataset $d$:

    \begin{{equation}}
    \hat{{s}}(m,d) > \max_{{(m', d) \in \mathcal{{E}}_{{md}}}} s(m',d).
    \end{{equation}}

    In other words, the discovery target is to identify model–dataset pairs that are not yet connected in $\mathcal{{G}}$ but are expected to outperform all existing models on the same dataset.


    \subsection{{Scalable Link Discovery Framework}}
    To conduct automatic link discovery in a scalable manner, we adopt a two-stage framework: (1) prediction and (2) verification. Since verification through actual model evaluation is computationally expensive, the prediction stage serves as a crucial filter to eliminate unpromising candidates and focus resources on the most likely discoveries.

    \paragraph{{Prediction}}
    In the prediction stage, the goal is not only to estimate $\hat{{s}}(m,d)$ for missing edges but also to rank all candidate links by their discovery potential. Concretely, we train a model that learns a scoring function

    \begin{{equation}}
    \hat{{s}}: \mathcal{{V}}_m \times \mathcal{{V}}_d \to \mathbb{{R}},
    \end{{equation}}

    which assigns each unseen pair $(m,d)\notin\mathcal{{E}}_{{md}}$ a predicted performance. The model is optimized to rank these candidates relative to observed edges, so that links expected to outperform existing results on the same dataset are ranked highest. The output is an ordered list of missing edges prioritized by their likelihood of being high-value discoveries.

    \paragraph{{Verification}}
    Once the ranked list is produced, we proceed to the verification stage. Given limited computational resources, we select the top-$k$ predictions for each dataset to maximize the chance of identifying true discoveries. Verification is performed by a coding agent designed for model–dataset auto-evaluation: it automatically downloads the target model and dataset from HuggingFace, loads them in the correct configuration, and executes an inference pipeline to obtain evaluation results. This design makes verification both feasible and reliable, leveraging the standardized structure of HuggingFace artifacts to enable large-scale, automated evaluation.

    \paragraph{{Overall algorithm}} Based on the two stages and the overall framework is predict-then-verify.


    Some tips are provided below:
    {section_tips}

    - Research Problem: **{problem}**
    - Importance: **{importance}**
    - Difficulty: **{difficulty}**
    - Novelty: **{novelty}**
    - **Proposed Approach** (from the original idea): **{approach}**
    - Experiment Plan: **{experiment}**

    **Technical Details Extracted from Implementation:**
    {code_method_details}

    **Full Implementation Code (for reference):**
    python
    {code}
    
    **IMPORTANT GUIDELINES**:
    1. **CRITICAL**: Base your Method section on the **Proposed Approach** described above - this is the core methodology you must explain in detail
    2. Structure your Method section with multiple \paragraph{{}} blocks (minimum 3-5)
    3. Start with formal problem definition using mathematical notation
    4. Include equations for key transformations (use \begin{{equation}}...\end{{equation}})
    5. For each major component, explain BOTH what it does AND why (design rationale)
    6. If the code reveals specific architectures/hyperparameters, incorporate them with proper notation
    7. Build upon the motivation from previous sections while introducing technical details
    8. Ensure the Method section fully elaborates on the approach outlined in the original idea
    
    **IMPORTANT**: You have access to the previous sections that were already written. Make sure your Method section builds upon the motivation and problem definition established earlier, while introducing your technical approach clearly and coherently.

    **Previous content:**
    {previous_context}

    Be sure to use \cite or \citet where relevant, referring to the works provided in the file.
    Do not cite anything that is not already in `references.bib`. Do not add any new entries to this.
    
    **Citation Usage**: Each citation must support a specific claim in that sentence (e.g., cite papers when referencing their methods, when building upon their techniques, or when comparing approaches).

    Keep the experimental results (figures and tables) only in the Results section, and make sure that any captions are filled in. DO NOT SAY CONCRETE EXPERIMENTAL SETUP IN METHOD SECTION. JUST DESCRIBE THE ALGORITHM AND THE MAJOR COMPONENT OF YOUR METHOD.
    In this pass, do not reference anything in later sections of the paper.

    Begin with:
    \section{{Method}}
    ...

  Experimental_Setup: |
    Please fill in the Experimental Setup section of the writeup. This section should **clearly document how the experiments were conducted** so that they can be replicated by other researchers. The description should be based on the provided **implementation, dataset, and experimental configuration**.

    **EXPERIMENT DESIGN REFERENCE** (Use this to guide your writing, do NOT copy the table directly):
    {experiment_table}

    An example for experimental setting is:

    \paragraph{{Model settings}} We select Qwen2.5-7b-Instruct as our base LLM for the training of both the policy model and reward model. We select GPT-4o for LLM-as-the-judge in \sotopiaeval.

    \paragraph{{Evaluation settings}}
    We evaluate our method on two configurations of the \sotopia benchmark: (1) \sotopia-hard, and (2) \sotopia-all. \sotopia-hard is a subset of \sotopia-all, consisting of 14 challenging social scenarios identified as difficult among all scenarios, and we use 10 distinct agent pairings per scenario. For \sotopia-all, we evaluate on the full coverage of 90 social scenarios, using 2 agent combos per scenario to ensure diversity while maintaining scalability. More statistical information is in Appendix~\S\ref{{artifact-details}}.


    Some tips are provided below:
    {section_tips}

    - Research Problem: **{problem}**
    - Importance: **{importance}**
    - Difficulty: **{difficulty}**
    - Novelty: **{novelty}**
    - Experiment Plan: **{experiment}**

    The experiments were run using the following method: **{experiment}**.
    The dataset, optimizer, and model settings are derived from the provided implementation.

    python
    {code}
    
    **IMPORTANT**: You have access to the previous sections that were already written. Make sure your Experimental Setup section clearly connects to the method introduced earlier and provides specific implementation details that follow logically from the proposed approach.

    **Previous content:**
    {previous_context}

    Be sure to use \cite or \citet where relevant, referring to the works provided in the file.
    Do not cite anything that is not already in `references.bib`. Do not add any new entries to this.
    
    **Citation Usage**: Each citation must support a specific claim in that sentence (e.g., cite datasets when referencing them, cite papers when using their evaluation metrics or experimental protocols).

    Keep the experimental results (figures and tables) only in the Results section, and make sure that any captions are filled in.
    In this pass, do not reference anything in later sections of the paper.

    Begin with:
    \section{{Experimental Setup}}
    ...

  Discussion: |
    Please fill in the Discussion section of the writeup. Follow the instructions carefully.

    **EXPERIMENT DESIGN REFERENCE** (Use this to understand the experimental design and rationale):
    {experiment_table}

    An example for discussion is as follow:
    
    To assess the effectiveness of \modelname, we first ensure that its performance gains are genuine and not the result of reward hacking (RQ1). We then analyze how our improvements come from the design of the reward attribution (RQ2) and the reward combination (RQ3).

    \subsection*{{RQ1: Does our improvement come from reward hacking or shortcut learning?}}  \textit{{No, \sotopia-RL learns high-quality social skills instead of overfitting on partner models or evaluator models.}}

    Reward hacking occurs when performance improvements are confined to a specific partner model, a particular evaluator, or fail to generalize to human interactions. In Figure~\ref{{fig:goal-curve-b}} and Figure~\ref{{fig:goal-curve-c}}, we conduct a thorough analysis and show that the performance gains of \modelname are consistent across settings. Specifically, the improvements hold when switching between five different partner models and five different evaluator models, demonstrating strong robustness. Moreover, Table~\ref{{tab:human-eval}} confirms that these gains extend to human evaluation, further validating that the improvements are not evaluator-specific artifacts. Finally, Appendix~\S\ref{{safety-evaluation}} and \S\ref{{diversity-evaluation}} provide additional evidence from our safety and diversity evaluations. These results show that our trained policy model does not exhibit shortcut degeneration and remains safe and diverse.


    \subsection*{{RQ2: Why does utterance-level reward attribution bring improvement?}} \textit{{The key to effective reward design lies in offline attribution, rather than in using a strong LLM.}}

    Unlike standard MDP tasks, social interactions cannot be accurately evaluated based only on the preceding dialogue context—the quality of an utterance often depends on how the entire conversation unfolds. To address this, we attribute episode-level rewards to each utterance using information from the whole dialogue, making the reward attribution inherently \textit{{offline}}. We point out that offline attribution is the key to our improvement. Table~\ref{{tab:utterance-reward-comparison}} compares two settings for training utterance-level reward models: (1) online reward labels attributed using only the preceding dialogue history, and (2) offline reward labels attributed offline using the full episode. The offline approach achieves a substantially higher goal score (7.81) than the online approach, clearly demonstrating its effectiveness. Importantly, such an improvement does not rely on GPT-4o itself. As shown in Figure~\ref{{fig:reward-corr}}, replacing GPT-4o with weaker models for utterance-level reward labeling produces highly correlated reward signals ($>$0.7). This indicates that with well-designed prompts, precise offline credit assignment for utterance-level rewards can be reliably achieved even without state-of-the-art LLMs. More in-depth analysis and human evaluation results on utterance-level rewards are available in Appendix~\S\ref{{analysis-of-utterance-level-rewards}}.

    \subsection*{{RQ3: Why does the reward combination bring improvement?}} \textit{{Using rewards with multiple dimensions makes RM training more robust, and a better RM helps prevent RL from overfitting.}}

    To discuss why the reward combination brings improvement, we first rule out the possibility that the observed gains are merely due to reward label smoothing. To test this, we increased the attribution granularity from a 3-point to a 10-point scale and reran the pipeline. The 10-point scale did not outperform the 3-point scale on \goalcompletion\ (6.44 vs.\ 6.74), indicating that the benefits cannot be explained by finer reward scaling alone. Next, we examine whether the improvement comes from capturing complementary aspects of social interactions. As shown in Table~\ref{{tab:bc-partner-hard}}, models trained on knowledge, relationship, and goal rewards exhibit positive but only moderate correlations. This suggests that each objective captures a distinct facet, and combining them allows the model to leverage a broader range of social signals. Finally, Figure~\ref{{fig:goal-curve-a}} shows that training with combined rewards stabilizes RL and regularizes the single-dimension objective in later stages. Such regularization contributes to the consistent improvement we observe.


    **CRITICAL INSTRUCTIONS FOR YOUR DISCUSSION**:
    
    Following the example above, you MUST structure your Discussion to **anticipate and address reviewer challenges**. Your goal is to make the paper more convincing by proactively defending against potential attacks.
    
    **Required Structure**:
    1. Write an opening paragraph that outlines what challenges you will address (like the example above)
    2. Create 3-4 subsections, each formatted as:
       - \subsection*{{RQ#: [Question that challenges your method's validity]?}}
       - \textit{{[One-sentence answer defending your method]}}
       - 2-3 paragraphs with concrete evidence (cite specific experiments, figures, tables, numbers)
    
    **What Challenges to Address** (pick 3-4 most relevant to your method):
    - **Data Leakage/Contamination**: "Could the improvement be due to data leakage or train-test contamination?" → Show evidence of proper data splits, cross-validation, etc.
    - **Unfair Baseline Comparison**: "Are the baselines implemented fairly, or are they artificially weak?" → Show you used same hyperparameters, fair comparison, or cite baseline papers
    - **Source of Improvement**: "Does the gain come from your claimed innovation, or from other factors?" → Provide ablation studies showing your specific component is responsible
    - **Cherry-Picking/Generalization**: "Do results only work on this specific dataset/setting?" → Show results on multiple datasets or different configurations
    - **Statistical Significance**: "Could the improvement be due to random variance?" → Provide error bars, significance tests, or multiple runs
    - **Evaluation Metric Gaming**: "Could the metric be misleading or easily gamed?" → Show additional metrics or human evaluation
    - **Hidden Implementation Tricks**: "Are there undisclosed tricks making it work?" → Explicitly state you follow standard practices
    - **Overfitting to Test Set**: "Did you tune on test data?" → Show proper validation methodology
    
    **Tips for Strong Defense**:
    - Be specific with numbers: "improves by 15.3% (p<0.01)" not "improves significantly"
    - Reference concrete evidence: "as shown in Table X" or "Figure Y demonstrates"
    - Compare to multiple baselines or settings when possible
    - Admit minor limitations honestly, but explain why they don't invalidate the method
    
    Some additional tips for writing a strong Discussion section:
    {section_tips}

    - Experimental results: **{experiment_results}**
    - Baseline results** (may be empty if not available): **{baseline_results}**

    **IMPORTANT**: You have access to all previous sections that were already written. Make sure your Discussion section synthesizes the results with the method, experimental setup, and broader context established earlier. Analyze the implications of your findings in relation to the original problem and methodology.

    **Previous content:**
    {previous_context}

    Be sure to use \cite or \citet where relevant, referring to the works provided in the file.
    Do not cite anything that is not already in `references.bib`. Do not add any new entries to this.
    
    **Citation Usage**: Each citation must support a specific claim in that sentence (e.g., cite papers when comparing your results to theirs, when discussing similar findings, or when referencing related analyses).

    Keep the experimental results (figures and tables) only in the Results section, and make sure that any captions are filled in.
    In this pass, do not reference anything in later sections of the paper.

    Begin with:
    \section{{Discussion}}
    ...

  Results: |
    Please fill in the Results section of the research paper. Follow the instructions carefully.

    **EXPERIMENT DESIGN REFERENCE** (Use this to understand evaluation metrics and baselines, do NOT copy):
    {experiment_table}

    A good result section should look like this:

    \paragraph{{sotopia-RL helps build state-of-the-art social agents on the {{sotopia}} benchmark}} In Table~\ref{{tab:sota-performance}}, Qwen-2.5-7B-Instruct trained with {{sotopia-RL}} reaches the highest goal completion score, achieving the 7.17 score in the {{sotopia-hard}}. It indicates that our utterance-level RM provides better guidance during the training of RL. It also indicates that for multi-turn social interactions, improving the quality of single-turn interactions with suitable single-turn rewards can effectively optimize multi-turn performance. Notably, AMPO~\citep{{wang2025think}} reaches 7.50 on {{sotopia-hard}}. But it includes an explicit reasoning process and requires more than 640 inference tokens per utterance on average. Therefore, it is unfair to compare AMPO with ours since we only utilize GRPO to generate utterances without extra tokens for reasoning. Full results are available in Appendix~\S\ref{{additional-experimental-results}}.

    \paragraph{{sotopia-RL goes beyond distillation from GPT}} Our training pipeline begins with GPT-based self-play episodes and GPT-based offline reward annotations. Importantly, GPT annotations are applied \textit{{offline}}, conditioning on the entire episode, whereas during RL training, rewards are computed \textit{{online}}, conditioned only on the preceding dialogue history. As shown in Table~\ref{{tab:sota-performance}}, {{ourmodel}} not only matches but surpasses GPT-4o when used directly as a policy model (7.17 vs.\ 6.97). If {{ourmodel}} were merely a stronger form of distillation, as in behavior cloning, it could at best equal GPT-4o’s performance, not exceed it.


    Some tips for writing a strong Results section:
    {section_tips}

    - Experimental results: **{experiment_results}**
    - Baseline results** (may be empty if not available): **{baseline_results}**

    **IMPORTANT**: You have access to the previous sections that were already written. Make sure your Results section clearly connects the experimental outcomes to the method and setup described earlier. Reference the experimental design and evaluation criteria established in previous sections.

    **Previous content:**
    {previous_context}

    **Citation Usage**: If citing prior work (e.g., for baselines or comparisons), each citation must support a specific claim in that sentence (e.g., cite papers when reporting their baseline results or when comparing performance).

    Ensure that all results presented have been obtained and are recorded in the logs.
    In this pass, do not reference anything in later sections of the paper.

    Begin with:
    \section{{Results}}
    ...

  Conclusion: |
    Please write a **BRIEF and CONCISE** Conclusion section as **ONE single paragraph** (~100-150 words).

    {section_tips}

    **Previous content:**
    {previous_context}

    **CRITICAL REQUIREMENTS**:
    1. Write ONLY ONE paragraph (no paragraph breaks)
    2. Structure: Problem → Contribution → Key findings → Future direction
    3. Total length: 4-6 sentences maximum (~100-150 words)
    4. NO new information, NO detailed explanations
    5. Do NOT repeat content from Discussion or Results in detail
    6. Keep it concise and high-level

    The proposed experiment was: {experiment}

    Begin with:
    \section{{Conclusion}}
    
    [Write one paragraph here]

citation_related_work_prompt: |
  Please search and collect a paper list which contains at least {num_papers} unique, relevant published paper titles that would be appropriate for inclusion in the Related Work section of a research paper with following topic.

  The title of the paper is: {idea_title}
  The problem of the paper is: {problem}

  This is round {round_num} out of {total_rounds}. You have already collected the following papers:
  {collected_papers}

  Please return only a JSON array (strictly valid) of new paper titles. These must be actual paper titles that are published and relevant to the topic. Example:

  json
  ["Title 1", "Title 2", "Title 3"]
  

add_citation_prompt: |
  Given current version of the paper

  The title of the paper is: {idea_title}
  The problem of the paper is: {problem}
  The challenges of the paper are: {challenges}

  You are reviewing the following section: {section}

  Current content of the section:
  """
  {section_content}
  """
  Based on the type of section (e.g., Introduction, Method, Experimental Setup, Discussion) and the depth of the content provided, determine how many references would be reasonably appropriate to support the key statements and claims.

  Your task:
  - Return a list of **real, published academic papers** that should be cited in this section.
  - All references must be directly relevant to the corresponding section's current content.
  - Prefer widely recognized or foundational papers if possible.
  - Do **not** fabricate or suggest speculative titles.

  You **must return at least 6** real paper titles.
  All titles must be real and verifiable.
  Please return only a JSON array (strictly valid) of new paper titles. These must be actual paper titles that are published and relevant to the topic. Example:

  json
  ["Title 1", "Title 2", "Title 3"]
  

embed_citation_prompt: |
  You are assisting with embedding citation placeholders into an academic LaTeX section.

  You are reviewing the following section: {section}

  Here is the current content of the section:
  """
  {section_content}
  """

  You must cite **all** of the following papers using LaTeX \cite{{...}} format:
  {references}

  INSTRUCTIONS:
  - Each paper is formatted as: **[bibtex_key]** Paper Title (authors, venue, year)
  - When citing a paper, you MUST use the bibtex_key shown in square brackets [].
  - For example, if you see **[chen2022cross]** Cross-lingual Transfer..., you must write \cite{{chen2022cross}}
  - Integrate citations into the most relevant parts of the section.
  - Use `\cite{{...}}` format strictly (no markdown, no commentary).
  - Do not fabricate new citations.
  - Slightly rewrite or expand sentences as needed to fit in the citations smoothly, without changing the original meaning.
  - Preserve and return the entire section content with all citations embedded.
  - The output must be valid, standalone LaTeX with consistent formatting.

  **MEANINGFUL CITATION PLACEMENT**:
  - Each citation must support a specific claim, statement, or finding in that exact sentence
  - Place citations right after the claim they support (e.g., "Transformers excel at NLP tasks~\cite{{vaswani2017}}")
  - Read the paper title and abstract to understand what it contributes, then cite it where relevant
  - Do NOT place citations randomly or where they don't support the sentence content
  - If a paper discusses method X, cite it in sentences about method X, not about dataset Y
  - If no appropriate sentence exists for a paper, add a brief sentence making a claim that paper supports

  FINAL OUTPUT: Return only valid LaTeX (no markdown, no explanations).

related_work_prompt: |
  Please write the Related Work section of the research paper. Follow the instructions and structure strictly.

  **CRITICAL STRUCTURE REQUIREMENT**: 
  - Write **3 paragraphs** (at most 4 paragraphs)
  - Each paragraph should focus on a distinct category or theme of related work
  - Use \paragraph{{Category Name}} to start each paragraph
  - Group related papers together within each paragraph

  An example for related work is:

  \paragraph{{Graph Architecture Search}}
  Architecture search techniques have been applied to GNNs \cite{{gao2019graphnas,zhou2019auto}}.
  However, these works only focus on the design within each GNN layer instead of a general GNN design space, 
  and only evaluate the designs on a small number of node classification tasks.

  \paragraph{{Evaluation of GNN Models}}
  Multiple works discuss approaches for making fair comparison between GNN models \cite{{dwivedi2020benchmarking,errica2019fair,shchur2018pitfalls}}.
  However, these models only consider some specific GNN designs (\eg, GCN, GAT, GraphSAGE), while our approach extensively explores the general design space of GNNs.

  \paragraph{{Other graph learning models}} We focus on message passing GNNs due to their proven performance and efficient implementation over various GNN tasks. There are alternative designs of graph learning models \cite{{maron2019provably,morris2019weisfeiler,murphy2019relational,you2019position}}, but their design spaces are different from GNNs and are less modularized.

  \paragraph{{Transferable Architecture Search}}
  The idea of transferring architecture search results across tasks has been studied in the context of computer vision tasks \cite{{you2020graph,zoph2018learning}}.
  Meta-level architecture design has also been studied in \cite{{radosavovic2020designing,shaw2019meta,wong2018transfer,zamir2018taskonomy}}, with the assumption that different tasks follow the same distribution (\eg, variants of ImageNet dataset \cite{{deng2009imagenet}}).
  These approaches often make an assumption that a single neural architecture may perform well on all tasks, which fits well for tasks with relatively low variety.
  However, due to the great variety of graph learning tasks, such assumption no longer holds.

  Some tips for writing a strong Related Work section:

  {related_work_tips}

  - Academic siblings of our work, i.e. alternative attempts in literature at trying to solve the same problem.
  - Goal is to “Compare and contrast” - how does their approach differ in either assumptions or method? If their method is applicable to our Problem Setting I expect a comparison in the experimental section. If not, there needs to be a clear statement why a given method is not applicable.
  - Note: Just describing what another paper is doing is not enough. We need to compare and contrast.

  The motivation for this study is: {experiment}

  You must ground your discussion using **only** the following pre-selected relevant papers.
  Each paper is formatted as: **[bibtex_key]** Paper Title (authors, venue, year)
  
  CRITICAL: When citing a paper, you MUST use the bibtex_key shown in square brackets [].
  For example, if you see **[gao2019graphnas]** GraphNAS..., you must write \cite{{gao2019graphnas}}
  
  Use LaTeX `\cite{{...}}` format **strictly** to reference these works. Do not add any new citations.
  You must cite **all** of the following papers at least once using \cite{{...}}.
  Do not leave out any item from the list.

  **MEANINGFUL CITATION USAGE**:
  - Each citation must support a specific claim about that paper's contribution or approach
  - Cite papers when discussing their methods, findings, or limitations
  - Group related papers together and cite them when making comparative statements
  - Do NOT just list papers with citations at the end - integrate them into your arguments
  - Example: "Prior work has explored X~\cite{{paper1,paper2}}, but these approaches suffer from Y."

  {references}


title_refinement_prompt: |
  Great job! Now that there is a complete draft of the entire paper, let's refine each section again.
  Here is the current paper draft:
  {full_draft}

  Please provide a single, concise title that clearly reflects the paper's concept.
  Do not include any additional text, explanation, or formatting – output only the title.

citation_aider_format: |
  The following citations have just been added to the end of the `references.bib` file definition at the top of the file:
  """
  {bibtex}
  """
  You do not need to add them yourself.
  ABSOLUTELY DO NOT ADD IT AGAIN!!!

  Make the proposed change to the draft incorporating these new cites:
  {description}

  Use your judgment for whether these should be cited anywhere else.
  Make sure that any citation precisely matches the name in `references.bib`. Change its name to the correct name in the bibtex if needed.
  Ensure the citation is well-integrated into the text.
  You must use \cite or \citet to reference papers, do not manually type out author names.
