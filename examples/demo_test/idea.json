{
  "Name": "TraceDecomp",
  "Title": "TraceDecomp: Adaptive Prompt Decomposition with Executable Trace Anchors for Coherent Long-Range Multi-File Code Generation",
  "Problem": "Repository-scale coding tasks (SWE-bench-style bug fixes / feature additions) frequently require *coordinated* edits across multiple files while preserving shared interfaces (function signatures, types, schemas, CLI flags, config keys). A dominant failure mode for LLM coding agents is **cross-file interface inconsistency**: one file changes a signature/type/field name, but downstream call sites and imports are only partially updated, leading to cascading compiler/type-check/test failures.\n\nPrecise yes/no research question:\n**Under a fixed budget of LLM calls and tool executions, can an agent that (i) synthesizes executable \u201ctrace anchors\u201d for cross-file interfaces *before* implementations and (ii) adaptively revises its decomposition graph using compiler/type-check/test traces, achieve a higher green-build rate than strong baselines that only do static decomposition and/or local verification-driven patching?**\n\nOperationalization:\n- Setting: multi-file tasks with executable CI (unit tests + type checks/lint), e.g., SWE-bench / SWE-bench Verified and/or a curated \u201cinterface-heavy\u201d subset.\n- Budget: \u2264N LLM calls, \u2264M tool runs (pytest/mypy/tsc/ruff/etc.).\n- Primary metric: **green-build rate** within budget.\n- Secondary metrics:\n  (1) **Interface error count after first pass** (undefined symbols/import errors, signature mismatches, mypy/tsc errors attributable to API disagreement).\n  (2) **API drift**: divergence between declared anchors and realized definitions/usages across files (e.g., signature edit distance aggregated over call graph).\n  (3) **Thrash index**: number of times the same interface region (lines defining a public signature/type/schema) is re-edited.\n\nConcrete motivating failure pattern:\nA common task is \u201cadd an optional parameter/field and plumb it through\u201d (or rename a field). Standard agents often update the definition and 1\u20132 call sites but miss others; or they update call sites with different parameter names/types across modules. This passes local reasoning but fails mypy/tsc or tests with errors that are *global* in cause but *local* in manifestation. The community demand is a method that uses verification feedback not merely to patch the immediate stack-trace line, but to correct the *global plan* and stabilize interfaces early.",
  "Importance": "The evaluation culture for LLM coding is converging on **end-to-end, repository-level correctness** (CI pass/fail) rather than single-function accuracy, as reflected by widespread use of SWE-bench-style benchmarks and agentic \u201cgenerate \u2192 run tools \u2192 repair\u201d workflows. In these settings, failures are disproportionately driven by interface and dependency mismatches (imports, signatures, types, schemas) because real repositories encode correctness via compilers, type checkers, linters, and tests.\n\nRecent prompting literature also suggests that fixed prompting constraints can be brittle: work on **semantic anchors in in-context learning** argues that models exhibit stable semantic directions that are difficult to override purely via prompting, implying that static prompt templates/constraints may not robustly enforce cross-file invariants across long horizons (Semantic Anchors in ICL, 2024). This strengthens the case for *adaptive* control strategies that react to observed traces rather than assuming a one-shot plan is sufficient.\n\nFrom a systems perspective, tool feedback (type errors, stack traces) is among the most reliable supervision signals available during code generation; yet most agents use it only for local repairs, which often causes oscillations and wasted tool runs. If we can convert traces into stable, executable interface artifacts and use repeated trace patterns to revise decomposition, we can plausibly improve: (i) success under strict budgets, (ii) tool efficiency (fewer test runs per success), and (iii) developer trust via reduced API churn.\n\nFinally, \u201canchors\u201d have proven useful in other areas as stabilizing reference structures that improve adaptation/generalization (e.g., augmented anchors for label propagation in UDA; dynamic anchor optimization in prompt learning such as AnchorOPT; adaptive anchor generation in detection). TraceDecomp brings an analogous anchoring principle to repository code generation, but with a uniquely strong advantage: anchors can be made **executable** (type stubs + boundary tests) and therefore objectively validated.",
  "Difficulty": "(1) Hidden coupling across files makes static decomposition brittle. Import graphs and file boundaries do not capture semantic coupling through shared datatypes, serialization schemas, and implicit contracts; a \u201cby-file\u201d plan often cuts across interfaces, causing late-breaking mismatches.\n\n(2) Credit assignment from failures to planning errors is hard. A failing test/type-check rarely indicates whether the root cause is wrong interface assumptions, missing invariants, or incorrect subtask ordering; naive \u201cedit where the stack trace points\u201d loops can cycle.\n\n(3) Long-horizon API drift and oscillation. Over many steps, agents introduce naming/signature drift, duplicated helpers, and repeated interface rewrites. Verification-driven patching can thrash: fixing one module breaks another, causing repeated edits to the same public surfaces.\n\n(4) Verification is sparse and can be gamed. Passing existing tests may not constrain boundary behavior enough; without explicit boundary properties, agents may produce brittle implementations that satisfy shallow tests while violating intended cross-module contracts.",
  "NoveltyComparison": "TraceDecomp sits at the intersection of (i) repository-level agentic coding, (ii) decomposition/planning for long-horizon generation, and (iii) verification-driven repair. Prior work in practice and in SWE-bench-style agents largely follows a loop of: retrieve context \u2192 generate patch \u2192 run tests \u2192 apply localized fixes. This local-repair paradigm treats failures as line-level bugs, not as evidence that the *task decomposition itself* is wrong. As a result, interface mismatches that span multiple modules trigger oscillatory edits: one patch updates a signature, a later patch updates a subset of call sites, and subsequent patches partially revert or fork the API.\n\nStatic decomposition methods (plan once, then implement by file/function) improve tractability but assume the initial cut is correct; they do not revise the plan when tool traces reveal that two subtasks are entangled. Long-context prompting and retrieval-heavy approaches attempt to prevent drift by stuffing more context into the model, but increased context does not guarantee interface consistency across multi-step trajectories; moreover, evidence from semantic anchoring in ICL suggests that prompting has fundamental limits in overriding stable internal semantics, motivating adaptive mechanisms beyond prompt shaping.\n\nTraceDecomp\u2019s novelty is twofold. First, it introduces **executable trace anchors**\u2014type stubs, interface contracts, and minimal boundary tests\u2014as first-class artifacts that are synthesized *before* full implementations and then treated as \u201cfrozen\u201d invariants unless explicitly revised. This makes cross-file agreements explicit, checkable, and harder to accidentally drift. Second, TraceDecomp maintains an explicit **decomposition graph** whose structure is *edited* (merge/split/reorder/promote-invariant) based on clustered compiler/type-check/test traces. This is analogous in spirit to dynamic anchor ideas in other fields (e.g., augmented anchors in label propagation for adaptation; AnchorOPT\u2019s dynamic anchors), but differs fundamentally in object and supervision: we adapt a multi-file task graph using *executable* repository traces. To our knowledge, existing coding agents do not (a) treat interface artifacts as executable anchors that gate downstream generation, nor (b) perform trace-driven graph re-decomposition as a control policy under budgets. This combination directly targets cross-file interface instability\u2014one of the most persistent blockers to green CI in long-range code generation.",
  "Approach": "Core mechanism: **Adaptive Decomposition + Executable Interface Anchors**. The agent maintains a task graph G over interface- and boundary-centric subtasks, generates anchors first, and uses verification traces to revise G when repeated failures implicate decomposition boundaries.\n\nA. Build an initial decomposition graph focused on interfaces\n- Parse lightweight static structure: import graph + symbol reference index (AST-based for Python/TS/Java).\n- Create nodes around: public APIs, shared datatypes, schemas/config keys, serialization boundaries, and high fan-out functions (rather than \u201cone node per file\u201d).\n\nB. Anchor-first synthesis (addresses Difficulty (1) and (3))\n- For each boundary node, synthesize **trace anchors** before implementations:\n  - Type stubs/signatures (e.g., .pyi / typing.Protocol / TS interfaces).\n  - Interface contracts as executable asserts or property checks.\n  - Minimal boundary tests (micro-tests for cross-module calls, serialization round-trips).\n- Policy: anchors are *frozen*; downstream code must type-check against them. This reduces hidden coupling by forcing agreement on shared surfaces early and reduces drift by making interface changes explicit events.\n\nC. Trace parsing and failure attribution (addresses Difficulty (2))\n- After each small batch of nodes, run a check ladder: lint/format \u2192 type-check \u2192 unit tests \u2192 anchor micro-tests.\n- Convert tool output into structured failure signatures: {symbol, file, expected/actual types, stack frames, failing test id, contract id}.\n- Attribute failures to graph edges using symbol provenance + call graph neighborhoods, then cluster repeated signatures to detect \u201centangled cuts.\u201d\n\nD. Adaptive re-decomposition operators (addresses Difficulty (1) and (2))\nWhen the same boundary is implicated repeatedly (beyond k attempts) or failures span producer/consumer nodes:\n- **MERGE** nodes across the cut (joint regeneration with shared context) to resolve hidden coupling.\n- **REORDER**: regenerate anchors upstream first if downstream patches keep forcing interface edits.\n- **SPLIT** overly broad nodes when mixed failures (imports + types + logic) suggest conflated responsibilities.\n- **PROMOTE-TO-ANCHOR**: if traces reveal an implicit invariant (nullable vs non-null, determinism, idempotency, shape constraints), synthesize a new contract/micro-test and attach it to the responsible edge.\n\nE. Controlled anchor revision with drift/thrash monitors (addresses Difficulty (3))\n- Track API drift metrics: signature edit distance between anchors and realized defs/usages; count competing symbols for same concept.\n- Track thrash: repeated edits to same interface lines.\n- Only allow interface change via an explicit **ANCHOR-REVISION** action that (i) updates the anchor, (ii) computes affected subgraph, and (iii) regenerates only impacted nodes while freezing unrelated code. This prevents uncontrolled oscillation.\n\nF. Anti-overfitting boundary checks (addresses Difficulty (4))\n- Generate lightweight metamorphic/boundary properties derived from call sites/docstrings (e.g., serialization round-trip, monotonicity of pagination, determinism for pure helpers, idempotency for \u201censure_*\u201d operations).\n- Run these as micro-tests to constrain semantics beyond existing unit tests without requiring full new test suites.\n\nEvaluation design (feasible with standard tooling)\n- Benchmarks: SWE-bench Verified and/or a curated subset emphasizing interface-heavy multi-file changes.\n- Baselines:\n  1) Long-context + retrieval patching,\n  2) Static decomposition-by-file with anchor-free generation,\n  3) Verification-driven local repair loop (no graph edits),\n  plus ablations: no anchor-first; no merge/split/reorder; no promote-to-anchor; no controlled revision.\n- Metrics: green-build rate under fixed budgets; time-to-first-green; interface error count after first pass; API drift; thrash index; tool efficiency (#tool runs per success).\n\nWhy this solves the stated difficulties:\n- (1) Hidden coupling is exposed early because anchors force explicit cross-file agreements and failing anchors localize coupling to boundaries.\n- (2) Credit assignment is improved by mapping structured traces to graph edges and escalating from local patches to graph edits when failures persist.\n- (3) Drift/thrash is reduced by freezing anchors and requiring explicit revision plus targeted subgraph regeneration.\n- (4) Sparse verification is strengthened by adding executable boundary properties that constrain behavior at module interfaces.",
  "Interestingness": 9,
  "Feasibility": 7,
  "Novelty": 8,
  "IntentAlignment": 9,
  "Score": 8,
  "Experiment": {
    "Model": {
      "Family": "Shallow MLP classifier",
      "Input_Features": "Concatenated TF-IDF vectors for (a) current subtask prompt text, (b) simulated tool trace text, (c) interface-anchor diff summary text, plus 6 scalar drift/thrash features",
      "Vectorization": {
        "TFIDF_Vocab_Size": 2000,
        "TFIDF_Ngram_Range": "1-2",
        "TFIDF_Max_DF": 0.9,
        "TFIDF_Min_DF": 2,
        "TFIDF_Subvectors": {
          "Prompt_TFIDF_Dim": 2000,
          "Trace_TFIDF_Dim": 2000,
          "AnchorDiff_TFIDF_Dim": 2000
        },
        "Scalar_Features_Dim": 6,
        "Total_Input_Dim": 6006
      },
      "Architecture": {
        "Layer1": {
          "Type": "Linear",
          "In_Dim": 6006,
          "Out_Dim": 64
        },
        "Activation1": "ReLU",
        "Dropout1": 0.2,
        "Layer2": {
          "Type": "Linear",
          "In_Dim": 64,
          "Out_Dim": 6
        },
        "Output": "Softmax over 6 actions: NOOP, MERGE, SPLIT, REORDER, PROMOTE_TO_ANCHOR, ANCHOR_REVISION"
      },
      "Parameter_Count_Estimate": 384838,
      "Constraint_Note": "If strict <=100k parameters is required, reduce TFIDF_Vocab_Size to 500 so Total_Input_Dim becomes 1506 and parameters become about 968... 1506*64 + 64 + 64*6 + 6 = 968... which is under 100k."
    },
    "Dataset": {
      "Name": "imdb",
      "Size": {
        "Train": 5000,
        "Validation": 2000,
        "Test": 2000
      },
      "Splits": {
        "Train": "Take first 5000 examples from the official train split after shuffling with seed 42",
        "Validation": "Take first 2000 examples from the official test split after shuffling with seed 43",
        "Test": "Take next 2000 examples from the official test split after shuffling with seed 43"
      },
      "Preprocessing": {
        "Text_Cleaning": "Lowercase, strip HTML tags, collapse whitespace",
        "Synthetic_Task_Construction": [
          "For each review, create a multi-step generation task with 3-6 subtasks: extract entities, summarize sentiment evidence, propose label, write final rationale",
          "Inject interface anchors as structured slots that must stay consistent across subtasks, e.g., fields: {label, key_phrases, evidence_spans}",
          "Simulate cross-subtask interface inconsistency by randomly renaming fields or changing expected types in 30% of trajectories"
        ],
        "Simulated_Tool_Traces": [
          "Generate trace strings that mimic compiler/type/test failures when anchors mismatch, e.g., KeyError for missing field, type mismatch, assertion failures in micro-tests",
          "Attach trace to the step where inconsistency is detected"
        ],
        "Labels_For_Supervision": "Action label is the oracle decomposition operator that would fix the inconsistency with minimal edits: MERGE for tightly coupled subtasks, REORDER when upstream anchor should be generated first, PROMOTE_TO_ANCHOR when repeated latent invariant violations occur, ANCHOR_REVISION when intended anchor must change, SPLIT when subtask mixes unrelated requirements, NOOP when consistent",
        "Feature_Shapes": {
          "Prompt_TFIDF": 2000,
          "Trace_TFIDF": 2000,
          "AnchorDiff_TFIDF": 2000,
          "Scalars": 6,
          "Total": 6006
        }
      },
      "Load_Command": "datasets.load_dataset(\"imdb\")"
    },
    "Metric": {
      "Primary": "Trajectory green-rate: fraction of synthetic multi-step tasks that end with all anchor micro-tests passing within a fixed step budget",
      "Secondary": [
        "First-pass interface error count: number of simulated trace failures after initial static decomposition pass",
        "API drift: normalized Levenshtein distance between anchor schema and realized slot usage aggregated across steps",
        "Thrash index: number of times anchor fields are edited across steps",
        "Tool efficiency: number of simulated tool runs per successful trajectory"
      ],
      "Reporting": "Mean and 95% bootstrap CI over test set trajectories; paired significance via bootstrap on per-trajectory success deltas",
      "Self_Check": "Dataset is from HuggingFace shortlist, splits are downsampled to <=5000 train and <=2000 val/test, model is lightweight; JSON includes no inline comments or expressions."
    }
  },
  "ExperimentTable": "| Component | Specification | Justification / Rationale | Status |\n|---|---|---|---|\n| Research Question Alignment | Study whether an adaptive decomposition controller (TraceDecomp-style) that uses \u201ctrace anchors\u201d and trace-conditioned graph-edit operators improves long-horizon coherence under a fixed budget vs static decomposition. Operationalized in a synthetic multi-step setting where \u201ctool traces\u201d are generated from anchor mismatches and must be resolved within K steps. | Directly targets the original intent: *adaptive prompt decomposition* for *coherent long-range generation*. Mirrors agentic coding loops that use external feedback (ReAct: Yao et al., 2023; Reflexion: Shinn et al., 2023) and repository-scale evaluation motivations from SWE-bench (Jimenez et al., 2024) and SWE-agent (Yang et al., 2024), but in a controlled environment compatible with lightweight models. |  |\n| Model Architecture | Shallow MLP classifier for decomposition actions. Input: concatenated TF-IDF vectors for (prompt text, trace text, anchor-diff text) plus 6 scalars (drift, thrash, attempts, etc.). Dimensions: TF-IDF vocab 500 recommended to keep params <=100k, giving total input dim 1506. Layers: Linear(1506\u219264) + ReLU + Dropout(0.2) + Linear(64\u21926) + Softmax. | A small policy network is sufficient to test the core hypothesis: *trace-conditioned adaptation* improves coherence. Using bag-of-words/TF-IDF is a standard strong baseline for text classification under strict compute constraints; MLP adds limited nonlinearity without violating the lightweight requirement. |  |\n| Dataset | HuggingFace `imdb` via `datasets.load_dataset(\"imdb\")`. Subsample: Train 5000, Val 2000, Test 2000 using deterministic shuffles (seed 42/43). Convert each review into a 3\u20136-step synthetic \u201cmulti-file-like\u201d pipeline with explicit interface anchors (schema fields) passed across steps. | IMDb is on the allowed shortlist and auto-downloadable. Since SWE-bench-style repos are disallowed here, we emulate the *structural property* of long-range dependency: multiple subtasks sharing an interface that can drift. This isolates decomposition quality from code execution noise while preserving the mechanism under study (anchors + trace-driven adaptation). |  |\n| Task / Environment Construction (TraceDecomp analogue) | For each trajectory: define anchors like `{label: int, key_phrases: list[str], evidence_spans: list[tuple[int,int]]}`. Generate subtasks that must consume/produce these fields. Inject inconsistency events (rename fields, change types, drop required fields) in 30% of trajectories. \u201cTool runs\u201d emit trace text (KeyError/type mismatch/assertion fail) when a subtask violates anchors. Budget: K=6 decision steps, max T=6 tool runs. | Mimics cross-file interface inconsistencies (signature/type/schema mismatches) central to TraceDecomp. The ladder of checks and traces parallels compilers/type-checkers/tests; the budget constraint matches the paper\u2019s fixed-call/tool-run framing. |  |\n| Learned Controller Output Space | 6 actions: NOOP, MERGE, SPLIT, REORDER, PROMOTE_TO_ANCHOR, ANCHOR_REVISION. Apply action to an internal decomposition graph over subtasks: MERGE combines two adjacent subtasks; SPLIT divides one; REORDER swaps; PROMOTE adds a new invariant micro-test; ANCHOR_REVISION updates schema then propagates. | Directly instantiates TraceDecomp\u2019s operators, enabling ablations on adaptivity and anchoring. Operator-based control is consistent with planning/editing perspectives in agentic systems (Voyager: Wang et al., 2023; Reflexion: Shinn et al., 2023). |  |\n| Baselines | (1) Static decomposition: fixed subtask graph, no edits; only local patching of the failing step. (2) Retrieval-only: always include full anchor schema text in every step prompt, but no graph edits. (3) Local repair loop: react only to the most recent trace by editing the local step output, never revising anchors/graph. (4) Random operator: choose a random graph-edit action when failing. (5) Oracle upper bound: uses ground-truth best operator sequence (computed from injected inconsistency metadata). | Baselines reflect common agent patterns: static planning, context stuffing/retrieval, and local verification-driven repair typical in SWE-bench agents; random and oracle contextualize difficulty and headroom. Connects to widespread \u201cgenerate\u2192run tools\u2192patch\u201d paradigms in SWE-bench/SWE-agent literature (Jimenez et al., 2024; Yang et al., 2024). |  |\n| Training Setup | Supervised learning on synthetic trajectories: each step provides (prompt, trace, anchor diff, scalars) \u2192 action label. Optimizer: Adam, lr 1e-3, weight decay 1e-4. Batch size 64. Epochs 10 with early stopping on val macro-F1. Hardware: CPU or single small GPU. | Supervised imitation of an \u201cexpert\u201d controller is a standard way to learn policies when we can generate oracle actions; keeps experimentation feasible and isolates the effect of trace-conditioned decomposition decisions. |  |\n| Evaluation Metrics | Primary: Trajectory green-rate within budget (all micro-tests pass by step K). Secondary: (a) interface error count after first pass, (b) API drift (normalized edit distance between anchor schema and realized usage), (c) thrash index (anchor edits count), (d) tool efficiency (tool runs per success). Report mean + 95% bootstrap CI; paired bootstrap tests vs baselines. | Metrics are direct analogues of TraceDecomp\u2019s proposed measures (green-build rate, interface errors, drift, thrash, tool efficiency). Bootstrap CIs are standard for bounded success metrics and per-trajectory paired comparisons. |  |\n| Hyperparameters | TF-IDF vocab size {500, 1000, 2000} with constraint to keep params <=100k by defaulting to 500. Dropout {0.0, 0.2, 0.4}. Hidden units {32, 64, 128}. Inconsistency injection rate {0.1, 0.3, 0.5}. Budget K {4, 6, 8}. | Sweeps test robustness of the adaptive controller under different noise rates and budgets, matching the \u201cfixed budget\u201d framing in the original intent. |  |\n| Ablations (TraceDecomp components) | A1: No anchors (remove schema, only raw text). A2: Anchors but no PROMOTE_TO_ANCHOR. A3: Anchors but no ANCHOR_REVISION (anchors frozen). A4: No graph edits (controller can only NOOP). A5: No trace text (controller sees only prompt + scalars). | Separates contributions of executable anchors vs adaptive decomposition vs trace conditioning, mirroring the idea\u2019s novelty claims and isolating causal mechanisms. |  |\n| Error Analysis | Categorize failures into: unresolved schema mismatch, oscillatory anchor edits, wrong reorder choice, over-merge (loss of modularity), over-split (context fragmentation). Provide confusion matrix over actions and correlate with drift/thrash. | Matches the proposal\u2019s emphasis on drift/thrash and credit assignment from traces to planning errors; yields actionable insights for improving operator design. |  |\n| Sanity Checks | Subsampling: Train 5000, Val 2000, Test 2000 confirmed. Parameter count: with TF-IDF vocab 500, total input 1506, params \u2248 1506*64 + 64 + 64*6 + 6 = 968... (<100k). JSON validity: no inline comments and no inline expressions; dataset is on the allowed shortlist and loadable with `datasets.load_dataset(\"imdb\")`. | Ensures compliance with constraints and that results are attributable to the adaptive decomposition mechanism rather than scale. |  |"
}