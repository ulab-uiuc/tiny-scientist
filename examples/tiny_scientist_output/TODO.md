# Experiment TODO Plan

- [x] Step 1: Repo Skeleton + main.py CLI [rows: Training Setup (Implementation Details), Sanity Checks]
  - Create a lightweight runnable workspace with entrypoint main.py and a minimal config system (argparse + JSON/YAML) that supports two modes: (a) quadratic_benchmark and (b) sst2_smoke_test. Define output directories, logging (CSV/JSONL), deterministic seeding, float64 default for quadratic runs, and divergence guards (NaN check, ||x||>1e12).
- [x] Step 2: Spectral Ladder Quadratic Generator [rows: Quadratic Generator (Spectral Ladder), Training Setup (Implementation Details)]
  - Implement generator for SPD quadratics in R^256: build eigenvalues for κ∈{10,1e2,1e3,1e4} with patterns {two-cluster, power-law, flat+spike}; construct Q as {I, block-diagonal blocks size 8, dense random orthogonal}; form H=Q^T diag(λ) Q. Sample x* with ||x*||≈1 then set b=-H x*; implement f(x)=0.5 x^T H x + b^T x, grad(x)=H x + b, and compute f* and x* for diagnostics.
- [x] Step 3: Noise/Oracle Model + Budget Accounting [rows: Noise & Oracle Model, Budget-Normalized Evaluation Protocol]
  - Add stochastic gradient oracle g(x)=grad(x)+ξ with ξ~N(0,Σ) where Σ families include isotropic (σ^2 I), aligned (Q^T diag(s) Q), misaligned (Q^T P^T diag(s) P Q with random orthogonal P); implement s spectra {∝λ, ∝1/λ, independent two-cluster} and sweep σ in logspace(1e-6,1e0). Implement an OracleBudget counter: +1 per gradient call, +α per value call (α=1 primary, α=0.2 sensitivity). All optimizers must consume budget through this interface so comparisons are budget-normalized.
- [x] Step 4: Step-Size Policies + Optimizer Loop [rows: Methods Under Test (Step-Size Policies), Baselines, Hyperparameters]
  - Implement 10–12 step-size strategies with a shared optimizer API: constant η, diminishing η0/sqrt(t), exact line search (quadratic formula), Armijo backtracking (with value calls counted), Wolfe line search if implemented, BB1/BB2 (with safeguards/clipping), Polyak step using f* in {true, true+δ} with δ∈{1e-6,1e-3,1e-1}, RMSProp scalar, Adam-like scalar (global moments), and diagonal RMSProp-style preconditioner. Add hyperparameter grids exactly as specified and a tuning routine that selects best hyperparams under equal oracle-budget validation per regime.
- [ ] Step 5: Benchmark Runner + Metrics + Phase Maps + Meta-Model [rows: Evaluation Metrics, Phase-Transition Maps, Predictive Meta-Model (Interpretability)]
  - Implement the benchmark runner: for each (H,b,Σ) ladder configuration and each method/hparam, run N=100 noise seeds under budgets B∈{200,500,1000,2000}; record trajectories (budget, f-f*, ||x-x*||, step size, divergence flags) and secondary diagnostics (Q-basis eigendirection errors). Compute primary metrics (median budget-to-ε for ε∈{1e-8,1e-6}, stability probability, expected suboptimality at fixed B) plus tail metrics (95th percentile, rare divergence). Produce phase-transition maps over (log κ, log σ) faceted by clustering/rotation/alignment; estimate stability boundary where stability>=0.95. Fit an interpretable predictor (logistic regression or ≤64-hidden MLP) using features (log κ, cluster gap ratio, spectral entropy, rotation score via off-diagonal energy in standard basis, noise alignment score, log σ) and evaluate on held-out ladder configs.
- [ ] Step 6: SST-2 Smoke Test Harness [rows: Dataset (Smoke Test Harness), Sanity Checks]
  - Add a minimal dataset pipeline using HuggingFace glue/sst2 with subsampling train=5000, val=2000, test=2000 (val reused as proxy test). Preprocess: lowercase, whitespace tokenize, hashing vectorizer to 256-d, one-hot labels to 2-d. Implement tiny 2-layer MLP 256→64→2 with squared loss; train briefly to report validation accuracy as a runtime sanity check. Enforce split caps and parameter count (16578) in code assertions.
