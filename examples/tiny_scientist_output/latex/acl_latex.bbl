\begin{thebibliography}{16}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Adeoye and Bemporad(2024)}]{Adeoye2024AnIS}
Adeyemi~Damilare Adeoye and A.~Bemporad. 2024.
\newblock An inexact sequential quadratic programming method for learning and
  control of recurrent neural networks.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  36:2762--2776.

\bibitem[{Ahmedai et~al.(2025)Ahmedai, Sibanda, Goqo, and
  Noreldin}]{Ahmedai2025ACS}
Salma Ahmedai, P.~Sibanda, S.~Goqo, and O.~Noreldin. 2025.
\newblock A comparative study of linearization techniques with adaptive block
  hybrid method for solving first‐order initial value problems.
\newblock \emph{International Journal of Differential Equations}.

\bibitem[{Beck and Teboulle(2009)}]{Beck2009AFI}
Amir Beck and M.~Teboulle. 2009.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock \emph{SIAM J. Imaging Sci.}, 2:183--202.

\bibitem[{Diakonikolas et~al.(2020)Diakonikolas, Daskalakis, and
  Jordan}]{Diakonikolas2020EfficientMF}
Jelena Diakonikolas, C.~Daskalakis, and Michael~I. Jordan. 2020.
\newblock Efficient methods for structured nonconvex-nonconcave min-max
  optimization.
\newblock pages 2746--2754.

\bibitem[{Duchi et~al.(2011)Duchi, Hazan, and Singer}]{Duchi2011AdaptiveSM}
John~C. Duchi, Elad Hazan, and Y.~Singer. 2011.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{J. Mach. Learn. Res.}, 12:2121--2159.

\bibitem[{Dvurechensky et~al.(2019)Dvurechensky, Gasnikov, Nurminsky, and
  Stonyakin}]{Dvurechensky2019AdvancesIL}
P.~Dvurechensky, A.~Gasnikov, E.~Nurminsky, and F.~Stonyakin. 2019.
\newblock Advances in low-memory subgradient optimization.
\newblock \emph{Numerical Nonsmooth Optimization}.

\bibitem[{Hu et~al.(2022)Hu, Lou, Wang, Yan, Yang, and
  Ye}]{Hu2022AcceleratedSR}
Mengqi Hu, Y.~Lou, Bao Wang, Ming Yan, Xiu Yang, and Q.~Ye. 2022.
\newblock Accelerated sparse recovery via gradient descent with nonlinear
  conjugate gradient momentum.
\newblock \emph{Journal of Scientific Computing}, 95:1--21.

\bibitem[{Luo et~al.(2019)Luo, Xiong, Liu, and Sun}]{Luo2019AdaptiveGM}
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu~Sun. 2019.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock \emph{ArXiv}, abs/1902.09843.

\bibitem[{Pei and Zhu(2014)}]{Pei2014ATA}
Yonggang Pei and D.~Zhu. 2014.
\newblock A trust-region algorithm combining line search filter technique for
  nonlinear constrained optimization.
\newblock \emph{International Journal of Computer Mathematics}, 91:1817 --
  1839.

\bibitem[{Qu et~al.(2020)Qu, Ding, and Wang}]{Qu2020AFA}
Quan Qu, Xianfeng Ding, and Xinyi Wang. 2020.
\newblock A filter and nonmonotone adaptive trust region line search method for
  unconstrained optimization.
\newblock \emph{Symmetry}, 12:656.

\bibitem[{Robles-Kelly and Nazari(2019)}]{Robles-Kelly2019IncorporatingTB}
A.~Robles-Kelly and A.~Nazari. 2019.
\newblock Incorporating the barzilai-borwein adaptive step size into sugradient
  methods for deep network training.
\newblock \emph{2019 Digital Image Computing: Techniques and Applications
  (DICTA)}, pages 1--6.

\bibitem[{Stich(2014)}]{Stich2014OnLC}
Sebastian~U. Stich. 2014.
\newblock On low complexity acceleration techniques for randomized
  optimization.
\newblock pages 130--140.

\bibitem[{Su et~al.(2014)Su, Boyd, and Candès}]{Su2014ADE}
Weijie Su, Stephen~P. Boyd, and E.~Candès. 2014.
\newblock A differential equation for modeling nesterov's accelerated gradient
  method: Theory and insights.
\newblock pages 2510--2518.

\bibitem[{Tovbis et~al.(2024)Tovbis, Krutikov, and
  Kazakovtsev}]{Tovbis2024NewtonianPO}
Elena~M. Tovbis, Vladimit~N. Krutikov, and L.~Kazakovtsev. 2024.
\newblock Newtonian property of subgradient method with optimization of metric
  matrix parameter correction.
\newblock \emph{Mathematics}.

\bibitem[{Vankov et~al.(2024)Vankov, Rodomanov, Nedić, Sankar, and
  Stich}]{Vankov2024OptimizingL}
Daniil Vankov, Anton Rodomanov, A.~Nedić, Lalitha Sankar, and Sebastian~U.
  Stich. 2024.
\newblock Optimizing (l0, l1)-smooth functions by gradient methods.

\bibitem[{Wu et~al.(2022)Wu, Magnússon, Feyzmahdavian, and
  Johansson}]{Wu2022DelayadaptiveSF}
Xuyang Wu, S.~Magnússon, Hamid~Reza Feyzmahdavian, and M.~Johansson. 2022.
\newblock Delay-adaptive step-sizes for asynchronous learning.
\newblock pages 24093--24113.

\end{thebibliography}
